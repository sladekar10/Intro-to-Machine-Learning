{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5d3c124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Supress numpy scientific notation\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fa108f",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    " \\newcommand{\\wv}{\\mathbf{w}}\n",
    " \\newcommand{\\yv}{\\mathbf{y}}\n",
    " \\newcommand{\\zv}{\\mathbf{z}}\n",
    " \\newcommand{\\uv}{\\mathbf{u}}\n",
    " \\newcommand{\\vv}{\\mathbf{v}}\n",
    " \\newcommand{\\tv}{\\mathbf{t}}\n",
    " \\newcommand{\\bv}{\\mathbf{b}}\n",
    " \\newcommand{\\av}{\\mathbf{a}}\n",
    " \\newcommand{\\Chi}{\\mathcal{X}}\n",
    " \\newcommand{\\R}{\\rm I\\!R}\n",
    " \\newcommand{\\sign}{\\text{sign}}\n",
    " \\newcommand{\\Ym}{\\mathbf{Y}}\n",
    " \\newcommand{\\Xm}{\\mathbf{X}}\n",
    " \\newcommand{\\Wm}{\\mathbf{W}}\n",
    " \\newcommand{\\Zm}{\\mathbf{Z}}\n",
    " \\newcommand{\\Im}{\\mathbf{I}}\n",
    " \\newcommand{\\Um}{\\mathbf{U}}\n",
    " \\newcommand{\\Vm}{\\mathbf{V}}\n",
    " \\newcommand{\\Am}{\\mathbf{A}}\n",
    " \\newcommand{\\muv}{\\boldsymbol\\mu}\n",
    " \\newcommand{\\Sigmav}{\\boldsymbol\\Sigma}\n",
    " \\newcommand{\\Lambdav}{\\boldsymbol\\Lambda}\n",
    "$\n",
    "\n",
    "# Neural Networks Regression and Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdeae4a",
   "metadata": {},
   "source": [
    "## Table of notation\n",
    "\n",
    "| Symbol                     | Meaning                     | Symbol    | Meaning                                                          |\n",
    "|----------------------------|-----------------------------|-----------|------------------------------------------------------------------|\n",
    "| $\\xv$ or $\\vec{x}$         | feature/input vector        | $x_i$     | $i$th element of $\\xv$                                           |\n",
    "| $\\Xm$                      | input matrix                | $x_{i,j}$ | $i$th row and $j$th column of $\\Xm$                              |\n",
    "| $\\yv$ or $\\tv$             | labels/targets              | $n$       | number of features or columns \n",
    "| $\\wv$ or $\\mathbf{\\theta}$ | weight/parameter vector     | $m$       | number of data samples <br>(also used to refer to the slope) |samples or rows                                   |\n",
    "| $f$ or $h$                 | hypothesis function <br> (i.e., a model)        | $\\hat{\\yv}$ <br> $f(\\xv {;} \\wv)$<br>$h(\\xv {;} \\wv)$ | predictions <br> y-hat |\n",
    "| $E$              | error or sum of error (loss)  | $SSE$      | sum of squared error function                                            |\n",
    "| $MSE$                      | mean squared error| $\\nabla$  | gradient (nabla)                                       |\n",
    "| $\\partial$                 | partial derivative          | $\\alpha$  | learning rate (alpha)                                  |       \n",
    "| $J$ | general placeholder for <br>the objective function | $x^T$| transpose of a vector or matrix |\n",
    "$b$ | bias or y-intercept term | $T$ | Threshold |\n",
    "$*$| element-wise<br> multiplication | $\\cdot$ | dot product|\n",
    "| $z$<br>$\\zv$| value before applying activation function |  $X, Y$ | Random variables |\n",
    "| $K$| number/set of classes | $k$ | current class|\n",
    "| $MLE$|  maximum likelihood estimation | $ML$ |  maximum likelihood|\n",
    "| $MLL$|  maximum log likelihood | $LL$ | log likelihood |\n",
    "| $L$|  likelihood | $NLL$ | negative log likelihood |\n",
    "| $g$ | activation function | $a$/$h$ <br> $\\av/\\mathbf{h}$<br>$\\Am$/$\\mathbf{H}$ | output of activation function <br> or neuron\n",
    "$w$<br>$\\wv$<br>$\\Wm$ | weights| $z$<br>$\\zv$<br>$\\Zm$ | linear combination output|\n",
    "|$\\Wm^{[l]}$| $l$th layer weights| $\\Am^{[l]}$| $l$th layer activations\n",
    "|$\\Zm^{[l]}$| $l$th layer linear combinations| $\\bv^{[l]}$| $l$th layer bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e786e1",
   "metadata": {},
   "source": [
    "## Perceptron Review\n",
    "\n",
    "\n",
    "Recall that the perceptron framework attempted to simulate, in the most naive manner, a single biological neuron. The perceptron did so by simply taking inputs, performing a weights sum, applying an function activation to the weighted summation and then producing an output. \n",
    "\n",
    "More specifically, the perceptron worked by dotting inputs $\\Xm$ with the weights $\\wv$ to get an output $\\zv$. An activation function $g$ is then applied to the continuous outputs $g(\\zv)$ which squashes the values into a new value range (discrete or another continuous value range). Recall, with logistic regression we could use the predictions made by the same perceptron structure to compute the error and then update the weights, given we had a loss/cost function that could be differentiated. \n",
    "<img src=\"https://sebastianraschka.com/images/faq/logisticregr-neuralnet/schematic.png\" width=500 height=500>\n",
    "\n",
    "\n",
    "Now, a simple extension might be to add multiple neurons. The idea being, adding more neurons could lead to a more complex model, in turn allowing for the modeling of more complex data. This is the exact idea of neural networks! Take for instance the below picture which represents the basic framework of the perceptron. Notice, that there is only a single neuron represented by the empty circle.\n",
    "\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51632957722_a8e02fd162.jpg\" width=\"300\" height=\"150\" alt=\"basic-perceptron\">\n",
    "\n",
    "By adding multiple neurons we can get the below image. This is an example of a neural network or in other words a network of multiple neurons.\n",
    "<img src=\"https://live.staticflickr.com/65535/51634637165_1ebcf48062.jpg\" width=\"300\" height=\"150\" alt=\"basic-nn\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd498bf9",
   "metadata": {},
   "source": [
    "## Neural Network Framework\n",
    "\n",
    "<img src=\"https://images.squarespace-cdn.com/content/v1/5ccb715016b640627a1c2782/1586907440135-6INS2V3VS31ICY3TNUIW/ai-artificial-neural-network-alex-castrounis.png?format=500w\" >\n",
    "\n",
    "The general neural network framework can be seen in the above image. The above neural network is an example of a two layer neural network. We'll be working with two layer neural networks throughout these notes. Let's start by first reviewing the general structure of a neuron and then talcking what each layer is doing. From there we can then understand how neural networks make predictions and, even better, how they can learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af292a2f",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "Let's start by understand the most basic unit in neural networks: the neuron. Recall, we actually already have all the basic ideas for how a neuron works as neurons in neural networks work almost exactly the same as the single neuron in Rosenblatt's perceptron. Below is an image of a neuron. There are 4 main components of a neuron: inputs, weighted summation of inputs (i.e., linear combination of inputs), applying an activation function to said weighted summation, and outputting the activation.\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51633984258_9b95f324ce.jpg\" width=\"300\" height=\"150\" alt=\"artifcal-neuron\">\n",
    "\n",
    "Let's start by reviewing the 4 basic steps mentioned:\n",
    "\n",
    "1. First, each neuron takes input from every neuron in the previous layer represented by $x$ or $a$. The 1st hidden layer always takes in the data $x$ as input.\n",
    "2. Next, each input is assigned a weight just like we have seen in previous algorithms. For clarity sack, we will keep the bias separate from the weights even though they can be combined as we have done in the past. Using the weights, a linear combination can be taken between the weights and inputs where the bias is also added such that we get the following: $z = \\wv^\\top \\xv + b$. Recall $z$ represents the continuous output of a neuron (i.e., the output after the linear combination of inputs). This linear combination of inputs is nothing new, we have seen this time and time again with linear regression and classification algorithms alike.\n",
    "3. Next, an activation function $g(\\cdot)$is applied to $z$ producing the  neuron output $a$ where $a = g(z)$. The activation function is often picked to be a non-linear function. We do this in order to convert our equation from a linear equation to a non-linear equation (more on this later). \n",
    "4. Finally, after applying the activation function, the output $a$ is passed to the next layer and acts as the inputs for the next layer. \n",
    "\n",
    "\n",
    "Here are the key take aways and notation details:\n",
    "- $x$ stands for an input data sample and $a$ stands for the output of a neuron after applying an activation. As we'll see, while $a$ is the activation output of a neuron, it will also act as input for the next layer. Thus, $x$ acts as input for the first hidden layer and $a$ acts as input for the proceeding hidden layer or output layer.\n",
    "- Each neuron has weights $\\wv$ where there is a weight for EACH input $x$ or $a$. If the bias $b$ is included in the weights then there is a weight for bias. However, since we are treating the bias as a separate entity, we can treat it as a parameter on its own that needs to be optimized. \n",
    "- $z$ acts as the continuous output after a linear combination of inputs $x$ or $a$ is taken with the neurons wights $\\wv$.\n",
    "- $a$ is the output of a neuron after applying an activation function $g(z)$ to $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526a892",
   "metadata": {},
   "source": [
    "### Layers\n",
    "Next, let's breakdown what the layers represent and how they interact with each other. The most important and general idea to note about layers is that EVERY neuron in a layer is fully connected to the neurons in the next layer. This means, there is connection between every neuron in the previous layer and every neuron in the current layer! This is why neural networks layers are sometimes called fully connected layers.\n",
    "\n",
    "Before we do continue, let's review how we can tell how many layers a neural networks actually has. Recall, we will be working with two layer neural networks throughout this note. The below is an image of a two layer neural network which includes an input layer, hidden layer, and output layer. Now, you might be scratching your head and thinking, wait a second, you named three layers not two! \n",
    "\n",
    "While logically you would be correct, in this case you are actually wrong. The number of layers in a neural network is determined by the number of hidden layers plus the output layer such that the input layer is not counted. In other words, as we'll see later one, the number of layers corresponds to the number of weight matrices needed by a given neural network.\n",
    "\n",
    "<img src=\"https://images.squarespace-cdn.com/content/v1/5ccb715016b640627a1c2782/1586907440135-6INS2V3VS31ICY3TNUIW/ai-artificial-neural-network-alex-castrounis.png?format=500w\" width=300 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1fbd1c",
   "metadata": {},
   "source": [
    "\n",
    "#### Input layer\n",
    "The layer which takes in all the inputs of the data is referred to as the *input layer*. While this layer is not counted as an actual layer as it contains no neurons. It is often referred to as the input layer regardless. This layer is used to represent all the data features that will be input into the actual neural network. In other words, we can think of this layer as representing the input data as a matrix $\\Xm$ (multiple data samples) or a vector $\\xv$ (a single data sample) where each blue circle in the input layer image above can be thought of as a single feature in the data. Thus, the above data would have 3 features.\n",
    "\n",
    "#### Hidden Layer\n",
    "The first actual layer is called the *hidden layer*. The hidden layer acts as a intermediate layer between the input layer and output layers. A hidden layer is the 1st real layer in a neural network. The hidden layer contains neurons which are more specifically referred to as *hidden neurons*, also known as *hidden units*. This is partly done to differentiate the hidden neurons from the output neurons. \n",
    "\n",
    "**Hidden neurons/units:** \n",
    "There can be as many hidden neurons as you'd like in a given hidden layer. Thus, hidden neurons act as a hyper-parameter you can tune where the optimal number of hidden neurons is often unknown and changes based on the given problem at hand. Each white circle in the above image can be seen as representing a hidden unit. Thus, the above image would have 5 hidden neurons. \n",
    "\n",
    "**Multiple hidden layers:** A two layer neural network is the most simplistic neural network as it consists of only a single hidden layer. However, it is possible to stack hidden layers such that there are multiple hidden layers between the input and output layers. Once again, the number of hidden layers to use is often considered a hyper-parameter that needs to be tuned given the current problem at hand. When we stack enough of these layers (typically 3 or more) we'll see that we get what are classified as *deep neural networks* (more on this and deep learning in the future.)\n",
    "\n",
    "**Inputs-outputs:** Each neuron in the first hidden layer takes each of the data features $x$ as inputs. If there are multiple hidden layers, each proceeding hidden layer takes the output from previous hidden layer $a$ as input. Recall, each hidden unit or neuron outputs an activation $a$, thus all the outputs from all the hidden neurons can be represented as a matrix $\\Am$ for multiple data samples $\\Xm$ or a vector $\\av$ for a single data sample $\\xv$. \n",
    "\n",
    "#### Output Layer\n",
    "\n",
    "The second and last layer is called the *output layer*. The output layer contains neurons which produce the outputs of the neural network. In other words, the output layer is responsible for computing the predictions $\\hat{y}$. \n",
    "\n",
    "**Output neurons**: Output neurons function just like the neurons in the hidden layers. The main difference is that the outputs $a$ is the prediction for the model such that $a = \\hat{y}$. Each orange circle in the above image can be seen as representing an output neuron. The above image would have 1 output neuron and one prediction $\\hat{y}$. Thus, the output/prediction can be represented as a vector $\\hat{\\yv}$ for multiple data samples $\\Xm$ or a single value $\\hat{y}$ for a single data sample $\\xv$ . \n",
    "\n",
    "**Regression outputs:** In the case of regression, the output layers typically have a single neuron with a linear activation function. As we'll see, a linear activation just means the weighted sum of inputs is used as the output directly. In other words, you can think about this as there being no activation function applied. Further, regression problems typically have a single output neuron as regression problems tend to require the prediction of single continuous number. Although, it is totally possible and valid to have multiple outputs for a regression problem and therefore multiple output neurons. \n",
    "\n",
    "**Inputs-outputs:** Each neuron in the output layer takes the outputs from the each neuron in the last hidden layer as inputs. In other words, the output layer takes the outputs from the last hidden layer $\\Am$ or $\\av$ as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ae099",
   "metadata": {},
   "source": [
    "# Feed-forward: Making Predictions \n",
    "\n",
    "Given a basic understanding of neurons and layers we are ready to see how a neural network makes predictions. To do so we are going to look at how data flows through an two layer neural network. Further, we'll look at the notation for neural networks as the notation can become cluttered quickly.\n",
    "\n",
    "To keep things simple, let's look at a two layer neural network that takes 3 input features, has 2 hidden neurons (i.e., neurons), and 1 output neuron. See the below image for a visualization of this network.\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51636259865_8edb486543.jpg\" width=\"500\" height=\"300\" alt=\"single_layer_nn\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4d384",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "First let's start with notation for each input, layer, weight, linear combination, activation, and output. First, we need to cover the general notation in regards to the super and subscript. Unless otherwise specified, subscript indicates the neuron $a_1$ or feature number $x_1$ in a particular layer while superscript indicates the layer $a^{[1]}$. For instance, $a^{[1]}_2$ would be the activation for the 2nd neuron in layer 1 (i.e., the hidden layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d5d80",
   "metadata": {},
   "source": [
    "### Feature inputs $x$\n",
    "$x$ represents a SINGLE input features from the data. Here $x_1$ is the first input feature, $x_2$ is the second input feature, and $x_3$ is the third input feature. \n",
    "$$\n",
    "x_{\\text{feature}}\n",
    "$$\n",
    "- $\\xv$ represents a SINGLE data sample flowing through the network where each element corresponds to the features for the element. If we were using online gradient decent we'd only pass a single data sample like this. Visually $\\xv$ looks like the following:\n",
    "$$\n",
    "\\xv = \\begin{bmatrix}\n",
    "                   x_1\\\\\n",
    "                   x_2\\\\\n",
    "                   x_3\n",
    "             \\end{bmatrix}\n",
    "$$\n",
    "where the subscript $x_1$ indicates the feature for of the $n$ features in the data.\n",
    "- $\\Xm$ represents MULTIPLE data samples flowing through the network where each row corresponds to a feature and each column corresponds to a data sample such that the <u>shape is **(features, data samples)**</u>. Notice, this is reverse or transpose of what we are used to seeing (data samples, features)!!!! We'll see why this is the case soon! Visually $\\Xm$ looks like the following:\n",
    "$$\n",
    "\\Xm = \\begin{bmatrix}\n",
    "                       |  & | &  & | \\\\\n",
    "                   \\xv^{1} & \\xv^{2} & \\dots & \\xv^{m}\\\\\n",
    "                       |  & | &  & | \\\\\n",
    "             \\end{bmatrix}\n",
    "$$\n",
    "where the superscript $\\xv^{1}$ here refers to the data sample for each of the $m$ data samples in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8066c",
   "metadata": {},
   "source": [
    "### Weights $w$\n",
    "$w$ represents the weight for each neuron in a given layer. The weights are given by the connections between the inputs and hidden neurons and between the hidden neurons and output neuron. Now there is a lot of superscript and subscript notation here. Recall the superscript corresponds to the layer number while the subscript here corresponds to the neuron AND the weight index (i.e., the weight for a given input). This notation corresponds to a single weight given as follows:\n",
    "$$\n",
    "w^{[\\text{layer}]}_{\\text{neuron}, \\text{weight index}}\n",
    "$$\n",
    "For instance, $w^{[2]}_{1, 2}$ corresponds to the 2nd weight for the 1st neuron in the 2nd layer. Additionally, $w^{[1]}_{2, 3}$ corresponds to the 3rd weight for the 2nd neuron in 1st layer.\n",
    "\n",
    "- $\\wv$ or $\\wv^{[\\text{layer}]}_{\\text{neuron}}$ represents the weight vector for EACH neuron. Recall each neuron is connected to all the neurons in the previous/next layer. Thus, each neuron has a weight for each connection. For instance, the weight vector for the 1st layer and 1st neuron would be given as follows:\n",
    "$$\n",
    "\\wv^{[1]}_1=  \\begin{bmatrix}\n",
    "                   w^{[1]}_{1,1}\\\\\n",
    "                   w^{[1]}_{1,2}\\\\\n",
    "                   w^{[1]}_{1,3}\n",
    "             \\end{bmatrix}\n",
    "$$\n",
    "Additionally, $\\wv^{[2]}_2$ would then be a vector of ALL the weights for the 2nd neuron in the 2nd layer.\n",
    "- $\\Wm$ or $\\Wm^{[\\text{layer}]}$ corresponds to a matrix of weights for the ENTIRE layer. This notation corresponds to the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Wm^{[1]} =\\begin{bmatrix}\n",
    "                       - \\wv^{[1]\\top}_1 -\\\\\n",
    "                       - \\wv^{[1]\\top}_2 -\\\\\n",
    "             \\end{bmatrix} =  \\begin{bmatrix}\n",
    "                   w^{[1]}_{1,1} & w^{[1]}_{1,2} & w^{[1]}_{1,3}\\\\\n",
    "                   w^{[1]}_{2,1} & w^{[1]}_{2,2} & w^{[1]}_{2,3}\\\\\n",
    "             \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "where each row corresponds to a neuron and each column corresponds to the number of inputs. <u>Thus, the shape of $\\Wm$ is (neurons, inputs)</u>. For instance, $\\Wm^{[1]}$ corresponds to the weight matrix for the 1st layer. Lastly, notice $\\Wm$ is simply created by stacking the transpose of each neuron's weight vector $\\wv^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01578117",
   "metadata": {},
   "source": [
    "### Linear combination $z$\n",
    "\n",
    "$z$ represents the linear combination output for a given layer. Recall, $z$ is produced by taking the linear combination of weights $\\wv$ with the inputs $\\xv$ or $\\av$ plus a bias $b$. For a SINGLE data sample and a SINGLE neuron in a given layer $z$ would be given as follows:\n",
    "\n",
    "$$\n",
    "z^{[\\text{layer}]}_{\\text{neuron}} = \\wv^{[\\text{layer}]\\top}_{\\text{neuron}} \\xv + b^{[\\text{layer}]}_{\\text{neuron}}\n",
    "$$\n",
    "For instance, $z^{[1]}_{2}$ corresponds to the linear combination output for the 1st neuron in the 1st layer.\n",
    "- $\\zv$ represents the  linear combination output for MULTIPLE data samples and a SINGLE neuron. The equation for computing $\\zv$ for a SINGLE neuron and a given layer is given as follows:\n",
    "$$\n",
    "\\zv^{[\\text{layer}]}_{\\text{neuron}} = \\wv^{[\\text{layer}]\\top}_{\\text{neuron}} \\Xm + b^{[\\text{layer}]}_{\\text{neuron}}\n",
    "$$\n",
    "Thus, computing $\\zv^{[1]}_{2}$ for multiple data samples $\\Xm$ would be given as follows:\n",
    "\n",
    "$$\n",
    "\\zv^{[1]}_{2} = \\wv^{[1]\\top}_{2} \\Xm + b^{[1]}_2.\n",
    "$$\n",
    "- $\\Zm$ represents the linear combination output for MULTIPLE data samples and ALL neurons in a given layer. <u>$\\Zm$ has the shape (neurons, data samples)</u>. The equation for computing $\\Zm$ for ALL neurons for a particular layer is given as follows:\n",
    "$$\n",
    "\\Zm^{[\\text{layer}]} = \\Wm^{[\\text{layer}]} \\Xm + \\bv^{[\\text{layer}]}\n",
    "$$\n",
    "Thus, computing $\\Zm^{[1]}$ for multiple data samples $\\Xm$ would be given as follows:\n",
    "$$\n",
    "\\Zm^{[1]} = \\Wm^{[1]} \\Xm + \\bv^{[1]}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e438b6b",
   "metadata": {},
   "source": [
    "### Activation $a$\n",
    "\n",
    "Lastly, $a$ represents the activation of the neuron after applying the activation function $g(\\cdot)$.  For a SINGLE data sample and a SINGLE neuron in a given layer $a$ would be given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{[\\text{layer}]}_{\\text{neuron}} &= g(z^{[\\text{layer}]}_{\\text{neuron}}) \\\\\n",
    "\\\\\n",
    "&= g(\\wv^{[\\text{layer}]\\top}_{\\text{neuron}} \\xv + b^{[\\text{layer}]}_{\\text{neuron}})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- $\\av$ represents the neuron activation output for MULTIPLE data samples and a SINGLE neuron. The equation for computing $\\av$ for a SINGLE neuron and a given layer is given as follows:\n",
    "$$\n",
    "\\av^{[\\text{layer}]}_{\\text{neuron}} = g(\\zv^{[\\text{layer}]}_{\\text{neuron}})\n",
    "$$\n",
    "Thus, computing $\\av^{[1]}_{2}$ for multiple data samples $\\Xm$ would be given as follows:\n",
    "\n",
    "$$\n",
    "\\av^{[1]}_{2} = g(\\zv^{[1]}_{2})\n",
    "$$\n",
    "- $\\Am$ represents the neuron activation output for MULTIPLE data samples and a ALL neurons in a given layer. <u>$\\Am$ has the shape (neurons, data samples)</u>.  The equation for computing $\\Am$ for a ALL neurons for a particular layer is given as follows:\n",
    "$$\n",
    "\\Am^{[\\text{layer}]} = g(\\Zm^{[\\text{layer}]})\n",
    "$$\n",
    "Thus, computing $\\Am^{[1]}$ for multiple data samples $\\Xm$ would be given as follows:\n",
    "$$\n",
    "\\Am^{[1]} = g(\\Zm^{[1]} )\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2822de",
   "metadata": {},
   "source": [
    "## Activation functions: the key element\n",
    "\n",
    "Now, recall each neuron can have an activation function. Previously we have seen the sigmoid and binary/step-function activation functions when working with Rosenblatt's perceptron and logistic regression. However, there are many, MANY, more activation functions out there. \n",
    "\n",
    "**The KEY element of neurons in neural networks are non-linear activation functions.** By introducing non-linearity with non-linear activation functions neural networks are able to learn more complex non-linear models. \n",
    "\n",
    "Without non-linear activation functions, neural networks would simply amount to a linear combination of many linear equations. As you might know, the output of a combination of linear equations is itself linear.  Thus, even if we used hundreds of neurons with a linear activation function (i.e., no activation $z = a$) the output could only ever be linear. \n",
    "\n",
    "However, by wrapping the output of a neuron in a non-linear activation function,  the output of a neural network can now be non-linear. This simple addition of a non-linear activation is what allows neural networks to begin to learn more complex models that can deal with complex and non-linear data. \n",
    "\n",
    "\n",
    "### Types of activation functions\n",
    "\n",
    "As mentioned, there are many different types of activation functions. See the below animation which gives you a glimpse an just handful of the activation functions used throughout history and today. \n",
    "<img src=\"https://mlfromscratch.com/content/images/2019/12/activationfunctions.2019-08-01-16_58_53.gif\">\n",
    "\n",
    "For now, we are going to focus on about 4 of the activation functions given in the above GIF for the rest of the course. Three of these functions are non-linear and one is a linear activation function which acts as if no activation function was applied.\n",
    "\n",
    "The 3 non-linear activation functions are the sigmoid, tanh, and ReLU. The sigmoid and tanh activation functions were some of the first activation functions used in neural networks while ReLU is a modern day activation function that is extremely effective and alleviates many of the issues the sigmoid and tanh activation functions introduced into neural networks. The one linear activation function we'll look at is the identity activation also referred to as just the linear activation.\n",
    "\n",
    "For this module, we'll simply be looking at the sigmoid, tanh, and linear activation function. In following modules we'll take a closer look at the ReLU and, while not mentioned, the softmax activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b44ffa8",
   "metadata": {},
   "source": [
    "#### Sigmoid \n",
    "\n",
    "The sigmoid activation squashes values between 0 and 1 and is one of the first non-linear activation functions to be used. Recall, this is because it is the smooth version of the binary activation function used by Rosenblatt's perceptron.\n",
    "\n",
    "While the sigmoid activation function is more biologically plausible, it actually doesn't work well in neural networks (unless you are using it as as the activation function for an output neuron to predict probabilities in binary classification). The issue here is two fold: \n",
    "1. The first issue is that the mean of the sigmoid function is 0.5. Recall, we like to standardize our data to have a mean of zero. One reason we do this is because it helps create smoother learning as values near zero produce more stable weight/gradient updates. Now, since the sigmoid has a mean of 0.5 this can be seen as \"undoing\" the standardization we performed on our data and creates more unstable gradients.\n",
    "2. The second issue is referred to as saturating. What this means is that the output of the sigmoid gets saturated near the boundaries of the function which means the derivative is near zero. Thus, when learning the weights using gradient decent the derivative will be near zero and little to no learning will occur. Having the majority of your neurons with near zero gradients is particularly bad for neural networks as gradients are propagated through the entire network leading to a problem called the [*vanishing gradient problem*](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).\n",
    "\n",
    "Recall the sigmoid activation function equation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z) &= \\frac{1}{1 + e^{-z}} \\\\\n",
    "&= \\frac{e^z}{1 + e^{z}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Further the derivative of the sigmoid activation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g'(z) &= \\frac{1}{1 + e^{-z}} (1 - \\frac{1}{1 + e^{-z}}) \\\\\n",
    "&= g(z)(1- g(z))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Below is the `Sigmoid` class which contains two static methods `activation()` and `derivative`. The activation computes the sigmoid activation function while the `derivative` computes the derivative of the activation function which we'll need when learning weights of a neural network. \n",
    "\n",
    "Notice that the methods have a decorator that make the methods static methods. Recall, [static methods](https://pythonbasics.org/static-method/) are allowed to be called without initializing the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87f4896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        return Sigmoid.activation(z) * (1 -  Sigmoid.activation(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d14ac",
   "metadata": {},
   "source": [
    "Below is the plot for the sigmoid activation function and the derivative of the sigmoid function for values between -10 and 10. \n",
    "\n",
    "Notice, as the sigmoid function (orange line) approaches 0 and 1 the derivative (blue line) goes to 0! Finally, note that the doted black line indicates the mean of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6579b98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10, 10 , .2)\n",
    "y = Sigmoid.activation(x)\n",
    "dy = Sigmoid.derivative(x)\n",
    "\n",
    "plt.hlines(.5, xmin=-10, xmax=10, colors='black', linestyles='dotted')\n",
    "plt.plot(x ,dy, label='Derivative')\n",
    "plt.plot(x ,y, label='Activation')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad4283",
   "metadata": {},
   "source": [
    "#### Tanh\n",
    "\n",
    "The tanh is the evolution of the sigmoid function which squashes values between -1 and 1. Tanh acts almost identically to the sigmoid function, however the tanh activation function has a mean of 0. Still, tanh, just like the sigmoid, suffers from the saturation problem where values at -1 and 1 have a near zero derivative. Regardless, when choosing between the sigmoid and tanh there is almost no reason to ever go with the sigmoid!\n",
    "\n",
    "The tanh activation function equation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z) &= \\tanh(z) \\\\\n",
    "&= \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Further, the derivative of the tanh activation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g'(z) &=  1 - \\tanh^2(z)\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "beebf80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        return 1 - np.tanh(z)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f644e0",
   "metadata": {},
   "source": [
    "Below is the plot containing the tanh activation function and the derivative of the tanh function for values between -10 and 10. \n",
    "\n",
    "Notice, as the tanh function (orange line) approaches -1 and 1 the derivative (blue line) goes to 0! Finally, note that the doted black line indicates the mean of the tanh function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5e26d31d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10, 10 , .2)\n",
    "y = Tanh.activation(x)\n",
    "dy = Tanh.derivative(x)\n",
    "\n",
    "plt.hlines(0, xmin=-10, xmax=10, colors='black', linestyles='dotted')\n",
    "plt.plot(x ,dy, label='Derivative')\n",
    "plt.plot(x ,y, label='Activation')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae9869",
   "metadata": {},
   "source": [
    "#### Identity or linear\n",
    "\n",
    "The last activation function is more of a placeholder activation function when you want to apply no activation function. The identity or linear activation function simply servers to act as if no activation was applied. To do so, we simply apply a linear or the identity transformation where the input is equal to the output.\n",
    "\n",
    "The main take away here is that anytime you hear there is no activation function for a neuron then you can think of the neuron as having the identity/linear activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "28dfea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear():\n",
    "    @staticmethod\n",
    "    def activation(z):\n",
    "        return z\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative(z):\n",
    "        return np.ones(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ec2667",
   "metadata": {},
   "source": [
    "Below is the plot containing the identity/linear activation function and the derivative of the identity/linear function for values between -10 and 10. \n",
    "\n",
    "Notice, that the identity/linear function is just a straight line where the x value is equal to the y value (i.e., input equals output). Further, notice that the derivative is always 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7ef27d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb7dc94ab50>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10, 10 , .2)\n",
    "y = Linear.activation(x)\n",
    "dy = Linear.derivative(x)\n",
    "\n",
    "plt.plot(x ,dy, label='Derivative')\n",
    "plt.plot(x ,y, label='Activation')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc9aa7",
   "metadata": {},
   "source": [
    "## Feed-forward Code\n",
    "\n",
    "\n",
    "Finally, it's time to see how we can make a prediction using neural network. To do so, we'll compute each layers output starting from the input layer all the way to the output layer. Let's do so using the very basic two layer neural network given below. \n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51636259865_8edb486543.jpg\" width=\"500\" height=\"409\" alt=\"single_layer_nn\">\n",
    "\n",
    "The goal will be to efficiently compute output of each layer. Recall, we can do so by computing the output for EVERY neuron in the layer at once by applying the following equations in order.\n",
    "\n",
    "1. First we compute the 1st layer outputs by computing $\\Am^{[1]}$. Here we'll use the sigmoid activation function for $g$.\n",
    "$$\n",
    "\\Zm^{[1]} = \\Wm^{[1]} \\Xm + \\bv^{[1]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Am^{[1]} = g(\\Zm^{[1]} )\n",
    "$$\n",
    "\n",
    "2. Next, we compute 2nd layer (i.e., output layer) SINGLE output by computing $\\av^{[2]}$. Here we'll use the identity/linear activation function for $g$ as linear regression requires no activation for the output neuron.\n",
    "\n",
    "$$\n",
    "\\zv^{[2]} = \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\av^{[2]} = g(\\zv^{[2]})\n",
    "$$\n",
    "\n",
    "3. Lastly, since the output of the 2nd layer is the output layer then the activations are the same as the predictions.\n",
    "\n",
    "$$\n",
    "\\hat{\\yv} = \\av^{[2]\\top} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888aa88b",
   "metadata": {},
   "source": [
    "### Toy Data\n",
    "\n",
    "Below, the `nonlinear_data()` function defines some toy non-linear regression data that we'll use to test out the neural network implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "49011fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression, make_friedman1\n",
    "\n",
    "def nonlinear_data(m=100, n=3):\n",
    "    X, _ = make_regression(\n",
    "        n_samples=m, \n",
    "        n_features=n, \n",
    "        n_informative=1,\n",
    "        noise=3.5,\n",
    "        random_state=42,\n",
    "    )\n",
    "    rng = np.random.RandomState(42)\n",
    "    X[:, 0] = np.sort(6 * rng.rand(m, 1) - 3, axis=0).reshape(m,)\n",
    "    y = (2 + .5* X[:, 0]**2 + X[:, 0]) + rng.rand(m)\n",
    "\n",
    "    return X, y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfebb6",
   "metadata": {},
   "source": [
    "Below we can see the shape of the data which has 100 data samples and 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5f7ab82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = nonlinear_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f510aff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b499f3e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.9668673 ,  0.34115197, -0.07710171],\n",
       "       [-2.87649303,  1.16316375, -1.43586215],\n",
       "       [-2.84748524, -0.16128571, -0.80227727],\n",
       "       [-2.79366887, -1.23695071,  0.78182287],\n",
       "       [-2.72863627, -0.33450124, -1.20029641],\n",
       "       [-2.72129752,  0.06980208, -0.60021688],\n",
       "       [-2.65149833,  2.13303337, -1.2378155 ],\n",
       "       [-2.6186499 ,  1.46564877, -1.4123037 ],\n",
       "       [-2.60969044, -1.91877122, -0.07444592],\n",
       "       [-2.55573209,  0.17457781,  1.8861859 ],\n",
       "       [-2.55269614, -1.07774478,  0.06428002],\n",
       "       [-2.46904499,  0.75193303,  1.14282281],\n",
       "       [-2.41396732,  0.09965137, -0.56629773],\n",
       "       [-2.35265144,  0.18463386, -1.60748323],\n",
       "       [-2.30478564,  0.91786195, -1.26088395],\n",
       "       [-2.28243452, -1.15099358,  0.11092259],\n",
       "       [-2.26777059,  1.35624003,  0.81252582],\n",
       "       [-2.16303684,  0.62962884,  0.81286212],\n",
       "       [-2.15445465,  0.06856297, -1.55066343],\n",
       "       [-2.06403288,  3.85273149,  0.51504769],\n",
       "       [-2.06388816, -0.51827022,  1.47789404],\n",
       "       [-1.97685526, -0.32206152, -0.78325329],\n",
       "       [-1.9090502 , -1.10633497, -0.18565898],\n",
       "       [-1.89957294,  1.76545424, -0.65332923],\n",
       "       [-1.89087327, -0.83921752,  0.93128012],\n",
       "       [-1.82410283,  1.6324113 , -1.24778318],\n",
       "       [-1.80770591,  0.61167629, -0.676922  ],\n",
       "       [-1.80195731, -0.21967189, -1.98756891],\n",
       "       [-1.72596534, -1.0708925 , -0.85715756],\n",
       "       [-1.44732011, -0.70766947,  1.26691115],\n",
       "       [-1.37190581, -2.6197451 ,  1.56464366],\n",
       "       [-1.31439294, -0.65160035,  0.04557184],\n",
       "       [-1.25262516,  0.8496021 , -0.58936476],\n",
       "       [-1.24713211, -1.1913035 ,  0.47383292],\n",
       "       [-1.17454654, -1.42474819,  0.0675282 ],\n",
       "       [-1.17231738,  0.13074058, -0.44004449],\n",
       "       [-1.13410607,  1.40279431, -0.90938745],\n",
       "       [-1.12973354, -0.70205309,  0.96864499],\n",
       "       [-1.04890007,  1.05712223, -0.46063877],\n",
       "       [-1.04801802, -0.20812225, -0.62269952],\n",
       "       [-1.01461185, -0.01349722,  1.85227818],\n",
       "       [-0.85948004, -1.32818605, -1.95967012],\n",
       "       [-0.84920563, -0.23413696, -0.23415337],\n",
       "       [-0.80182894,  0.31424733, -1.01283112],\n",
       "       [-0.75275929, -1.02438764, -3.24126734],\n",
       "       [-0.66793626,  0.01300189,  0.82718325],\n",
       "       [-0.43475389,  0.35778736, -1.1429703 ],\n",
       "       [-0.40832989,  0.97554513,  0.33126343],\n",
       "       [-0.35908504,  0.85243333, -0.66178646],\n",
       "       [-0.26358009, -0.47193187, -1.61271587],\n",
       "       [-0.16671045,  1.89679298,  0.82206016],\n",
       "       [-0.03722642, -0.29169375, -0.60063869],\n",
       "       [-0.02893854,  0.58831721, -0.1517851 ],\n",
       "       [ 0.08540663,  0.91540212, -0.50175704],\n",
       "       [ 0.12040813,  0.50498728, -0.11473644],\n",
       "       [ 0.13639698, -2.02514259,  0.63391902],\n",
       "       [ 0.14853859,  1.05380205,  1.08305124],\n",
       "       [ 0.2561765 ,  2.19045563,  0.58685709],\n",
       "       [ 0.28026168,  0.96337613, -0.82068232],\n",
       "       [ 0.36766319,  0.51503527, -0.93782504],\n",
       "       [ 0.55448741,  2.72016917, -0.26465683],\n",
       "       [ 0.58739987, -1.47852199, -0.3011037 ],\n",
       "       [ 0.59195091,  1.53803657,  0.36139561],\n",
       "       [ 0.60669007,  0.00511346,  0.26105527],\n",
       "       [ 0.64526911,  0.95400176,  1.13556564],\n",
       "       [ 0.67111737, -0.71435142,  0.29307247],\n",
       "       [ 0.73978876, -0.84679372, -0.07282891],\n",
       "       [ 0.82534483, -1.86726519,  2.31465857],\n",
       "       [ 0.97513371,  0.51326743, -0.5297602 ],\n",
       "       [ 1.10539816, -1.51936997,  1.03246526],\n",
       "       [ 1.24114406, -0.48536355, -0.23681861],\n",
       "       [ 1.24843547,  0.85639879, -0.44651495],\n",
       "       [ 1.27946872,  2.46324211,  0.06023021],\n",
       "       [ 1.37404301,  0.89959988, -0.6929096 ],\n",
       "       [ 1.37763707,  0.15372511, -0.88385744],\n",
       "       [ 1.39196365,  0.17136828,  0.73846658],\n",
       "       [ 1.56471029,  0.36163603,  1.0035329 ],\n",
       "       [ 1.62580308,  0.22745993, -1.23086432],\n",
       "       [ 1.62762208,  0.32408397, -1.76304016],\n",
       "       [ 1.63346862, -0.88951443, -0.75373616],\n",
       "       [ 1.65079694, -1.72491783, -1.91328024],\n",
       "       [ 1.71105577, -0.92693047,  0.77463405],\n",
       "       [ 1.81318188,  0.66213067,  0.11351735],\n",
       "       [ 1.85038409, -1.22084365,  0.82254491],\n",
       "       [ 1.89276857,  0.29698467,  0.52194157],\n",
       "       [ 1.97242505, -0.46947439,  0.76743473],\n",
       "       [ 1.99465584,  0.7870846 , -0.97468167],\n",
       "       [ 2.17862056,  0.46210347, -0.98150865],\n",
       "       [ 2.19705687, -0.29900735,  0.08704707],\n",
       "       [ 2.32327646, -0.03471177,  0.30154734],\n",
       "       [ 2.3689641 ,  0.75896922, -0.31526924],\n",
       "       [ 2.45592241, -0.42064532, -1.41537074],\n",
       "       [ 2.53124541, -1.46351495, -0.39210815],\n",
       "       [ 2.63699365,  0.17318093, -1.24573878],\n",
       "       [ 2.69331322,  0.71400049, -0.22346279],\n",
       "       [ 2.70428584, -0.46572975, -0.46341769],\n",
       "       [ 2.7937922 , -0.73036663,  0.67959775],\n",
       "       [ 2.81750777, -0.91942423,  0.47359243],\n",
       "       [ 2.81945911, -0.68002472,  0.34644821],\n",
       "       [ 2.92132162,  0.64768854, -0.1382643 ]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa3575",
   "metadata": {},
   "source": [
    "Below we can see the shape of the labels which has 100 target/labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b5f8f055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b2d3eea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.46571267],\n",
       "       [3.89702347],\n",
       "       [3.52095684],\n",
       "       [3.6171947 ],\n",
       "       [3.90165814],\n",
       "       [3.23072481],\n",
       "       [3.27410629],\n",
       "       [3.56556489],\n",
       "       [3.02434983],\n",
       "       [2.78713108],\n",
       "       [2.9951841 ],\n",
       "       [2.74026788],\n",
       "       [3.42934944],\n",
       "       [3.22295334],\n",
       "       [2.98463654],\n",
       "       [3.19377974],\n",
       "       [3.10729321],\n",
       "       [2.3628974 ],\n",
       "       [3.05894177],\n",
       "       [2.60542522],\n",
       "       [2.87336916],\n",
       "       [2.8732144 ],\n",
       "       [2.23118961],\n",
       "       [2.01466766],\n",
       "       [2.12476275],\n",
       "       [2.26668052],\n",
       "       [2.64420919],\n",
       "       [2.68229834],\n",
       "       [1.77046497],\n",
       "       [2.11079494],\n",
       "       [1.98656797],\n",
       "       [1.77152927],\n",
       "       [1.6517751 ],\n",
       "       [1.86815231],\n",
       "       [2.45814295],\n",
       "       [1.83804957],\n",
       "       [2.02778284],\n",
       "       [2.21143436],\n",
       "       [1.86482521],\n",
       "       [2.47293495],\n",
       "       [2.46255405],\n",
       "       [1.76165523],\n",
       "       [2.00861798],\n",
       "       [1.82051419],\n",
       "       [1.81540448],\n",
       "       [1.59202011],\n",
       "       [2.26931592],\n",
       "       [2.17771578],\n",
       "       [1.75686475],\n",
       "       [2.0498036 ],\n",
       "       [2.75545162],\n",
       "       [2.20302837],\n",
       "       [2.11637505],\n",
       "       [2.57850654],\n",
       "       [3.11330764],\n",
       "       [2.38775432],\n",
       "       [2.83170599],\n",
       "       [3.05060931],\n",
       "       [2.55717252],\n",
       "       [3.16346764],\n",
       "       [3.07599869],\n",
       "       [3.39222501],\n",
       "       [3.40068355],\n",
       "       [3.32650118],\n",
       "       [2.94374499],\n",
       "       [3.73161912],\n",
       "       [3.33421253],\n",
       "       [3.35246038],\n",
       "       [3.49135172],\n",
       "       [4.30724365],\n",
       "       [4.68892772],\n",
       "       [4.04431885],\n",
       "       [4.61008189],\n",
       "       [4.54453588],\n",
       "       [4.97175181],\n",
       "       [4.53511148],\n",
       "       [5.47980718],\n",
       "       [5.33415625],\n",
       "       [5.88892889],\n",
       "       [5.10509942],\n",
       "       [5.35442856],\n",
       "       [5.28838521],\n",
       "       [6.38168978],\n",
       "       [6.43968408],\n",
       "       [5.94199663],\n",
       "       [6.5776394 ],\n",
       "       [6.80120401],\n",
       "       [7.10701513],\n",
       "       [7.14023691],\n",
       "       [7.26393549],\n",
       "       [7.26806233],\n",
       "       [8.36891562],\n",
       "       [8.63526513],\n",
       "       [8.74696286],\n",
       "       [8.65931107],\n",
       "       [8.71007636],\n",
       "       [9.4223853 ],\n",
       "       [9.68379303],\n",
       "       [9.68122038],\n",
       "       [9.96825717]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38c84b",
   "metadata": {},
   "source": [
    "Below is each feature of the toy data plotted against the target. Notice that we have 3 features. However, only the 1st feature has a non-linear trend with the target. This means, features 2 and 3 don't provide any information about predicting the target `y`.\n",
    "\n",
    "Thus, even though our data has 3 features, there is only one informative feature which corresponds to feature 1. Our neural network will have to learn that feature 1 is the only informative feature and it will have to learn feature 1's non-linear mapping in order to predict the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0574f70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature1 = X[:, 0]\n",
    "feature2 = X[:, 1]\n",
    "feature3 = X[:, 2]\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "ax[0].plot(feature1, y, 'ob')\n",
    "ax[0].set_title(\"Feature 1 vs Target\")\n",
    "ax[0].set_xlabel(\"Feature 1\")\n",
    "ax[0].set_ylabel(\"Target y\")\n",
    "\n",
    "ax[1].plot(feature2, y,  'or')\n",
    "ax[1].set_title(\"Feature 2 vs Target\")\n",
    "ax[1].set_xlabel(\"Feature 2\")\n",
    "\n",
    "ax[2].plot(feature3, y, 'og')\n",
    "ax[2].set_title(\"Feature 3 vs Target\")\n",
    "ax[2].set_xlabel(\"Feature 3\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21af6a0",
   "metadata": {},
   "source": [
    "### Hidden Layer 1\n",
    "\n",
    "Alright, the first step in computing the predictions for a neural network is compute the output of the first layer. In our case, this is the first and only hidden layer. Below is the image of the 1st hidden layer and the input layer. The input layer will provide the inputs that the hidden layer will use to compute its outputs.\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51636177013_f3088e6720.jpg\" width=\"300\" height=\"209\" alt=\"signle_nn_hidden_layers\">\n",
    "\n",
    "*Note, the red arrow in the picture points in the direction information is flowing. In this case, information is flowing forwards.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cad60f",
   "metadata": {},
   "source": [
    "#### Initializing weights\n",
    "Before we compute the 1st hidden layers outputs, we need to initialize the weights and bias for the 1st hidden layer. \n",
    "\n",
    "The shape of the weights must always be (neurons, inputs). In the case of the hidden layer weight $\\Wm^{[1]}$, it's shape would be (hidden neurons=2, feature inputs=3) or (2, 3) as we have 2 hidden neurons and 3 feature inputs. We'll refer to the hidden layer's weights as `W1`.\n",
    "\n",
    "We can initialize the weights to using a random uniform distribution that range between -0.5 and 0.5. Now there is a whole research field dedicated to researching the best ways to initialize weights. However, for now, we'll just use a naive method that keeps our weight values small, but not too small. This will help prevent issues like overflow/underflow when dealing with many hidden neurons. \n",
    "\n",
    "The shape of the bias is always (neurons, 1). In the case of the hidden layer bias $\\bv^{[1]}$, it's shape would be (hidden neurons=2, 1) or (2,1) as we have 2 hidden neurons. Recall that a bias must be added to each neuron, therefore the bias is simply initialized as a vector of 1s. We'll refer to the hidden layer's bias as `b1`.\n",
    "\n",
    "Below is the code for initializing the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e4857246",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "n_input_features = X.shape[1]\n",
    "hidden_neurons = 2\n",
    "\n",
    "W1 = rng.uniform(low=-0.5, high=0.5, size=(hidden_neurons, n_input_features))\n",
    "b1 = np.ones([hidden_neurons, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5129c23f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0488135 ,  0.21518937,  0.10276338],\n",
       "       [ 0.04488318, -0.0763452 ,  0.14589411]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158a0ed",
   "metadata": {},
   "source": [
    "Below are the shapes for the inputs $\\Xm$ which corresponds to `X`, weights $\\Wm^{[1]}$ which corresponds to `W1`, and bias $\\bv^{[1]}$ which corresponds to `b1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5dbb9747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data X shape: (100, 3)\n",
      "Hidden layer weights W1 shape: (2, 3)\n",
      "Hidden layer bias b1 shape: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input data X shape: {X.shape}\")\n",
    "print(f\"Hidden layer weights W1 shape: {W1.shape}\")\n",
    "print(f\"Hidden layer bias b1 shape: {b1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5345a9ab",
   "metadata": {},
   "source": [
    "Notice anything wrong here with the input features? Recall, the input layer requires the input data to be of shape (features, data samples). Thus, we need to transpose our data to be a (features=3, data samples=100) instead of (data samples=100, features=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1a903e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data X.T shape: (3, 100)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input data X.T shape: {X.T.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c0d10",
   "metadata": {},
   "source": [
    "#### Computing $\\Zm^{[1]}$\n",
    "\n",
    "The first component we must compute for each neuron is $\\Zm^{[1]}$ which contains the linear combination of inputs for all the neurons in the 1st hidden layer. We can do so using the following equation:\n",
    "$$\n",
    "\\Zm^{[1]} = \\Wm^{[1]} \\Xm + \\bv^{[1]}.\n",
    "$$\n",
    "where $\\Zm^{[1]}$ has the shape (neurons=2, data samples=100).\n",
    "\n",
    "Turning this into code looks like the below. Notice, input features `X` are transposed to be the correct shape of (features=3, data samples=100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "503ce014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100) =  (2, 3) @ (3, 100) + (2, 1)\n",
      "Continuous neuron outputs Z1 shape: (2, 100)\n"
     ]
    }
   ],
   "source": [
    "Z1 = W1 @ X.T + b1\n",
    "print(f\"{Z1.shape} =  {W1.shape} @ {X.T.shape} + {b1.shape}\")\n",
    "print(f\"Continuous neuron outputs Z1 shape: {Z1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02c681",
   "metadata": {},
   "source": [
    "Here, $\\Zm^{[1]}$ corresponds to `Z1`. `Z1` is a matrix where the rows correspond to the neurons and the columns correspond to the linear combination (i.e. continuous output) for each data sample. Indexing the first row of `Z1` provides us with the linear combinations for the 1st hidden neuron in 1st hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d76d84dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92066586, 0.96233472, 0.74385258, 0.67779535, 0.67147808,\n",
       "       0.82050429, 1.20237508, 1.04243346, 0.45206239, 1.10664388,\n",
       "       0.65008037, 1.15872559, 0.84541504, 0.75969968, 0.95543678,\n",
       "       0.65230357, 1.2646485 , 1.11343648, 0.75023613, 1.78124221,\n",
       "       0.93960153, 0.7537088 , 0.64966211, 1.22004385, 0.82281065,\n",
       "       1.13401029, 0.97382298, 0.6605198 , 0.5972205 , 0.90726036,\n",
       "       0.53007925, 0.80030553, 1.0611152 , 0.73145994, 0.64301503,\n",
       "       0.92568861, 1.153055  , 0.89332062, 1.12894418, 0.84006617,\n",
       "       1.13791514, 0.47085194, 0.88410112, 0.92440066, 0.40973428,\n",
       "       1.0551977 , 0.93831469, 1.22403667, 1.09789898, 0.71985089,\n",
       "       1.48450964, 0.8736898 , 1.10958907, 1.14959155, 1.10275473,\n",
       "       0.63601252, 1.34531569, 1.54417505, 1.13665277, 1.03240297,\n",
       "       1.58522092, 0.6795684 , 1.39700254, 1.05754195, 1.35348344,\n",
       "       0.90915588, 0.84640653, 0.87633449, 1.10360944, 0.83310571,\n",
       "       0.93180324, 1.19934304, 1.59870832, 1.18945045, 1.00949913,\n",
       "       1.18071057, 1.25732565, 1.00182033, 0.9680134 , 0.81086481,\n",
       "       0.51278207, 0.96366106, 1.24265667, 0.91213865, 1.20993709,\n",
       "       1.07411927, 1.1665768 , 1.10492271, 1.05184809, 1.13692568,\n",
       "       1.24656141, 0.8839155 , 0.76833175, 1.03797127, 1.26215158,\n",
       "       0.98416321, 1.04904541, 0.98835007, 1.02689578, 1.26776712])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a2103",
   "metadata": {},
   "source": [
    "Indexing the second row of `Z1` provides us with the linear combinations for the 2nd hidden neuron in 1st hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "d9d8c8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.82954355, 0.57260803, 0.76746166, 1.08310985, 0.7279515 ,\n",
       "       0.78496234, 0.53755546, 0.56452461, 1.01849654, 1.14714585,\n",
       "       0.97708559, 0.99850604, 0.80142605, 0.64578727, 0.64252398,\n",
       "       1.00161286, 0.91321555, 0.97343868, 0.6718341 , 0.6883645 ,\n",
       "       1.16254961, 0.82158825, 0.97169256, 0.68464027, 1.11507011,\n",
       "       0.61145747, 0.77340692, 0.64591871, 0.87923644, 1.17390171,\n",
       "       1.36670176, 0.99740108, 0.7929303 , 1.10410448, 1.06590727,\n",
       "       0.87320135, 0.70932682, 1.14421195, 0.80501133, 0.87800256,\n",
       "       1.22572792, 0.7769201 , 0.94559858, 0.79225399, 0.57153902,\n",
       "       1.08970943, 0.78641888, 0.95552405, 0.82225318, 0.78891367,\n",
       "       0.9676402 , 0.93296893, 0.9316414 , 0.86074336, 0.95011157,\n",
       "       1.2532169 , 1.08422496, 0.92988624, 0.81929717, 0.84035827,\n",
       "       0.77860343, 1.09531318, 0.96187242, 1.06492622, 1.12180062,\n",
       "       1.12741673, 1.0872274 , 1.5172959 , 0.9272927 , 1.316241  ,\n",
       "       1.05821123, 0.92550792, 0.87815715, 0.89189986, 0.92114697,\n",
       "       1.15713054, 1.18902955, 0.87602989, 0.79109342, 1.03125976,\n",
       "       0.92664589, 1.26057887, 1.04739239, 1.29626114, 1.13842833,\n",
       "       1.23633504, 0.88723605, 0.91930771, 1.13413834, 1.1509201 ,\n",
       "       1.00238707, 0.93584961, 1.16813642, 0.92338918, 1.03377205,\n",
       "       1.08932327, 1.28030358, 1.26574669, 1.22900768, 1.06149835])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ebc45",
   "metadata": {},
   "source": [
    "#### Computing $\\Am^{[1]}$\n",
    "\n",
    "Next, we need to apply the activation function to the linear combination outputs $\\Zm^{[1]}$ for each neuron. Doing so will give us the neuron outputs $\\Am^{[1]}$ for the 1st hidden layer. For now, we are just going to use the sigmoid activation function.\n",
    "\n",
    "$$\n",
    "\\Am^{[1]} = g(\\Zm^{[1]})\n",
    "$$\n",
    "\n",
    "We can simply get $\\Am^{[1]}$  by calling the `Sigmoid.activation()` method. As `Sigmoid.activation()` is a static method, this means we DO NOT have to initialize the `Sigmoid` class in order to call the `activation()` static method!\n",
    "\n",
    "Below is the code for applying the sigmoid activation function to `Z1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "243c4052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation neuron outputs A1 shape: (2, 100)\n"
     ]
    }
   ],
   "source": [
    "A1 = Sigmoid.activation(Z1)\n",
    "print(f\"Activation neuron outputs A1 shape: {A1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352a7b4",
   "metadata": {},
   "source": [
    "Here, 1st layer activations $\\Am^{[1]}$ corresponds to `A1`. `A1` is a matrix where the rows correspond to the neurons and the columns correspond to the activations for each data sample. Indexing the first row of `A1` provides us with the activation outputs for the 1st hidden neuron in 1st hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "89b52eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71517776, 0.72358901, 0.67783773, 0.66324647, 0.66183405,\n",
       "       0.69434338, 0.76894703, 0.73931927, 0.61112947, 0.7515029 ,\n",
       "       0.65702857, 0.76110107, 0.69960446, 0.68128853, 0.72220724,\n",
       "       0.65752938, 0.77982528, 0.75276922, 0.67923015, 0.85585019,\n",
       "       0.71901916, 0.67998629, 0.65693431, 0.77207127, 0.69483264,\n",
       "       0.75657822, 0.72588084, 0.65937715, 0.64502014, 0.71243922,\n",
       "       0.62950159, 0.69003983, 0.7429036 , 0.67512556, 0.65543469,\n",
       "       0.71619978, 0.76006848, 0.70957496, 0.755644  , 0.69847915,\n",
       "       0.75729665, 0.61558538, 0.70767135, 0.71593792, 0.60102416,\n",
       "       0.74177175, 0.71875911, 0.77277315, 0.74986623, 0.67257418,\n",
       "       0.81525277, 0.70551289, 0.75205249, 0.7594363 , 0.7507759 ,\n",
       "       0.65385153, 0.79336275, 0.82407083, 0.75706455, 0.7373815 ,\n",
       "       0.82994266, 0.66364236, 0.80170781, 0.74222053, 0.79469854,\n",
       "       0.7128274 , 0.69981279, 0.70606206, 0.75093579, 0.69701122,\n",
       "       0.71744098, 0.76840789, 0.83183778, 0.76664276, 0.73292212,\n",
       "       0.76507554, 0.77856539, 0.73141633, 0.72472335, 0.69229376,\n",
       "       0.62545843, 0.72385421, 0.77602611, 0.7134376 , 0.77028782,\n",
       "       0.7453795 , 0.7625257 , 0.75118134, 0.74112963, 0.75711474,\n",
       "       0.77670406, 0.70763295, 0.68315991, 0.73845837, 0.77939627,\n",
       "       0.72793351, 0.74059155, 0.72876191, 0.73631364, 0.78036028])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2027e",
   "metadata": {},
   "source": [
    "Indexing the second row of `A1` provides us with the activation outputs for the 2nd hidden neuron in 1st hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b5fb731b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69625841, 0.63936475, 0.68297154, 0.74708204, 0.67435558,\n",
       "       0.68674863, 0.63124357, 0.6374988 , 0.73467964, 0.75898921,\n",
       "       0.72652955, 0.73076475, 0.69027945, 0.65606051, 0.65532379,\n",
       "       0.73137557, 0.71365771, 0.72580437, 0.66191372, 0.665603  ,\n",
       "       0.76179568, 0.69457338, 0.72545673, 0.66477357, 0.75307313,\n",
       "       0.6482732 , 0.68425742, 0.65609017, 0.70666397, 0.76384954,\n",
       "       0.79684675, 0.73054729, 0.68846018, 0.75102837, 0.74381781,\n",
       "       0.7054114 , 0.67025239, 0.75845212, 0.69104543, 0.70640813,\n",
       "       0.77306999, 0.68501594, 0.72022916, 0.6883151 , 0.63911822,\n",
       "       0.748327  , 0.68706188, 0.72222475, 0.69471442, 0.68759803,\n",
       "       0.72464889, 0.71767723, 0.71740817, 0.70281594, 0.72113762,\n",
       "       0.77785623, 0.74729268, 0.7170522 , 0.69408713, 0.69854067,\n",
       "       0.68537904, 0.74938091, 0.72349654, 0.74363083, 0.75432256,\n",
       "       0.75536185, 0.74785926, 0.82013994, 0.71652571, 0.78855563,\n",
       "       0.74234856, 0.71616305, 0.70644019, 0.70928208, 0.71527575,\n",
       "       0.76081093, 0.76656745, 0.70599884, 0.68806606, 0.73716005,\n",
       "       0.71639432, 0.77912574, 0.74027385, 0.78520507, 0.75739096,\n",
       "       0.77492543, 0.70831946, 0.71490103, 0.75660181, 0.75967894,\n",
       "       0.73152765, 0.71826054, 0.762808  , 0.71573217, 0.73764653,\n",
       "       0.74825427, 0.78250145, 0.78001378, 0.77364485, 0.74297678])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7723e8f",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "\n",
    "The final step in computing the predictions for a neural network is to compute the output of the 2nd layer (i.e., output layer). Below is the image of the 1st hidden layer and the output layer. The 1st hidden layer will provide the inputs that the output layer will use to compute its SINGLE output per data sample.\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51636813390_2a66fa526a.jpg\" width=\"300\" height=\"239\" alt=\"signle_nn_output_layer\">\n",
    "\n",
    "*Note, the red arrow in the picture points in the direction information is flowing. In this case, information is flowing forwards.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c860d212",
   "metadata": {},
   "source": [
    "#### Initializing weights\n",
    "\n",
    "Before we compute the output layer output, we need to initialize the weights and biases for the layer. \n",
    "\n",
    "The shape of the weights must always be (neurons, inputs). In the case of the output layer weights $\\wv^{[2]}$, it's would be (output neurons=1, hidden neuron inputs=2) or (1, 2) as we have 1 output neuron and 2 hidden neurons from the 1st hidden layer acting as inputs. We can initialize the weights to random numbers for now. We'll refer to the output layer's weights as `W2`.\n",
    "\n",
    "The shape of the bias is always (neurons, 1). In the case of the output layer bias $\\bv^{[2]}$, it's shape would be (output neurons=1, 1) or (1,1) as we have 1 output neuron. Recall that a bias must be added to each neuron, therefore the bias simply initialized as a vector of 1s. We'll refer to the output layer's bias as `b2`.\n",
    "\n",
    "Below is the code for initializing the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cda5534f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_neurons = 1\n",
    "\n",
    "W2 = rng.uniform(low=-0.5, high=0.5, size=(output_neurons, hidden_neurons))\n",
    "b2 = np.ones([output_neurons, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b77621c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06241279,  0.391773  ]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196f7f3a",
   "metadata": {},
   "source": [
    "Below are the shapes for the inputs $\\Am^{[1]}$ which corresponds to `A1`, weights $\\wv^{[2]}$ which corresponds to `W2`, and bias $\\bv^{[2]}$ which corresponds to `b2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5f38f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input activations A1 shape: (2, 100)\n",
      "Output layer weights shape: (1, 2)\n",
      "Output layer bias shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input activations A1 shape: {A1.shape}\")\n",
    "print(f\"Output layer weights shape: {W2.shape}\")\n",
    "print(f\"Output layer bias shape: {b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a375d",
   "metadata": {},
   "source": [
    "#### Computing $\\zv^{[2]}$\n",
    "\n",
    "Like always, we first compute have to compute are the linear combinations $\\zv^{[2]}$ for each neuron in the output layer using the outputs from the previous layer's neurons $\\Am^{[1]}$. To do so, we use the following equation:\n",
    "$$\n",
    "\\zv^{[2]} = \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}\n",
    "$$\n",
    "where $\\zv^{[2]}$ s a vector as there is only 1 output neuron and $z$ always has the shape (neurons, data samples).\n",
    "\n",
    "This equation turned into code is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ff55df2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100) =  (1, 2) @ (2, 100) + (1, 1)\n",
      "Continuous neuron outputs Z2 shape: (1, 100)\n"
     ]
    }
   ],
   "source": [
    "Z2 = W2 @ A1 + b2\n",
    "print(f\"{Z2.shape} =  {W2.shape} @ {A1.shape} + {b2.shape}\")\n",
    "print(f\"Continuous neuron outputs Z2 shape: {Z2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41df6f10",
   "metadata": {},
   "source": [
    "Here, $\\zv^{[2]}$ corresponds to `Z2`. `Z2` is a vector where the row correspond to the neuron and the columns correspond to the linear combination for each data sample. Indexing the first row of `Z2` provides us with the linear combinations for the only neuron in output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7e78bbb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.22813901, 1.20532464, 1.22526407, 1.25129151, 1.2228874 ,\n",
       "       1.22571366, 1.19931206, 1.20361184, 1.24968535, 1.25044809,\n",
       "       1.24362768, 1.23879146, 1.22676858, 1.21450568, 1.2116632 ,\n",
       "       1.24549496, 1.23092075, 1.23736813, 1.21692728, 1.20734929,\n",
       "       1.25357499, 1.22967526, 1.24321326, 1.21225322, 1.25166728,\n",
       "       1.20675578, 1.22276933, 1.21588485, 1.23659436, 1.25479031,\n",
       "       1.27289409, 1.2431414 , 1.22335342, 1.25209617, 1.25050023,\n",
       "       1.23166111, 1.2151488 , 1.25285451, 1.22357109, 1.2331576 ,\n",
       "       1.25560295, 1.22995035, 1.2379986 , 1.22497959, 1.21287767,\n",
       "       1.24687827, 1.22431253, 1.23471723, 1.22536911, 1.22740511,\n",
       "       1.23301567, 1.23713353, 1.23412346, 1.22794577, 1.23566423,\n",
       "       1.26393437, 1.24325312, 1.22948914, 1.22467409, 1.22764734,\n",
       "       1.21671397, 1.25216744, 1.23340959, 1.24501043, 1.24592386,\n",
       "       1.25144083, 1.2493138 , 1.27724138, 1.23384743, 1.26543239,\n",
       "       1.24605463, 1.23261487, 1.22484688, 1.23002926, 1.23448201,\n",
       "       1.25031468, 1.25172799, 1.23094155, 1.2243337 , 1.24559142,\n",
       "       1.24162735, 1.26006267, 1.24158535, 1.26309452, 1.24864952,\n",
       "       1.25707365, 1.22990909, 1.2331956 , 1.25016019, 1.25036805,\n",
       "       1.23811651, 1.23722974, 1.25620966, 1.23431529, 1.2403457 ,\n",
       "       1.24771346, 1.26034056, 1.26010428, 1.25713778, 1.24237378])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f38761",
   "metadata": {},
   "source": [
    "#### Computing $\\av^{[2]}$\n",
    "\n",
    "Next, we need to apply the activation function to the linear combination outputs $\\zv^{[2]}$ for each neuron. Doing so will give us the neuron outputs $\\av^{[2]}$ for the output layer.  For now, we are just going to use the identity/linear activation function.\n",
    "\n",
    "$$\n",
    "\\av^{[2]} = g(\\zv^{[2]})\n",
    "$$\n",
    "\n",
    "As we are performing regression no activation function is used for the output neurons. This is because we want the network to predict any continuous values between negative infinity and positive infinity. However, we can simply apply the identity or linear activation which returns the input as output giving off the guise that no activation function was applied! \n",
    "\n",
    "Below is the code for applying the linear activation function to `Z2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0cf85cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation neuron outputs A2 shape: (1, 100)\n"
     ]
    }
   ],
   "source": [
    "A2 = Linear.activation(Z2)\n",
    "print(f\"Activation neuron outputs A2 shape: {A2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e8fff",
   "metadata": {},
   "source": [
    "Here, the output layer activation $\\av^{[2]}$ corresponds to `A1`. `A1` is a matrix where the rows correspond to the output neuron and the columns correspond to the activations for each data sample. Indexing the first row of `A1` provides us with the activation outputs for the only output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8c0b6b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.22813901, 1.20532464, 1.22526407, 1.25129151, 1.2228874 ,\n",
       "       1.22571366, 1.19931206, 1.20361184, 1.24968535, 1.25044809,\n",
       "       1.24362768, 1.23879146, 1.22676858, 1.21450568, 1.2116632 ,\n",
       "       1.24549496, 1.23092075, 1.23736813, 1.21692728, 1.20734929,\n",
       "       1.25357499, 1.22967526, 1.24321326, 1.21225322, 1.25166728,\n",
       "       1.20675578, 1.22276933, 1.21588485, 1.23659436, 1.25479031,\n",
       "       1.27289409, 1.2431414 , 1.22335342, 1.25209617, 1.25050023,\n",
       "       1.23166111, 1.2151488 , 1.25285451, 1.22357109, 1.2331576 ,\n",
       "       1.25560295, 1.22995035, 1.2379986 , 1.22497959, 1.21287767,\n",
       "       1.24687827, 1.22431253, 1.23471723, 1.22536911, 1.22740511,\n",
       "       1.23301567, 1.23713353, 1.23412346, 1.22794577, 1.23566423,\n",
       "       1.26393437, 1.24325312, 1.22948914, 1.22467409, 1.22764734,\n",
       "       1.21671397, 1.25216744, 1.23340959, 1.24501043, 1.24592386,\n",
       "       1.25144083, 1.2493138 , 1.27724138, 1.23384743, 1.26543239,\n",
       "       1.24605463, 1.23261487, 1.22484688, 1.23002926, 1.23448201,\n",
       "       1.25031468, 1.25172799, 1.23094155, 1.2243337 , 1.24559142,\n",
       "       1.24162735, 1.26006267, 1.24158535, 1.26309452, 1.24864952,\n",
       "       1.25707365, 1.22990909, 1.2331956 , 1.25016019, 1.25036805,\n",
       "       1.23811651, 1.23722974, 1.25620966, 1.23431529, 1.2403457 ,\n",
       "       1.24771346, 1.26034056, 1.26010428, 1.25713778, 1.24237378])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf467b",
   "metadata": {},
   "source": [
    "Also notice `A2` is equal to `Z2` as the linear function simply returns whatever the input is as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fa0b394f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(A2 == Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca386a5a",
   "metadata": {},
   "source": [
    "Finally, we can say that `A2` contains the predictions made by the neural network. We can therefore say $\\hat{\\yv} = \\av^{[2]\\top}$. Notice, we need to transpose the predictions to get them to match the shape of the ground truth `y` which has the shape (data samples, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "723c8f43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.22813901],\n",
       "       [1.20532464],\n",
       "       [1.22526407],\n",
       "       [1.25129151],\n",
       "       [1.2228874 ],\n",
       "       [1.22571366],\n",
       "       [1.19931206],\n",
       "       [1.20361184],\n",
       "       [1.24968535],\n",
       "       [1.25044809],\n",
       "       [1.24362768],\n",
       "       [1.23879146],\n",
       "       [1.22676858],\n",
       "       [1.21450568],\n",
       "       [1.2116632 ],\n",
       "       [1.24549496],\n",
       "       [1.23092075],\n",
       "       [1.23736813],\n",
       "       [1.21692728],\n",
       "       [1.20734929],\n",
       "       [1.25357499],\n",
       "       [1.22967526],\n",
       "       [1.24321326],\n",
       "       [1.21225322],\n",
       "       [1.25166728],\n",
       "       [1.20675578],\n",
       "       [1.22276933],\n",
       "       [1.21588485],\n",
       "       [1.23659436],\n",
       "       [1.25479031],\n",
       "       [1.27289409],\n",
       "       [1.2431414 ],\n",
       "       [1.22335342],\n",
       "       [1.25209617],\n",
       "       [1.25050023],\n",
       "       [1.23166111],\n",
       "       [1.2151488 ],\n",
       "       [1.25285451],\n",
       "       [1.22357109],\n",
       "       [1.2331576 ],\n",
       "       [1.25560295],\n",
       "       [1.22995035],\n",
       "       [1.2379986 ],\n",
       "       [1.22497959],\n",
       "       [1.21287767],\n",
       "       [1.24687827],\n",
       "       [1.22431253],\n",
       "       [1.23471723],\n",
       "       [1.22536911],\n",
       "       [1.22740511],\n",
       "       [1.23301567],\n",
       "       [1.23713353],\n",
       "       [1.23412346],\n",
       "       [1.22794577],\n",
       "       [1.23566423],\n",
       "       [1.26393437],\n",
       "       [1.24325312],\n",
       "       [1.22948914],\n",
       "       [1.22467409],\n",
       "       [1.22764734],\n",
       "       [1.21671397],\n",
       "       [1.25216744],\n",
       "       [1.23340959],\n",
       "       [1.24501043],\n",
       "       [1.24592386],\n",
       "       [1.25144083],\n",
       "       [1.2493138 ],\n",
       "       [1.27724138],\n",
       "       [1.23384743],\n",
       "       [1.26543239],\n",
       "       [1.24605463],\n",
       "       [1.23261487],\n",
       "       [1.22484688],\n",
       "       [1.23002926],\n",
       "       [1.23448201],\n",
       "       [1.25031468],\n",
       "       [1.25172799],\n",
       "       [1.23094155],\n",
       "       [1.2243337 ],\n",
       "       [1.24559142],\n",
       "       [1.24162735],\n",
       "       [1.26006267],\n",
       "       [1.24158535],\n",
       "       [1.26309452],\n",
       "       [1.24864952],\n",
       "       [1.25707365],\n",
       "       [1.22990909],\n",
       "       [1.2331956 ],\n",
       "       [1.25016019],\n",
       "       [1.25036805],\n",
       "       [1.23811651],\n",
       "       [1.23722974],\n",
       "       [1.25620966],\n",
       "       [1.23431529],\n",
       "       [1.2403457 ],\n",
       "       [1.24771346],\n",
       "       [1.26034056],\n",
       "       [1.26010428],\n",
       "       [1.25713778],\n",
       "       [1.24237378]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = A2.T\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418b776",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "We can put all the code together by putting the weight and bias initializations into a function called `init_weights()` and the forward pass code into a function called `forward()`.\n",
    "\n",
    "The `init_weights()` function will simply initialize the weights and biases for each layer for us and return them in a `weights` dictionary and a `bias` dictionary. \n",
    "\n",
    "**Inputs**: The `init_weights()` function takes in the following:\n",
    "- `n_input_features` number of input features\n",
    "- `hidden_neurons` the number of hidden neurons or units to use in the 1st hidden layer\n",
    "- `output_neurons` the number of output neurons to use in the output layer .\n",
    "\n",
    "**Outputs**: Returns the weights and biases for each layer which are formated as dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "73515f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(n_input_features, hidden_neurons, output_neurons, seed=0):\n",
    "    weights = {}\n",
    "    bias = {}\n",
    "    \n",
    "    rng = np.random.RandomState(seed)\n",
    "    weights['W1'] = rng.uniform(low=-0.5, high=0.5, size=(hidden_neurons, n_input_features))\n",
    "    weights['W2'] = rng.uniform(low=-0.5, high=0.5, size=(output_neurons, hidden_neurons))\n",
    "\n",
    "    \n",
    "    bias['b1'] = np.ones([hidden_neurons, 1])\n",
    "    bias['b2'] = np.ones([output_neurons, 1])\n",
    "    \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc79aa",
   "metadata": {},
   "source": [
    "\n",
    "The `forward()` function compute the outputs for each layer and then returns the predictions of the neural network. Notice, we store the transposed feature inputs `X.T` into `As['A0']`. For generalization purposes, feature inputs $\\Xm$ are often also denoted as $\\Am^{[0]}$.\n",
    "\n",
    "**Inputs**: The `forward()` function also takes in following:\n",
    "\n",
    "- `X` which corresponds to the input features\n",
    "- `weights` which corresponds to the `weights` dictionary\n",
    "- `bias` which corresponds to the `bias` dictionary\n",
    "\n",
    "**Outputs**: Returns predictions `y_hat` as a NumPy array along with the linear combinations for each layer $\\Zm$ as the dictionary `Zs` and activations for each layer $\\Am$ as the dictionary `As`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b3c4cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, weights, bias):\n",
    "    Zs = {}\n",
    "    As = {}\n",
    "    \n",
    "    As['A0'] = X.T\n",
    "    # Forward pass\n",
    "    Zs['Z1'] = weights['W1'] @ As['A0'] + bias['b1']\n",
    "    As['A1'] = Sigmoid.activation(Zs['Z1'])\n",
    "    Zs['Z2'] = weights['W2'] @ As['A1'] + bias['b2']\n",
    "    As['A2'] = Linear.activation(Zs['Z2'])\n",
    "    \n",
    "    y_hat = As['A2'].T\n",
    "    return y_hat, Zs, As"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90455edf",
   "metadata": {},
   "source": [
    "Below we call the `init_weights()` function passing the number of features, 2 hidden neurons, and 1 output neuron to replicate the structure our neural network. Further, we call the `forward()` function passing the data, weights, and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3b86f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias = init_weights(\n",
    "    n_input_features=X.shape[1],\n",
    "    hidden_neurons=2,\n",
    "    output_neurons=1\n",
    ")\n",
    "\n",
    "y_hat, Zs, As = forward(X, weights=weights, bias=bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026c30c",
   "metadata": {},
   "source": [
    "As we are looking at a non-linear regression problem, we also define the mean squared error (MSE) loss function and sum of square errors (SSE) cost function below. Recall, we'll need a cost/loss function to assess the performance of the network and for gradient descent to update the weights (see next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f89d5c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_labels(y):\n",
    "    if len(y.shape) != 2:\n",
    "        y = y.reshape(-1, 1)\n",
    "    return y\n",
    "\n",
    "def sse(y, y_hat):\n",
    "    y = reshape_labels(y)\n",
    "    y_hat = reshape_labels(y_hat)\n",
    "    \n",
    "    sqrd_err = (y_hat - y)**2\n",
    "    \n",
    "    return np.sum(sqrd_err)\n",
    "\n",
    "def mse(y, y_hat):\n",
    "    y = reshape_labels(y)\n",
    "    y_hat = reshape_labels(y_hat)\n",
    "    \n",
    "    sqrd_err = (y_hat - y)**2\n",
    "    \n",
    "    return np.mean(sqrd_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161da405",
   "metadata": {},
   "source": [
    "Below, you can see the output for the the SSE, MSE, `y_hat`, `weights`, `bias`, `Zs`, and `As`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "db769835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1183.8792691026304"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sse(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "91a200c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.838792691026304"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "47a6cae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1539904b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.0488135 ,  0.21518937,  0.10276338],\n",
       "        [ 0.04488318, -0.0763452 ,  0.14589411]]),\n",
       " 'W2': array([[-0.06241279,  0.391773  ]])}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "137a9e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b1': array([[1.],\n",
       "        [1.]]),\n",
       " 'b2': array([[1.]])}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ceda5dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Z1': array([[0.92066586, 0.96233472, 0.74385258, 0.67779535, 0.67147808,\n",
       "         0.82050429, 1.20237508, 1.04243346, 0.45206239, 1.10664388,\n",
       "         0.65008037, 1.15872559, 0.84541504, 0.75969968, 0.95543678,\n",
       "         0.65230357, 1.2646485 , 1.11343648, 0.75023613, 1.78124221,\n",
       "         0.93960153, 0.7537088 , 0.64966211, 1.22004385, 0.82281065,\n",
       "         1.13401029, 0.97382298, 0.6605198 , 0.5972205 , 0.90726036,\n",
       "         0.53007925, 0.80030553, 1.0611152 , 0.73145994, 0.64301503,\n",
       "         0.92568861, 1.153055  , 0.89332062, 1.12894418, 0.84006617,\n",
       "         1.13791514, 0.47085194, 0.88410112, 0.92440066, 0.40973428,\n",
       "         1.0551977 , 0.93831469, 1.22403667, 1.09789898, 0.71985089,\n",
       "         1.48450964, 0.8736898 , 1.10958907, 1.14959155, 1.10275473,\n",
       "         0.63601252, 1.34531569, 1.54417505, 1.13665277, 1.03240297,\n",
       "         1.58522092, 0.6795684 , 1.39700254, 1.05754195, 1.35348344,\n",
       "         0.90915588, 0.84640653, 0.87633449, 1.10360944, 0.83310571,\n",
       "         0.93180324, 1.19934304, 1.59870832, 1.18945045, 1.00949913,\n",
       "         1.18071057, 1.25732565, 1.00182033, 0.9680134 , 0.81086481,\n",
       "         0.51278207, 0.96366106, 1.24265667, 0.91213865, 1.20993709,\n",
       "         1.07411927, 1.1665768 , 1.10492271, 1.05184809, 1.13692568,\n",
       "         1.24656141, 0.8839155 , 0.76833175, 1.03797127, 1.26215158,\n",
       "         0.98416321, 1.04904541, 0.98835007, 1.02689578, 1.26776712],\n",
       "        [0.82954355, 0.57260803, 0.76746166, 1.08310985, 0.7279515 ,\n",
       "         0.78496234, 0.53755546, 0.56452461, 1.01849654, 1.14714585,\n",
       "         0.97708559, 0.99850604, 0.80142605, 0.64578727, 0.64252398,\n",
       "         1.00161286, 0.91321555, 0.97343868, 0.6718341 , 0.6883645 ,\n",
       "         1.16254961, 0.82158825, 0.97169256, 0.68464027, 1.11507011,\n",
       "         0.61145747, 0.77340692, 0.64591871, 0.87923644, 1.17390171,\n",
       "         1.36670176, 0.99740108, 0.7929303 , 1.10410448, 1.06590727,\n",
       "         0.87320135, 0.70932682, 1.14421195, 0.80501133, 0.87800256,\n",
       "         1.22572792, 0.7769201 , 0.94559858, 0.79225399, 0.57153902,\n",
       "         1.08970943, 0.78641888, 0.95552405, 0.82225318, 0.78891367,\n",
       "         0.9676402 , 0.93296893, 0.9316414 , 0.86074336, 0.95011157,\n",
       "         1.2532169 , 1.08422496, 0.92988624, 0.81929717, 0.84035827,\n",
       "         0.77860343, 1.09531318, 0.96187242, 1.06492622, 1.12180062,\n",
       "         1.12741673, 1.0872274 , 1.5172959 , 0.9272927 , 1.316241  ,\n",
       "         1.05821123, 0.92550792, 0.87815715, 0.89189986, 0.92114697,\n",
       "         1.15713054, 1.18902955, 0.87602989, 0.79109342, 1.03125976,\n",
       "         0.92664589, 1.26057887, 1.04739239, 1.29626114, 1.13842833,\n",
       "         1.23633504, 0.88723605, 0.91930771, 1.13413834, 1.1509201 ,\n",
       "         1.00238707, 0.93584961, 1.16813642, 0.92338918, 1.03377205,\n",
       "         1.08932327, 1.28030358, 1.26574669, 1.22900768, 1.06149835]]),\n",
       " 'Z2': array([[1.22813901, 1.20532464, 1.22526407, 1.25129151, 1.2228874 ,\n",
       "         1.22571366, 1.19931206, 1.20361184, 1.24968535, 1.25044809,\n",
       "         1.24362768, 1.23879146, 1.22676858, 1.21450568, 1.2116632 ,\n",
       "         1.24549496, 1.23092075, 1.23736813, 1.21692728, 1.20734929,\n",
       "         1.25357499, 1.22967526, 1.24321326, 1.21225322, 1.25166728,\n",
       "         1.20675578, 1.22276933, 1.21588485, 1.23659436, 1.25479031,\n",
       "         1.27289409, 1.2431414 , 1.22335342, 1.25209617, 1.25050023,\n",
       "         1.23166111, 1.2151488 , 1.25285451, 1.22357109, 1.2331576 ,\n",
       "         1.25560295, 1.22995035, 1.2379986 , 1.22497959, 1.21287767,\n",
       "         1.24687827, 1.22431253, 1.23471723, 1.22536911, 1.22740511,\n",
       "         1.23301567, 1.23713353, 1.23412346, 1.22794577, 1.23566423,\n",
       "         1.26393437, 1.24325312, 1.22948914, 1.22467409, 1.22764734,\n",
       "         1.21671397, 1.25216744, 1.23340959, 1.24501043, 1.24592386,\n",
       "         1.25144083, 1.2493138 , 1.27724138, 1.23384743, 1.26543239,\n",
       "         1.24605463, 1.23261487, 1.22484688, 1.23002926, 1.23448201,\n",
       "         1.25031468, 1.25172799, 1.23094155, 1.2243337 , 1.24559142,\n",
       "         1.24162735, 1.26006267, 1.24158535, 1.26309452, 1.24864952,\n",
       "         1.25707365, 1.22990909, 1.2331956 , 1.25016019, 1.25036805,\n",
       "         1.23811651, 1.23722974, 1.25620966, 1.23431529, 1.2403457 ,\n",
       "         1.24771346, 1.26034056, 1.26010428, 1.25713778, 1.24237378]])}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e7f8eac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A0': array([[-2.9668673 , -2.87649303, -2.84748524, -2.79366887, -2.72863627,\n",
       "         -2.72129752, -2.65149833, -2.6186499 , -2.60969044, -2.55573209,\n",
       "         -2.55269614, -2.46904499, -2.41396732, -2.35265144, -2.30478564,\n",
       "         -2.28243452, -2.26777059, -2.16303684, -2.15445465, -2.06403288,\n",
       "         -2.06388816, -1.97685526, -1.9090502 , -1.89957294, -1.89087327,\n",
       "         -1.82410283, -1.80770591, -1.80195731, -1.72596534, -1.44732011,\n",
       "         -1.37190581, -1.31439294, -1.25262516, -1.24713211, -1.17454654,\n",
       "         -1.17231738, -1.13410607, -1.12973354, -1.04890007, -1.04801802,\n",
       "         -1.01461185, -0.85948004, -0.84920563, -0.80182894, -0.75275929,\n",
       "         -0.66793626, -0.43475389, -0.40832989, -0.35908504, -0.26358009,\n",
       "         -0.16671045, -0.03722642, -0.02893854,  0.08540663,  0.12040813,\n",
       "          0.13639698,  0.14853859,  0.2561765 ,  0.28026168,  0.36766319,\n",
       "          0.55448741,  0.58739987,  0.59195091,  0.60669007,  0.64526911,\n",
       "          0.67111737,  0.73978876,  0.82534483,  0.97513371,  1.10539816,\n",
       "          1.24114406,  1.24843547,  1.27946872,  1.37404301,  1.37763707,\n",
       "          1.39196365,  1.56471029,  1.62580308,  1.62762208,  1.63346862,\n",
       "          1.65079694,  1.71105577,  1.81318188,  1.85038409,  1.89276857,\n",
       "          1.97242505,  1.99465584,  2.17862056,  2.19705687,  2.32327646,\n",
       "          2.3689641 ,  2.45592241,  2.53124541,  2.63699365,  2.69331322,\n",
       "          2.70428584,  2.7937922 ,  2.81750777,  2.81945911,  2.92132162],\n",
       "        [ 0.34115197,  1.16316375, -0.16128571, -1.23695071, -0.33450124,\n",
       "          0.06980208,  2.13303337,  1.46564877, -1.91877122,  0.17457781,\n",
       "         -1.07774478,  0.75193303,  0.09965137,  0.18463386,  0.91786195,\n",
       "         -1.15099358,  1.35624003,  0.62962884,  0.06856297,  3.85273149,\n",
       "         -0.51827022, -0.32206152, -1.10633497,  1.76545424, -0.83921752,\n",
       "          1.6324113 ,  0.61167629, -0.21967189, -1.0708925 , -0.70766947,\n",
       "         -2.6197451 , -0.65160035,  0.8496021 , -1.1913035 , -1.42474819,\n",
       "          0.13074058,  1.40279431, -0.70205309,  1.05712223, -0.20812225,\n",
       "         -0.01349722, -1.32818605, -0.23413696,  0.31424733, -1.02438764,\n",
       "          0.01300189,  0.35778736,  0.97554513,  0.85243333, -0.47193187,\n",
       "          1.89679298, -0.29169375,  0.58831721,  0.91540212,  0.50498728,\n",
       "         -2.02514259,  1.05380205,  2.19045563,  0.96337613,  0.51503527,\n",
       "          2.72016917, -1.47852199,  1.53803657,  0.00511346,  0.95400176,\n",
       "         -0.71435142, -0.84679372, -1.86726519,  0.51326743, -1.51936997,\n",
       "         -0.48536355,  0.85639879,  2.46324211,  0.89959988,  0.15372511,\n",
       "          0.17136828,  0.36163603,  0.22745993,  0.32408397, -0.88951443,\n",
       "         -1.72491783, -0.92693047,  0.66213067, -1.22084365,  0.29698467,\n",
       "         -0.46947439,  0.7870846 ,  0.46210347, -0.29900735, -0.03471177,\n",
       "          0.75896922, -0.42064532, -1.46351495,  0.17318093,  0.71400049,\n",
       "         -0.46572975, -0.73036663, -0.91942423, -0.68002472,  0.64768854],\n",
       "        [-0.07710171, -1.43586215, -0.80227727,  0.78182287, -1.20029641,\n",
       "         -0.60021688, -1.2378155 , -1.4123037 , -0.07444592,  1.8861859 ,\n",
       "          0.06428002,  1.14282281, -0.56629773, -1.60748323, -1.26088395,\n",
       "          0.11092259,  0.81252582,  0.81286212, -1.55066343,  0.51504769,\n",
       "          1.47789404, -0.78325329, -0.18565898, -0.65332923,  0.93128012,\n",
       "         -1.24778318, -0.676922  , -1.98756891, -0.85715756,  1.26691115,\n",
       "          1.56464366,  0.04557184, -0.58936476,  0.47383292,  0.0675282 ,\n",
       "         -0.44004449, -0.90938745,  0.96864499, -0.46063877, -0.62269952,\n",
       "          1.85227818, -1.95967012, -0.23415337, -1.01283112, -3.24126734,\n",
       "          0.82718325, -1.1429703 ,  0.33126343, -0.66178646, -1.61271587,\n",
       "          0.82206016, -0.60063869, -0.1517851 , -0.50175704, -0.11473644,\n",
       "          0.63391902,  1.08305124,  0.58685709, -0.82068232, -0.93782504,\n",
       "         -0.26465683, -0.3011037 ,  0.36139561,  0.26105527,  1.13556564,\n",
       "          0.29307247, -0.07282891,  2.31465857, -0.5297602 ,  1.03246526,\n",
       "         -0.23681861, -0.44651495,  0.06023021, -0.6929096 , -0.88385744,\n",
       "          0.73846658,  1.0035329 , -1.23086432, -1.76304016, -0.75373616,\n",
       "         -1.91328024,  0.77463405,  0.11351735,  0.82254491,  0.52194157,\n",
       "          0.76743473, -0.97468167, -0.98150865,  0.08704707,  0.30154734,\n",
       "         -0.31526924, -1.41537074, -0.39210815, -1.24573878, -0.22346279,\n",
       "         -0.46341769,  0.67959775,  0.47359243,  0.34644821, -0.1382643 ]]),\n",
       " 'A1': array([[0.71517776, 0.72358901, 0.67783773, 0.66324647, 0.66183405,\n",
       "         0.69434338, 0.76894703, 0.73931927, 0.61112947, 0.7515029 ,\n",
       "         0.65702857, 0.76110107, 0.69960446, 0.68128853, 0.72220724,\n",
       "         0.65752938, 0.77982528, 0.75276922, 0.67923015, 0.85585019,\n",
       "         0.71901916, 0.67998629, 0.65693431, 0.77207127, 0.69483264,\n",
       "         0.75657822, 0.72588084, 0.65937715, 0.64502014, 0.71243922,\n",
       "         0.62950159, 0.69003983, 0.7429036 , 0.67512556, 0.65543469,\n",
       "         0.71619978, 0.76006848, 0.70957496, 0.755644  , 0.69847915,\n",
       "         0.75729665, 0.61558538, 0.70767135, 0.71593792, 0.60102416,\n",
       "         0.74177175, 0.71875911, 0.77277315, 0.74986623, 0.67257418,\n",
       "         0.81525277, 0.70551289, 0.75205249, 0.7594363 , 0.7507759 ,\n",
       "         0.65385153, 0.79336275, 0.82407083, 0.75706455, 0.7373815 ,\n",
       "         0.82994266, 0.66364236, 0.80170781, 0.74222053, 0.79469854,\n",
       "         0.7128274 , 0.69981279, 0.70606206, 0.75093579, 0.69701122,\n",
       "         0.71744098, 0.76840789, 0.83183778, 0.76664276, 0.73292212,\n",
       "         0.76507554, 0.77856539, 0.73141633, 0.72472335, 0.69229376,\n",
       "         0.62545843, 0.72385421, 0.77602611, 0.7134376 , 0.77028782,\n",
       "         0.7453795 , 0.7625257 , 0.75118134, 0.74112963, 0.75711474,\n",
       "         0.77670406, 0.70763295, 0.68315991, 0.73845837, 0.77939627,\n",
       "         0.72793351, 0.74059155, 0.72876191, 0.73631364, 0.78036028],\n",
       "        [0.69625841, 0.63936475, 0.68297154, 0.74708204, 0.67435558,\n",
       "         0.68674863, 0.63124357, 0.6374988 , 0.73467964, 0.75898921,\n",
       "         0.72652955, 0.73076475, 0.69027945, 0.65606051, 0.65532379,\n",
       "         0.73137557, 0.71365771, 0.72580437, 0.66191372, 0.665603  ,\n",
       "         0.76179568, 0.69457338, 0.72545673, 0.66477357, 0.75307313,\n",
       "         0.6482732 , 0.68425742, 0.65609017, 0.70666397, 0.76384954,\n",
       "         0.79684675, 0.73054729, 0.68846018, 0.75102837, 0.74381781,\n",
       "         0.7054114 , 0.67025239, 0.75845212, 0.69104543, 0.70640813,\n",
       "         0.77306999, 0.68501594, 0.72022916, 0.6883151 , 0.63911822,\n",
       "         0.748327  , 0.68706188, 0.72222475, 0.69471442, 0.68759803,\n",
       "         0.72464889, 0.71767723, 0.71740817, 0.70281594, 0.72113762,\n",
       "         0.77785623, 0.74729268, 0.7170522 , 0.69408713, 0.69854067,\n",
       "         0.68537904, 0.74938091, 0.72349654, 0.74363083, 0.75432256,\n",
       "         0.75536185, 0.74785926, 0.82013994, 0.71652571, 0.78855563,\n",
       "         0.74234856, 0.71616305, 0.70644019, 0.70928208, 0.71527575,\n",
       "         0.76081093, 0.76656745, 0.70599884, 0.68806606, 0.73716005,\n",
       "         0.71639432, 0.77912574, 0.74027385, 0.78520507, 0.75739096,\n",
       "         0.77492543, 0.70831946, 0.71490103, 0.75660181, 0.75967894,\n",
       "         0.73152765, 0.71826054, 0.762808  , 0.71573217, 0.73764653,\n",
       "         0.74825427, 0.78250145, 0.78001378, 0.77364485, 0.74297678]]),\n",
       " 'A2': array([[1.22813901, 1.20532464, 1.22526407, 1.25129151, 1.2228874 ,\n",
       "         1.22571366, 1.19931206, 1.20361184, 1.24968535, 1.25044809,\n",
       "         1.24362768, 1.23879146, 1.22676858, 1.21450568, 1.2116632 ,\n",
       "         1.24549496, 1.23092075, 1.23736813, 1.21692728, 1.20734929,\n",
       "         1.25357499, 1.22967526, 1.24321326, 1.21225322, 1.25166728,\n",
       "         1.20675578, 1.22276933, 1.21588485, 1.23659436, 1.25479031,\n",
       "         1.27289409, 1.2431414 , 1.22335342, 1.25209617, 1.25050023,\n",
       "         1.23166111, 1.2151488 , 1.25285451, 1.22357109, 1.2331576 ,\n",
       "         1.25560295, 1.22995035, 1.2379986 , 1.22497959, 1.21287767,\n",
       "         1.24687827, 1.22431253, 1.23471723, 1.22536911, 1.22740511,\n",
       "         1.23301567, 1.23713353, 1.23412346, 1.22794577, 1.23566423,\n",
       "         1.26393437, 1.24325312, 1.22948914, 1.22467409, 1.22764734,\n",
       "         1.21671397, 1.25216744, 1.23340959, 1.24501043, 1.24592386,\n",
       "         1.25144083, 1.2493138 , 1.27724138, 1.23384743, 1.26543239,\n",
       "         1.24605463, 1.23261487, 1.22484688, 1.23002926, 1.23448201,\n",
       "         1.25031468, 1.25172799, 1.23094155, 1.2243337 , 1.24559142,\n",
       "         1.24162735, 1.26006267, 1.24158535, 1.26309452, 1.24864952,\n",
       "         1.25707365, 1.22990909, 1.2331956 , 1.25016019, 1.25036805,\n",
       "         1.23811651, 1.23722974, 1.25620966, 1.23431529, 1.2403457 ,\n",
       "         1.24771346, 1.26034056, 1.26010428, 1.25713778, 1.24237378]])}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "As"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b820a",
   "metadata": {},
   "source": [
    "#  Feedback: Learning Weights with Backpropagation\n",
    "\n",
    "<img src=\"https://machinelearningknowledge.ai/wp-content/uploads/2019/10/Backpropagation.gif\" width=500 height=500>\n",
    "\n",
    "Now the question is, given the predictions predictions made by the forward pass, how can we compute the gradients to then learn best weights and biases for the given data? Well, you might have seen this coming, the answer is gradient descent.\n",
    "\n",
    "However, remember that neural networks have multiple parameters where each layer has its own weights and biases. This means the hidden layer has its own weights and biases and the output layer has its own weights and biases. With a two layer neural network we then have four sets of weights and biases to learn. So, how do we update multiple weights and biases? Further, how do we update weights and biases whose values influence each other? Recall, later layers, like the output layer, are influenced by earlier layers. This means, changes in the weights in the hidden layer will influence the output in the output layer! \n",
    "\n",
    "What if we just tried updating each weight and bias one at a time while treating the other weights and biases as constants? Well, this is the general idea of *backpropagation*, a method for computing the gradients with respect to each set of weights and bias in the network.\n",
    "\n",
    "The goal of backpropogration is to compute the gradient of each layer's weights and biases one at a time. However, instead of starting at the 1st layer we start at the last layer and we backpropogate the error/loss (i.e., the parent or root equation) through the network. When we say \" backpropogate the error/loss through the network\" all this means is we compute the gradient starting with the loss function and then we continue to compute the gradient of subsequent equations until we reach our desired weight or bias parameter. In order to do this, we need to utilize and understand the concept of partial derivatives and the chain rule from calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ed444",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Partial derivatives with Chain Rule\n",
    "If you are unfamiliar with partial derivatives and the Chain Rule it is highly recommended you check out [this video](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/multivariable-chain-rule/v/multivariable-chain-rule) or [post](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/differentiating-vector-valued-functions/a/multivariable-chain-rule-simple-version). With this in mind, we will quickly review the concept by using a simple example.\n",
    "\n",
    "The goal of computing partial derivatives with the Chain Rule is to compute the derivatives for an equation with multiple variables and multiple equations or sub-functions. [Partial derivatives](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives) refer to computing the derivatives for two or more variables in an equation. The [Chain Rule](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review) refers to computing the derivative of equations within equations such as $h'(x) = f'(g(x))g'(x)$. As we will soon see, neural networks are just a combination many equations with different variables nested inside one another (i.e., there are many variables and many small equations that make up neural networks).\n",
    "\n",
    "Let's look at a very basic example. Given the equations below, the goal will be to compute the derivative of the \"parent\" or root equation $f(x, y)$ with respect to the variable $s$. However, notice $s$ is nested in the variable $t$ and $t$ is nested in the variable $x$.\n",
    "$$\n",
    "f(x, y) = x^2 y \\\\\n",
    "x = t^3 \\\\\n",
    "t = \\log(s)\n",
    "$$\n",
    "If we wrote out the entire equation plugging in each variable we would get the following:\n",
    "$$\n",
    "f(x, y) = (\\log(s)^3)^2 y \n",
    "$$\n",
    "Notice, when framed this way, this is just a matter of computing the derivative of $s$. However, computing that derivative directly is a small nightmare. It would be much easier if we computed the derivative in smaller parts. This is the exact idea of the computing partial derivative with the Chain Rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc14e58",
   "metadata": {},
   "source": [
    "Let's solve this problem. Our goal is to then compute the derivative $s$ which can be written in partial derivative notation as $\\frac{\\partial f(x, y)}{\\partial s}$ where $\\partial$ indicates the partial derivative (i.e., the derivative for a single variable). Notation wise, the variable we want to find, $s$ in this case, goes in the denominator and the equation the variable is nested inside, $f(x, y)$ in this case, goes in the numerator.\n",
    "\n",
    "Now we can solve for $\\frac{\\partial f(x, y)}{\\partial s}$ by holding each variable constant and solving for the derivative of each equation one at a time. The expanded form is given as follows: \n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x, y)}{\\partial s} = \\frac{\\partial f(x, y)}{\\partial x}\\frac{\\partial x}{\\partial t}\\frac{\\partial t}{\\partial s}.\n",
    "$$\n",
    "\n",
    "Notice, the first derivative we most compute is  $\\frac{\\partial f(x, y)}{\\partial x}$. This says, treat all other variables as a constant (in this case we would treat $y$ as a constant as it is the only other variable) and solve for $x$. We solve for $x$ because $s$ is nested within $x$. The next derivative we have to compute is $\\frac{\\partial x}{\\partial t}$. Once again, we need to compute the derivative of $x$'s equation with respect to $t$ because $s$ is nested in $t$. Finally, $\\frac{\\partial t}{\\partial s}$ computes the derivative of $t$'s equation with respect to $s$. Now, all we do is multiply all the solutions together and that gives us the derivative of $\\frac{\\partial f(x, y)}{\\partial s}$.\n",
    "\n",
    "Computing the derivative for each partial equation looks as follows:\n",
    "$$\n",
    "\\frac{\\partial f(x, y)}{\\partial x} = 2x \\\\\n",
    "\\frac{\\partial x}{\\partial t} = 3t^2 \\\\\n",
    "\\frac{\\partial t}{\\partial s} = \\frac{1}{s}\n",
    "$$\n",
    "Combining all the partial derivatives gives us the following solution:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial f(x, y)}{\\partial s} &= \\frac{\\partial f(x, y)}{\\partial x}\\frac{\\partial x}{\\partial t}\\frac{\\partial t}{\\partial s} \\\\\n",
    "&=  (2x)(3t^2)(\\frac{1}{s})\n",
    "\\end{align}\n",
    "$$\n",
    "where we simply plugged the partial derivatives solutions back into the original expanded equation.\n",
    "\n",
    "Now, all we have to do is take this idea and apply it to computing the derivatives/gradients of our parameters for our neural network and then apply the gradient descent updates!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467da4f7",
   "metadata": {},
   "source": [
    "## Backpropagation \n",
    "\n",
    "Recall, we can write out all the equations for our two layer neural network including the loss as follows:\n",
    "\n",
    "\n",
    "1. Hidden layer (layer 1) equations\n",
    "\n",
    "$$\n",
    "\\Zm^{[1]} = \\Wm^{[1]} \\Xm + \\bv^{[1]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Am^{[1]} = g(\\Zm^{[1]} )\n",
    "$$\n",
    "\n",
    "2. Output layer (layer 2) equations\n",
    "\n",
    "$$\n",
    "\\zv^{[2]} = \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\av^{[2]} = g(\\zv^{[2]})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "3. MSE loss\n",
    "$$\n",
    "\\hat{\\yv} = \\av^{[2]\\top}\n",
    "$$\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{2m} (\\hat{\\yv} - {\\yv})^2 \n",
    "$$\n",
    " \n",
    "Thus, we need to compute the gradients or partial derivatives with respect to $\\Wm^{[1]}$, $\\bv^{[1]}$, $\\wv^{[2]}$, and $\\bv^{[2]}$ using all of the above equations. \n",
    "\n",
    "As this is a non-regression problem, we'll use the mean squared error (MSE) loss to measure the performance of the network and compute the gradients. Remember the negative gradient corresponds to how we need to change the weights such that the value of the loss decreases or is minimized! Thus, the MSE loss will be the \"parent\" equation we use to compute the gradients with. \n",
    "\n",
    "Remember we will need to compute the gradients with respect to each of the previously mentioned weights and biases. This means we need to solve for $\\frac{\\partial MSE}{\\partial  \\Wm^{[1]}}$, $\\frac{\\partial MSE}{\\partial \\bv^{[1]}}$,  $\\frac{\\partial MSE}{\\partial \\wv^{[2]}}$, and $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e37142",
   "metadata": {},
   "source": [
    "Before we start, we define `delta_mse` which is the derivative of the MSE the loss function $MSE = \\frac{1}{2m}(\\hat{y} - y)^2$ where we remove the $\\frac{1}{m}$ for convenience sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "917e3759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_mse(y, y_hat):\n",
    "    return y_hat - y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1cc36",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "\n",
    "To start computing gradients, we first compute the derivative of the parent function, the MSE loss, and then slowly compute the derivatives of each equation up until we reach the desired weight and bias parameter. The first parameters we will run into are the weights and biases for the output layer. Thus, we'll compute the partial derivatives $\\frac{\\partial MSE}{\\partial \\wv^{[2]}}$ and $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$ first as they are the first parameters we run into when propagating backwards through the network.\n",
    "\n",
    "To compute gradients for $\\frac{\\partial MSE}{\\partial \\wv^{[2]}}$ and $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$ we'll only need information regarding the loss, output layer, and hidden layer outputs $\\Am^{[1]}$. Visually, this looks like the below image where we are backpropagating through the network up to the end of the output layer.\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51635129152_217a68e8d4.jpg\" width=\"300\" height=\"246\" alt=\"signle_nn_output_layers_back\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470f5812",
   "metadata": {},
   "source": [
    "#### Weight update\n",
    "\n",
    "To compute the gradient or partial derivative of the  weights $\\wv^{[2]}$ we need the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "MSE &= \\frac{1}{2m} (\\av^{[2]\\top} - {\\yv})^2  \\\\ \n",
    " \\\\\n",
    "\\av^{[2]} &= g(\\zv^{[2]}) \\\\\n",
    "\\\\\n",
    "\\zv^{[2]} &= \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\hat{\\yv} = \\av^{[2]\\top}$.\n",
    "\n",
    "Thus, if we expand $\\frac{\\partial MSE}{\\partial \\wv^{[2]}}$ we get the following equation:\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\wv^{[2]}} = \\frac{\\partial MSE}{\\partial \\av^{[2]}}\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\frac{\\partial \\zv^{[2]} }{\\partial  \\wv^{[2]} }\n",
    "$$\n",
    "\n",
    "If we compute the derivative of each of the partial derivatives we get the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\av^{[2]}} &= (\\av^{[2]\\top} - {\\yv}) \\\\\n",
    "\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} } &= \\mathbf{1} \\\\\n",
    "\\frac{\\partial \\zv^{[2]} }{\\partial  \\wv^{[2]} } &= \\Am^{[1]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Heres a short description of what we did to find each of the partial derivatives.\n",
    "1. Computing $\\frac{\\partial MSE}{\\partial \\av^{[2]}}$ simply requires taking the derivative of the MSE loss function. Recall, for regression problems, the output of the output layer is equal to the predictions such that $\\hat{\\yv} = \\av^{[2]\\top}$ is true.\n",
    "2. Computing $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ gives us matrix or vector of 1s because we set $g$ as the identity/linear activation in the output neuron. Recall, this means the input is equal to the output. Thus, the derivative for each data sample is simply 1. The shape of $\\mathbf{1}$ is the same as $\\zv$.\n",
    "3. Computing $\\frac{\\partial \\zv^{[2]} }{\\partial  \\wv^{[2]} }$ simply requires taking derivative of the linear combination equation $\\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}$ where $\\wv^{[2]}$ goes away and $\\bv^{[2]}$ goes to zero as it is treated as constant and does not depend on $\\wv^{[2]}$. This leaves only the activation outputs $\\Am^{[1]}$ from the hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82bda59",
   "metadata": {},
   "source": [
    "Turning all this into code looks as follows. Below are the weights before updating the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d5beeb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights['W2'] shape: (1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.06241279,  0.391773  ]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"weights['W2'] shape: {weights['W2'].shape}\")\n",
    "weights['W2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f515c31",
   "metadata": {},
   "source": [
    "Next, we set each of the partial derivatives equal to their derivative where\n",
    "- $\\frac{\\partial MSE}{\\partial \\av^{[2]}}$ corresponds to  `delta_mse_A2` \n",
    "- $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ corresponds to `delta_A2_Z2`\n",
    "- $\\frac{\\partial \\zv^{[2]} }{\\partial \\wv^{[2]} }$ corresponds to `delta_Z2_W2` \n",
    "\n",
    "Below is a short description of what we did to find each of the partial derivatives.\n",
    "\n",
    "```Python\n",
    "delta_mse_A2 = delta_mse(y, y_hat)\n",
    "```\n",
    "To compute the partial derivative $\\frac{\\partial MSE}{\\partial \\av^{[2]}}$ we use the `delta_mse()` which is the derivative of the MSE where we remove the $\\frac{1}{m}$ for convenience sake. \n",
    "\n",
    "```Python\n",
    "delta_A2_Z2 = Linear.derivative(Zs['Z2'])\n",
    "```\n",
    "To compute the partial derivative $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ we can call the static method `Linear.derivative()` and pass `Z2` which will return a NumPy array of ones whose shape is the same as `Z2` which is (neurons=1, data samples=100).\n",
    "\n",
    "```Python\n",
    "delta_Z2_W2 = As['A1']\n",
    "```\n",
    "Lastly, the partial derivative for $\\frac{\\partial \\zv^{[2]} }{\\partial  \\wv^{[2]} }$ is simply set to `As['A1']` which corresponds to activation outputs from the hidden layer $\\Am^{[1]}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "df96ca4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta_mse_A2 = delta_mse(y, y_hat)\n",
    "delta_A2_Z2 = Linear.derivative(Zs['Z2'])\n",
    "delta_Z2_W2 = As['A1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d35ebd",
   "metadata": {},
   "source": [
    "Below are the shapes of each of the partial derivatives, the labels, and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e60764a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (100, 1)\n",
      "y_hat shape: (100, 1)\n",
      "delta_mse_A2 shape: (100, 1)\n",
      "delta_A2_Z2 shape: (1, 100)\n",
      "delta_Z2_W2 shape: (2, 100)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y_hat shape: {y_hat.shape}\")\n",
    "print(f\"delta_mse_A2 shape: {delta_mse_A2.shape}\")\n",
    "print(f\"delta_A2_Z2 shape: {delta_A2_Z2.shape}\")\n",
    "print(f\"delta_Z2_W2 shape: {delta_Z2_W2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff535dbd",
   "metadata": {},
   "source": [
    "Sadly, we can't always compute $\\frac{\\partial MSE}{\\partial \\wv^{[2]}}$ by simply multiplying all the values for the expanded partial derivative equation. Since we are working with vectors and matrices we need to utilize the dot product and, in turn, rearrange some of the partial derivatives. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\wv^{[2]}} &=  \\frac{\\partial MSE}{\\partial \\av^{[2]}} \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} } \\frac{\\partial \\zv^{[2]}}{\\partial  \\wv^{[2]}}\\\\\n",
    "\\\\\n",
    "&=  \\big (\\frac{\\partial MSE}{\\partial \\av^{[2]}}^\\top * \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} } \\big ) \\cdot \\frac{\\partial \\zv^{[2]}}{\\partial  \\wv^{[2]}}^\\top  \\\\\n",
    "\\\\\n",
    "&= \\big( (\\av^{[2]} - {\\yv})^\\top * \\mathbf{1} \\big )  \\cdot  \\Am^{[1]\\top}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\cdot$ represents the dot product and $*$ represents element-wise multiplication. Notice, that the equation inherently computes the sum of the gradients due to the addition of the dot product. This means we only need to divide by the number of data samples used to compute the gradient to get the average gradient.\n",
    "\n",
    "Further, we need to add some transposes to make the equation function properly. If we don't do this, shape mismatch errors will be thrown.\n",
    "\n",
    "- First, we have to transpose $\\frac{\\partial MSE}{\\partial \\av^{[2]}}^\\top$ (i.e., `delta_mse_A2`). This allows us to take the element-wise product $*$ with the partial derivative of the activation function $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ (i.e., `delta_A2_Z2`) which has the shape (1, data samples=100).\n",
    "    - Original shape: (data samples=100, 1) \n",
    "    - Transposed shape: (1, data samples=100) \n",
    "\n",
    "- Second, we have to transpose $\\frac{\\partial \\zv^{[2]}}{\\partial  \\wv^{[2]}}^\\top$  (i.e., ``delta_Z2_W2``) because its shape ()to get the shape (data samples, neurons) so that when we take the dot product with $\\big( (\\av^{[2]} - {\\yv})^\\top * \\mathbf{1} \\big )$ which has shape (1 , data samples=100).\n",
    "    - Original shape: (input=2, data samples=100) \n",
    "    - Transposed shape: (data samples=100, input=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "861ba991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.96239859, -1.95823307]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient\n",
    "delta_mse_W2 = (delta_mse_A2.T * delta_A2_Z2) @ delta_Z2_W2.T\n",
    "# Average Gradient\n",
    "W2_avg_grad = delta_mse_W2 / len(y)\n",
    "W2_avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584f6f41",
   "metadata": {},
   "source": [
    "Below are the shapes of the variables used in the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "70c93d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2) =  (1, 100) * (1, 100) @ (100, 2)\n",
      "(1, 2) = (1, 2) / 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"{delta_mse_W2.shape} =  {delta_mse_A2.T.shape} * {delta_A2_Z2.shape} @ {delta_Z2_W2.T.shape}\")\n",
    "print(f\"{W2_avg_grad.shape} = {delta_mse_W2.shape} / {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5da30",
   "metadata": {},
   "source": [
    "Additionally, we add the implementation to the `get_output_layer_weight_grads()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "416cc48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layer_weight_grads(y, y_hat, Zs, As):\n",
    "    delta_mse_A2 = delta_mse(y, y_hat)\n",
    "    delta_A2_Z2 = Linear.derivative(Zs['Z2'])\n",
    "    delta_Z2_W2 = As['A1']\n",
    "\n",
    "    delta_mse_W2 = (delta_mse_A2.T * delta_A2_Z2) @ delta_Z2_W2.T\n",
    "    W2_avg_grad = delta_mse_W2 / len(y)\n",
    "\n",
    "    return W2_avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7ae64eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.96239859, -1.95823307]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2_avg_grad = get_output_layer_weight_grads(y, y_hat, Zs, As)\n",
    "W2_avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640c137",
   "metadata": {},
   "source": [
    "Finally, to update the weights `W2` using the computed gradient we just need to apply the minimization gradient update equation which scales the gradient by the learning rate $\\alpha$ and then subtracts the scaled gradient. Below, we can see what the new values of the weights would be after the update.\n",
    "\n",
    "*Note, we typically do not want to update any parameters until we have computed the gradients for ALL weights and biases in the network. Updating the weights and biases too soon can cause issues with the gradients of weights and biases that haven't been updated yet. We are only displaying what the new weights would be, we are NOT actually updating them right now.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bd81ce62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13382707, 0.58759631]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = .1\n",
    "W2 = weights['W2'] - alpha * W2_avg_grad\n",
    "W2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998b83e",
   "metadata": {},
   "source": [
    "#### Bias update\n",
    "To compute the gradient or partial derivative of the biases $\\bv^{[2]}$ we need same equations as we used in the weight computation.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "MSE &= \\frac{1}{2m} (\\av^{[2]} - {\\yv})^2  \\\\ \n",
    " \\\\\n",
    "\\av^{[2]} &= g(\\zv^{[2]}) \\\\\n",
    "\\\\\n",
    "\\zv^{[2]} &= \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\hat{\\yv} = \\av^{[2]\\top}$.\n",
    "\n",
    "If we expand $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$ we get the following equation:\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\bv^{[2]}} = \\frac{\\partial MSE}{\\partial \\av^{[2]}}\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\frac{\\partial \\zv^{[2]} }{\\partial  \\bv^{[2]} }\n",
    "$$\n",
    "Notice, the only new partial derivative is $\\frac{\\partial \\zv^{[2]} }{\\partial \\bv^{[2]}}$, the rest we have already seen before. If we compute the partial derivatives we get the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\av^{[2]}} &= (\\av^{[2]} - {\\yv}) \\\\\n",
    "\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} } &= \\mathbf{1} \\\\\n",
    "\\frac{\\partial \\zv^{[2]} }{\\partial  \\bv^{[2]} } &= \\mathbf{1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Heres a short description of what we did to find each of the NEW partial derivatives.\n",
    "1. Computing $\\frac{\\partial \\zv^{[2]} }{\\partial \\bv^{[2]} }$ simply requires taking derivative of the linear combination equation $\\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}$. Here $\\wv^{[2]}$ and $\\Am^{[1]}$ act constants and go to zero as they do not depend on $\\bv^{[2]}$. Meanwhile, the derivative of $\\bv^{[2]}$ is simply an vector of ones of shape (1, data samples). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a9515",
   "metadata": {},
   "source": [
    "Turning all this into code looks as follows. Below are the biases before updating the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "09e178ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias['b2'] shape: (1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"bias['b2'] shape: {bias['b2'].shape}\")\n",
    "bias['b2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b7e86",
   "metadata": {},
   "source": [
    "Next, we set each of the partial derivatives equal to their derivative where\n",
    "- $\\frac{\\partial MSE}{\\partial \\av^{[2]}}$ corresponds to  `delta_mse_A2` \n",
    "- $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ corresponds to `delta_A2_Z2`\n",
    "- $\\frac{\\partial \\zv^{[2]} }{\\partial \\bv^{[2]} }$ corresponds to `delta_Z2_b2` \n",
    "\n",
    "Below is a short description of what we did to find each of the NEW partial derivatives.\n",
    "\n",
    "```Python\n",
    "delta_Z2_b2 = np.ones([1, len(y)])\n",
    "```\n",
    "We set `delta_Z2_b2` equal to an array of ones with a shape of (1, data samples=100). We do this because the derivative of $\\frac{\\partial \\zv^{[2]} }{\\partial \\bv^{[2]} }$ is one and we have to set the partial derivative for each data sample.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dc49317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mse_A2 = delta_mse(y, y_hat)\n",
    "delta_A2_Z2 = Linear.derivative(Zs['Z2'])\n",
    "delta_Z2_b2 = np.ones([1, len(y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877e5213",
   "metadata": {},
   "source": [
    "Below are the shapes of each of the partial derivatives, the labels, and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1a109151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (100, 1)\n",
      "y_hat shape: (100, 1)\n",
      "delta_mse_A2 shape: (100, 1)\n",
      "delta_A2_Z2 shape: (1, 100)\n",
      "delta_Z2_b2 shape: (1, 100)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y_hat shape: {y_hat.shape}\")\n",
    "print(f\"delta_mse_A2 shape: {delta_mse_A2.shape}\")\n",
    "print(f\"delta_A2_Z2 shape: {delta_A2_Z2.shape}\")\n",
    "print(f\"delta_Z2_b2 shape: {delta_Z2_b2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a990104c",
   "metadata": {},
   "source": [
    "Using the same equation format as the output layer weight update, we can compute $\\frac{\\partial MSE}{\\partial \\bv^{[2]}}$ by using the dot product and element-wise multiplication.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\bv^{[2]}} &=  (\\frac{\\partial MSE}{\\partial \\av^{[2]}} * \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }) \\cdot \\frac{\\partial \\zv^{[2]}}{\\partial  \\bv^{[2]}}^\\top \\\\\n",
    "&=  \\big ( (\\av^{[2]} - {\\yv})^\\top * \\mathbf{1} \\big ) \\cdot \\mathbf{1}^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\cdot$ represents the dot product and $*$ represents element-wise multiplication. Notice, that the equation inherently computes the sum of the gradients due to the addition of the dot product. This means we only need to divide by the number of data samples used to compute the gradient to get the average gradient.\n",
    "\n",
    "\n",
    "Further, we need to add some transposes to make the equation function properly. If we don't do this, shape mismatch errors will be thrown.\n",
    "\n",
    "- First, we have to transpose $\\frac{\\partial MSE}{\\partial \\av^{[2]}}^\\top$ (i.e., `delta_mse_A2`). This allows us to take the element-wise product $*$ with the partial derivative of the activation function $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ (i.e., `delta_A2_Z2`) which has the shape (1, data samples=100).\n",
    "    - Original shape: (data samples=100, 1) \n",
    "    - Transposed shape: (1, data samples=100)\n",
    "\n",
    "- Second, we have to transpose $\\frac{\\partial \\zv^{[2]}}{\\partial  \\bv^{[2]}}^\\top$ (i.e., `delta_Z2_b2`). This allows us to take the dot product with $\\big( (\\av^{[2]} - {\\yv})^\\top * \\mathbf{1} \\big )$ which has shape (1 , data samples=100)\n",
    "    - Original shape: (1, data samples=100)\n",
    "    - Transposed shape: (data samples=100, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e11c0b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.67470392]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient\n",
    "delta_mse_b2 = (delta_mse_A2.T * delta_A2_Z2) @ delta_Z2_b2.T\n",
    "# Average Gradient\n",
    "b2_avg_grad = delta_mse_b2 / len(y)\n",
    "\n",
    "b2_avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af84bd4",
   "metadata": {},
   "source": [
    "Below are the shapes of the variables used in the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "86c474f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1) =  ((1, 100) * (1, 100)) @ (100, 1)\n",
      "(1, 1) = (1, 1) / 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"{delta_mse_b2.shape} =  ({delta_mse_A2.T.shape} * {delta_A2_Z2.shape}) @ {delta_Z2_b2.T.shape}\")\n",
    "print(f\"{b2_avg_grad.shape} = {delta_mse_b2.shape} / {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8060eafb",
   "metadata": {},
   "source": [
    "Additionally, we add the implementation to the `get_output_layer_bias_grads()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fcff2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layer_bias_grads(y, y_hat, Zs):\n",
    "    delta_mse_A2 = delta_mse(y, y_hat)\n",
    "    delta_A2_Z2 = Linear.derivative(Zs['Z2'])\n",
    "    delta_Z2_b2 = np.ones([1, len(y)])\n",
    "    \n",
    "    delta_mse_b2 = (delta_mse_A2.T * delta_A2_Z2) @ delta_Z2_b2.T\n",
    "    b2_avg_grad = delta_mse_b2 / len(y)\n",
    "    \n",
    "    return b2_avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "9740f9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.67470392]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2_avg_grad = get_output_layer_bias_grads(y, y_hat, Zs)\n",
    "b2_avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb802a",
   "metadata": {},
   "source": [
    "Finally, below is an example of what the updated bias values `b2` would be if the gradient update equation was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a7bc49af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.26747039]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = .1\n",
    "b2 = bias['b2'] - alpha * b2_avg_grad\n",
    "b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20d621",
   "metadata": {},
   "source": [
    "### Hidden Layer\n",
    "Now, it's time to propagate the error further into the network. Therefore, we need to now compute the gradients for the hidden layer weights and biases. Thus, we need to compute the partial derivatives $\\frac{\\partial MSE}{\\partial \\Wm^{[1]}}$ and $\\frac{\\partial MSE}{\\partial \\bv^{[1]}}$.\n",
    "\n",
    "\n",
    "As we'll see, to compute gradients for $\\frac{\\partial MSE}{\\partial \\Wm^{[1]}}$ and $\\frac{\\partial MSE}{\\partial \\bv^{[1]}}$ we'll need to first compute the derivatives of the loss and the output layer again before we can compute the gradients for the hidden layer's weights and biases. \n",
    "\n",
    "Visually, this looks like the below image where we backpropagating through the ENTIRE network to reach the hidden layer.\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/51640582927_8ec720874d.jpg\" width=\"400\" height=\"389\" alt=\"signle_nn_hidden_layers_back\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7821b8f",
   "metadata": {},
   "source": [
    "#### Weight update\n",
    "\n",
    "To compute the gradient or partial derivative of the  weights $\\Wm^{[1]}$ we need the following equations:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{2m} (\\av^{[2]} - {\\yv})^2  \\\\ \n",
    "\\av^{[2]} = g(\\zv^{[2]}) \\\\\n",
    "\\zv^{[2]} = \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}\\\\\n",
    "\\Am^{[1]} = g(\\Zm^{[1]}) \\\\\n",
    "\\Zm^{[1]} = \\Wm^{[1]} \\Xm + \\bv^{[1]}\n",
    "$$\n",
    "where $\\hat{\\yv} = \\av^{[2]\\top}$.\n",
    "\n",
    "Thus, if we expand $\\frac{\\partial MSE}{\\partial \\Wm^{[1]}}$ we get the following equation:\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\Wm^{[1]}} = \\frac{\\partial MSE}{\\partial \\av^{[2]}}\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\frac{\\partial \\zv^{[2]} }{\\partial  \\Am^{[1]} } \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} }\n",
    "$$\n",
    "\n",
    "Notice, in order to compute $\\frac{\\partial MSE}{\\partial \\Wm^{[1]}}$ we have to recompute ALL the partial derivatives up to $\\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} }$. This is where we can start to see how backpropagation got its name! If we compute the partial derivatives we get the following:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\av^{[2]}} = (\\av^{[2]} - {\\yv}) \\\\\n",
    "\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} } = \\mathbf{1} \\\\\n",
    "\\frac{\\partial \\zv^{[2]} }{\\partial  \\Am^{[1]} }  = \\wv^{[2]} \\\\\n",
    "\\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } = g( \\Zm^{[1]}) (\\mathbf{1} - g( \\Zm^{[1]})) \\\\\n",
    " \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} } = \\Xm\n",
    "$$\n",
    "\n",
    "Heres a short description of what we did to find each of the NEW partial derivatives.\n",
    "1. Computing $\\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]}}$ simply requires taking derivative of the linear combination equation $\\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}$ where $\\Am^{[1]}$ goes away and $\\bv^{[2]}$ goes to zero as it is treated as constant and does not depend on $\\Am^{[1]}$. This simply leaves the output layer weights $\\wv^{[2]}$.\n",
    "1. Computing $\\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]}}$ requires computing the derivative of the activation function. Remember, we used the sigmoid activation function for $g$ in the hidden neurons, thus we are left with the derivative of the sigmoid activation function.\n",
    "1. Computing $ \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]}}$ simply requires taking derivative of the linear combination equation $\\Wm^{[1]} \\Xm + \\bv^{[1]}$ where $\\Wm^{[1]}$ goes away and $\\bv^{[1]}$ goes to zero as it is treated as constant and does not depend on $\\Wm^{[1]}$. This simply leaves the input features $\\Xm$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb23e39",
   "metadata": {},
   "source": [
    "Turning all this into code looks as follows. Below are the weights before updating the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "28d0726b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights['W1'] shape: (2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0488135 ,  0.21518937,  0.10276338],\n",
       "       [ 0.04488318, -0.0763452 ,  0.14589411]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"weights['W1'] shape: {weights['W1'].shape}\")\n",
    "weights['W1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4aeadd",
   "metadata": {},
   "source": [
    "Next, we set each of the partial derivatives equal to their derivative where\n",
    "- $\\frac{\\partial MSE}{\\partial \\av^{[2]}}$ corresponds to  `delta_mse_A2` \n",
    "- $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ corresponds to `delta_A2_Z2`\n",
    "- $\\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} }$ corresponds to `delta_Z2_A1` \n",
    "- $\\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} }$ corresponds to `delta_A1_Z1`\n",
    "- $ \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} }$ corresponds to `delta_Z1_W1`\n",
    "\n",
    "Below is a short description of what we did to find each of the NEW partial derivatives.\n",
    "\n",
    "```Python\n",
    "delta_Z2_A1 = weights['W2']\n",
    "```\n",
    "To get partial derivative for $\\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} }$ is equal to the weights for the output layer which corresponds to `weights['W2']`.\n",
    "\n",
    "```Python\n",
    "delta_A1_Z1 = Sigmoid.derivative(Zs['Z1'])\n",
    "```\n",
    "The partial derivative for $\\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} }$  is equal to he derivative of the sigmoid activation function which is accessed by calling the static method `Sigmoid.derivative()` and passing `Z1`.\n",
    "\n",
    "```Python\n",
    "delta_Z1_W1 = X.T\n",
    "```\n",
    "The partial derivative for $\\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} }$ is equal to the input features which corresponds to `X.T`. Remember we assume neural networks take inputs with shape (features=3, data samples=100) so we have to transpose `X` which has the shape (data samples=100, features=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8154c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mse_A2 = delta_mse(y, y_hat)\n",
    "delta_A2_Z2 = Linear.derivative(Zs['Z2'])\n",
    "delta_Z2_A1 = weights['W2']\n",
    "delta_A1_Z1 = Sigmoid.derivative(Zs['Z1'])\n",
    "delta_Z1_W1 = X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0dc12d",
   "metadata": {},
   "source": [
    "Below are the shapes of each of the partial derivatives, the labels, and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f614b667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (100, 1)\n",
      "y_hat shape: (100, 1)\n",
      "delta_mse_A2 shape: (100, 1)\n",
      "delta_A2_Z2 shape: (1, 100)\n",
      "delta_Z2_A1 shape: (1, 2)\n",
      "delta_A1_Z1 shape: (2, 100)\n",
      "delta_Z1_W1 shape: (3, 100)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y_hat shape: {y_hat.shape}\")\n",
    "print(f\"delta_mse_A2 shape: {delta_mse_A2.shape}\")\n",
    "print(f\"delta_A2_Z2 shape: {delta_A2_Z2.shape}\")\n",
    "print(f\"delta_Z2_A1 shape: {delta_Z2_A1.shape}\")\n",
    "print(f\"delta_A1_Z1 shape: {delta_A1_Z1.shape}\")\n",
    "print(f\"delta_Z1_W1 shape: {delta_Z1_W1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e3e21",
   "metadata": {},
   "source": [
    "Sadly, we can't compute $\\frac{\\partial MSE}{\\partial \\Wm^{[1]}}$ by simply multiplying all the values for the expanded partial derivative equation. Since we are working with vectors and matrices we need to utilize the dot product and, in turn, rearrange some of the partial derivatives. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\Wm^{[1]}} &= \\frac{\\partial MSE}{\\partial \\av^{[2]}} \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }  \\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} } \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} } \\\\\n",
    "\\\\\n",
    " &= \\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} }^\\top \\cdot \\big(\\frac{\\partial MSE}{\\partial \\av^{[2]}}^\\top * \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\big) * \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\cdot \\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} }^\\top \\\\\n",
    " \\\\\n",
    "&=   \\Wm^{[2]\\top} \\cdot \\big ( (\\av^{[2]} - {\\yv} )^\\top * \\mathbf{1} \\big )  * \\big( \\Zm^{[1]} *(\\mathbf{1}  - g( \\Zm^{[1]}) \\big )\\cdot \\Xm^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\cdot$ represents the dot product and $*$ represents element-wise multiplication. Notice, that the equation inherently computes the sum of the gradients due to the addition of the dot product. This means we only need to divide by the number of data samples used to compute the gradient to get the average gradient.\n",
    "\n",
    "Further, we need to add some transposes to make the equation function properly. If we don't do this shape mismatch errors will be thrown.\n",
    "\n",
    "- First, we have to transpose $\\frac{\\partial MSE}{\\partial \\av^{[2]}}^\\top$ (i.e., `delta_mse_A2`). This will allow us to take the element-wise product $*$ with the partial derivative of the activation function $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ (i.e., `delta_A2_Z2`) which has the shape (1, data samples=100).\n",
    "    - Original shape: (data samples=100, 1)\n",
    "    - Transposed shape: (1, data samples=100)\n",
    "\n",
    "- Second, we have to transpose $\\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} }^\\top $  (i.e., `delta_Z2_A1`). This will allow us to take the dot product with $\\big( (\\av^{[2]} - {\\yv})^\\top * \\mathbf{1} \\big )$ which has shape (1 , data samples=100).\n",
    "    - Original shape: (neurons=1, inputs=2)\n",
    "    - Transposed shape: (inputs=2, neurons=1) \n",
    "\n",
    "- Third, we have to transpose $\\frac{\\partial \\Zm^{[1]}}{\\partial \\Wm^{[1]} }^\\top $ (i.e., `delta_Z1_W1`). This will allow us to take the dot product with $\\Wm^{[2]\\top} \\cdot \\big ( (\\av^{[2]} - {\\yv} )^\\top * \\mathbf{1} \\big )  * \\big( \\Zm^{[1]} *(\\mathbf{1}  - g( \\Zm^{[1]}) \\big )$ which has the shape (2, data samples=100).\n",
    "    - Original shape: (features=3, data samples=100)\n",
    "    - Transposed shape: (data samples=100. features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4a93e7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.027934  , -0.00273238, -0.00690201],\n",
       "       [-0.16938226, -0.00982561,  0.05053177]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_mse_A1 = delta_Z2_A1.T @ (delta_mse_A2.T * delta_A2_Z2)\n",
    "delta_mse_W1 = ((delta_mse_A1 * delta_A1_Z1) @ delta_Z1_W1.T)\n",
    "W1_avg_grad = delta_mse_W1 / len(y)\n",
    "W1_avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdcb5cd",
   "metadata": {},
   "source": [
    "Below are the shapes of the variables used in the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b9c7dd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100) =  (2, 1) @ ((1, 100) * (1, 100))\n",
      "(2, 3) =  ((2, 100) * (2, 100)) @ (100, 3)\n",
      "(2, 3) = (2, 3) / 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"{delta_mse_A1.shape} =  {delta_Z2_A1.T.shape} @ ({delta_mse_A2.T.shape} * {delta_A2_Z2.shape})\")\n",
    "print(f\"{delta_mse_W1.shape} =  ({delta_mse_A1.shape} * {delta_A1_Z1.shape}) @ {delta_Z1_W1.T.shape}\")\n",
    "print(f\"{W1_avg_grad.shape} = {delta_mse_W1.shape} / {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f5e20",
   "metadata": {},
   "source": [
    "Additionally, we add the implementation to the `get_hidden_layer1_weight_grads()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5a925cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_layer1_weight_grads(X, y, y_hat, weights, bias, Zs):\n",
    "    delta_mse_A2 = delta_mse(y, y_hat)\n",
    "    delta_A2_Z2 = Linear.derivative(Zs['Z2'])\n",
    "    delta_Z2_A1 = weights['W2']\n",
    "    delta_A1_Z1 = Sigmoid.derivative(Zs['Z1'])\n",
    "    delta_Z1_W1 = X.T\n",
    "\n",
    "    delta_mse_A1 = delta_Z2_A1.T @ (delta_mse_A2.T * delta_A2_Z2)\n",
    "    delta_mse_W1 = ((delta_mse_A1 * delta_A1_Z1) @ delta_Z1_W1.T)\n",
    "    W1_avg_grad = delta_mse_W1 / len(y)\n",
    "\n",
    "    return W1_avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3c7b1a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.027934  , -0.00273238, -0.00690201],\n",
       "       [-0.16938226, -0.00982561,  0.05053177]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1_avg_grad = get_hidden_layer1_weight_grads(X, y, y_hat, weights, bias, Zs)\n",
    "W1_avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d50e1",
   "metadata": {},
   "source": [
    "Finally, below is an example of what the updated weight values `W1` would be if the gradient update equation was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b427cb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0460201 ,  0.2154626 ,  0.10345358],\n",
       "       [ 0.06182141, -0.07536264,  0.14084094]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = .1\n",
    "W1 = weights['W1'] - alpha * W1_avg_grad\n",
    "W1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78281fae",
   "metadata": {},
   "source": [
    "#### Bias update\n",
    "\n",
    "To compute the gradient or partial derivative of the bias $\\bv^{[1]}$ we need the following equations once again:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{2m} (\\av^{[2]} - {\\yv})^2  \\\\ \n",
    "\\av^{[2]} = \\zv^{[2]} \\\\\n",
    "\\zv^{[2]} = \\wv^{[2]} \\Am^{[1]} + \\bv^{[2]}\\\\\n",
    "\\Am^{[1]} = \\Zm^{[1]} \\\\\n",
    "\\Zm^{[1]} = \\Wm^{[1]} \\Xm + \\bv^{[1]}\n",
    "$$\n",
    "where $\\hat{\\yv} = \\av^{[2]\\top}$.\n",
    "\n",
    "Thus, if we expand $\\frac{\\partial MSE}{\\partial \\bv^{[1]}}$ we get the following equation:\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\bv^{[1]}} = \\frac{\\partial MSE}{\\partial \\av^{[2]}}\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\frac{\\partial \\zv^{[2]} }{\\partial  \\Am^{[1]} } \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} }\n",
    "$$\n",
    "\n",
    "Notice, the only new partial derivative is $\\frac{\\partial \\Zm^{[1]} }{\\partial \\bv^{[1]}}$, the rest we have already seen before. If we compute the partial derivatives we get the following:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\av^{[2]}} = (\\av^{[2]} - {\\yv}) \\\\\n",
    "\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} } = \\mathbf{1} \\\\\n",
    "\\frac{\\partial \\zv^{[2]} }{\\partial  \\Am^{[1]} }  = \\wv^{[2]} \\\\\n",
    "\\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } = g( \\Zm^{[1]}) (\\mathbf{1} - g( \\Zm^{[1]})) \\\\\n",
    " \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} } = \\mathbf{1}\n",
    "$$\n",
    "\n",
    "Heres a short description of what we did to find each of the NEW partial derivatives.\n",
    "1. 1. Computing $\\frac{\\partial \\Zm^{[1]} }{\\partial \\bv^{[1]} }$ simply requires taking derivative of the linear combination equation $\\Wm^{[1]} \\Xm + \\bv^{[1]}$. Here $\\Wm^{[1]} \\Xm$ act as constants and goes to zero as it does not depend on $\\bv^{[1]}$. Meanwhile, the derivative of $\\bv^{[1]}$ is simply an array of ones of shape (1, data samples). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8bead5",
   "metadata": {},
   "source": [
    "Turning all this into code looks as follows. Below are the biases before updating the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "acca8a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias['b1'] shape: (2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"bias['b1'] shape: {bias['b1'].shape}\")\n",
    "bias['b1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e3875",
   "metadata": {},
   "source": [
    "Next, we set each of the partial derivatives equal to their derivative where\n",
    "- $\\frac{\\partial MSE}{\\partial \\av^{[2]}}$ corresponds to  `delta_mse_A2` \n",
    "- $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ corresponds to `delta_A2_Z2`\n",
    "- $\\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} }$ corresponds to `delta_Z2_A1` \n",
    "- $\\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} }$ corresponds to `delta_A1_Z1`\n",
    "- $ \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} }$ corresponds to `delta_Z1_b1`\n",
    "\n",
    "Below is a short description of what we did to find each of the NEW partial derivatives.\n",
    "\n",
    "```Python\n",
    "delta_Z1_b1 = np.ones([1, len(y)])\n",
    "```\n",
    "We set `delta_Z2_b2` equal to an array of ones with a shape of (1, data samples=100). We do this because the derivative of $\\frac{\\partial \\zv^{[2]} }{\\partial \\bv^{[2]} }$ is one and we have to set the derivative for each data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "352ee149",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mse_A2 = delta_mse(y, y_hat)\n",
    "delta_A2_Z2 = Linear.derivative(Zs['Z2'])\n",
    "delta_Z2_A1 = weights['W2']\n",
    "delta_A1_Z1 = Sigmoid.derivative(Zs['Z1'])\n",
    "delta_Z1_b1 = np.ones([1, len(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b46bdfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y shape: (100, 1)\n",
      "y_hat shape: (100, 1)\n",
      "delta_mse_A2 shape: (100, 1)\n",
      "delta_A2_Z2 shape: (1, 100)\n",
      "delta_Z2_A1 shape: (1, 2)\n",
      "delta_A1_Z1 shape: (2, 100)\n",
      "delta_Z1_b1 shape: (1, 100)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y_hat shape: {y_hat.shape}\")\n",
    "print(f\"delta_mse_A2 shape: {delta_mse_A2.shape}\")\n",
    "print(f\"delta_A2_Z2 shape: {delta_A2_Z2.shape}\")\n",
    "print(f\"delta_Z2_A1 shape: {delta_Z2_A1.shape}\")\n",
    "print(f\"delta_A1_Z1 shape: {delta_A1_Z1.shape}\")\n",
    "print(f\"delta_Z1_b1 shape: {delta_Z1_b1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178e3722",
   "metadata": {},
   "source": [
    "Using the same equation format as the hidden layer weight update, we can compute $\\frac{\\partial MSE}{\\partial \\bv^{[1]}}$ by using the dot product and element-wise multiplication. Since we are working with vectors and matrices, we need to utilize the dot product and, in turn, rearrange some of the partial derivatives. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial \\bv^{[1]}} &= \\frac{\\partial MSE}{\\partial \\av^{[2]}} \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }  \\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} } \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} } \\\\\n",
    "\\\\\n",
    " &= \\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} }^\\top \\cdot \\big(\\frac{\\partial MSE}{\\partial \\av^{[2]}} * \\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }\\big) * \\frac{\\partial \\Am^{[1]}}{\\partial \\Zm^{[1]} } \\cdot \\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} }^\\top \\\\\n",
    " \\\\\n",
    "&=  \\Wm^{[2]\\top} \\cdot \\big ( (\\av^{[2]} - {\\yv} )^\\top * \\mathbf{1} \\big )  *\\big( ( \\Zm^{[1]}) (\\mathbf{1} - g( \\Zm^{[1]})) \\big ) \\cdot \\mathbf{1}^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "where $\\cdot$ represents the dot product and $*$ represents element-wise multiplication. Notice, that the equation inherently computes the sum of the gradients due to the addition of the dot product. This means we only need to divide by the number of data samples used to compute the gradient to get the average gradient.\n",
    "\n",
    "Further, we need to add some transposes to make the equation function properly. If we don't do this shape mismatch errors will be thrown.\n",
    "\n",
    "\n",
    "- First, we have to transpose $\\frac{\\partial MSE}{\\partial \\av^{[2]}}^\\top$ (i.e., `delta_mse_A2`). This will allow us to take the element-wise product $*$ with the partial derivative of the activation function $\\frac{\\partial \\av^{[2]}}{\\partial \\zv^{[2]} }$ (i.e., `delta_A2_Z2`) which has the shape (1, data samples=100).\n",
    "    - Original shape: (data samples=100, 1)\n",
    "    - Transposed shape: (1, data samples=100)\n",
    "\n",
    "- Second, we have to transpose $\\frac{\\partial \\zv^{[2]} }{\\partial \\Am^{[1]} }^\\top $  (i.e., `delta_Z2_A1`). This will allow us to take the dot product with $\\big( (\\av^{[2]} - {\\yv})^\\top * \\mathbf{1} \\big )$ which has shape (1 , data samples=100).\n",
    "    - Original shape: (neurons=1, inputs=2)\n",
    "    - Transposed shape: (inputs=2, neurons=1) \n",
    "\n",
    "- Third, we have to transpose $\\frac{\\partial \\Zm^{[1]}}{\\partial \\bv^{[1]} }^\\top$ (i.e., `delta_Z1_b1`). This will allow us to take the dot product with $\\Wm^{[2]\\top} \\cdot \\big ( (\\av^{[2]} - {\\yv} )^\\top * \\mathbf{1} \\big )  * \\big( \\Zm^{[1]} *(\\mathbf{1}  - g( \\Zm^{[1]}) \\big )$ which has the shape (neurons=2, data samples=100).\n",
    "    - Original shape: (1, data samples=100)\n",
    "    - Transposed shape: (data samples=100, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "81670987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03230379],\n",
       "       [-0.20405097]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_mse_A1 = delta_Z2_A1.T @ (delta_mse_A2.T * delta_A2_Z2)  \n",
    "delta_mse_b1 = (delta_mse_A1 * delta_A1_Z1) @ delta_Z1_b1.T\n",
    "b1_avg_grad = delta_mse_b1 / len(y)\n",
    "b1_avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c9d4cd",
   "metadata": {},
   "source": [
    "Below are the shapes of the variables used in the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3bf488fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 100) =  (2, 1) @ ((1, 100) * ((1, 100))\n",
      "(2, 1) =  ((2, 100) * (2, 100)) @ (100, 1)\n",
      "(2, 1) = (2, 1) / 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"{delta_mse_A1.shape} =  {delta_Z2_A1.T.shape} @ ({delta_mse_A2.T.shape} * ({delta_A2_Z2.shape})\")\n",
    "print(f\"{delta_mse_b1.shape} =  ({delta_mse_A1.shape} * {delta_A1_Z1.shape}) @ {delta_Z1_b1.T.shape}\")\n",
    "print(f\"{b1_avg_grad.shape} = {delta_mse_b1.shape} / {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7f123",
   "metadata": {},
   "source": [
    "Additionally, we add the implementation to the `get_hidden_layer1_bias_grads()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "637de2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_layer1_bias_grads(y, y_hat, weights, bias, Zs):\n",
    "    delta_mse_A2 = delta_mse(y, y_hat)\n",
    "    delta_A2_Z2 = Linear.derivative(Zs['Z2'])\n",
    "    delta_Z2_A1 = weights['W2']\n",
    "    delta_A1_Z1 = Sigmoid.derivative(Zs['Z1'])\n",
    "    delta_Z1_b1 = np.ones([1, len(y)])\n",
    "    \n",
    "    delta_mse_A1 = delta_Z2_A1.T @ (delta_mse_A2.T * delta_A2_Z2)  \n",
    "    delta_mse_b1 = (delta_mse_A1 * delta_A1_Z1) @ delta_Z1_b1.T\n",
    "    b1_avg_grad = delta_mse_b1 / len(y)\n",
    "    \n",
    "    return b1_avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "07cef2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03230379],\n",
       "       [-0.20405097]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_avg_grads = get_hidden_layer1_bias_grads(y, y_hat, weights, bias, Zs)\n",
    "b1_avg_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b8d09",
   "metadata": {},
   "source": [
    "Finally, below is an example of what the updated bias values `b1` would be if the gradient update equation was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "91395118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99676962],\n",
       "       [1.0204051 ]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = .1\n",
    "b1 = bias['b1'] - alpha * b1_avg_grad\n",
    "b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3efd6",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Now, to put everything together. Below we use mini-batch gradient descent to train our two layer neural networks. To do so, we define the `fit()` method. All this function does is run the `forward()` function to get the predictions and update the parameters using said predictions. \n",
    "\n",
    "To compute the gradients for each weight and bias in each layer we simply call the `get_output_layer_weight_grads()`, `get_output_layer_bias_grads()`, `get_hidden_layer1_weight_grads()`, and `get_hidden_layer1_bias_grads()` functions. After computing the gradients, we then apply the updates for each parameter. Finally, we compute the sum of squared error for each mini-batch and track the running SSE in `epoch_sse_loss`. Once an epoch is over, we compute the MSE for the entire dataset `X` by dividing `epoch_sse_loss` by the number of data samples $m$. We store each epoch's MSE into `epoch_mse_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a4d44f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mini_batches(data_len: int, \n",
    "                     batch_size: int = 32, \n",
    "                     seed: int = None) -> List[np.ndarray]:\n",
    "    \"\"\" Generates mini-batches based on the data indexes\n",
    "        \n",
    "        Args:\n",
    "            data_len: Length of the data\n",
    "            \n",
    "            batch_size: Size of each mini batch where the last mini-batch\n",
    "                might be smaller than the rest if the batch_size does not \n",
    "                evenly divide the data length.\n",
    "            \n",
    "            seed: Random seed for generating the same mini-batches.\n",
    "    \n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    X_idx = np.arange(data_len)\n",
    "    rng.shuffle(X_idx)\n",
    "    batches = [X_idx[i:i+batch_size] for i in range(0, data_len, batch_size)]\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d85062eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    hidden_neurons: int,\n",
    "    output_neurons: int = 1, \n",
    "    batch_size: int = 32, \n",
    "    alpha: float = .1, \n",
    "    epochs: int =1\n",
    "): \n",
    "    \"\"\" Training function for a two layer neural network \"\"\"\n",
    "    m = len(X)\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    weights, bias = init_weights(\n",
    "        n_input_features=X.shape[1],\n",
    "        hidden_neurons=hidden_neurons,\n",
    "        output_neurons=output_neurons\n",
    "    )\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print(f\"Epoch: {e+1}\")\n",
    "        batches = get_mini_batches(data_len=m, batch_size=batch_size)\n",
    "        epoch_sse_loss = 0\n",
    "        for mb in batches:\n",
    "            # Forward pass to get predictions\n",
    "            y_hat, Zs, As = forward(X[mb], weights=weights, bias=bias)\n",
    "\n",
    "            # Backward pass to update weights and biases\n",
    "            W2_avg_grad = get_output_layer_weight_grads(y[mb], y_hat, Zs, As)\n",
    "            b2_avg_grad = get_output_layer_bias_grads(y[mb], y_hat, Zs)\n",
    "            W1_avg_grads = get_hidden_layer1_weight_grads(X[mb], y[mb], y_hat, weights, bias, Zs)\n",
    "            b1_avg_grads = get_hidden_layer1_bias_grads(y[mb], y_hat, weights, bias, Zs)\n",
    "\n",
    "            # Update output layer's weights and biases\n",
    "            weights['W2'] -= alpha * W2_avg_grad\n",
    "            bias['b2'] -=  alpha * b2_avg_grad\n",
    "            # Update hidden layer's weights and biases\n",
    "            weights['W1'] -= alpha * W1_avg_grads\n",
    "            bias['b1'] -= alpha * b1_avg_grads\n",
    "            \n",
    "            # Compute squared error for current mini-batch\n",
    "            batch_sse_loss = sse(y[mb], y_hat)\n",
    "            epoch_sse_loss += batch_sse_loss\n",
    "            \n",
    "        # Compute MSE for entire dataset\n",
    "        epoch_mse_loss = epoch_sse_loss / m\n",
    "        print(f\"\\t MSE train loss: {epoch_mse_loss}\")\n",
    "        epoch_losses.append(epoch_mse_loss)\n",
    "        \n",
    "    return weights, bias, epoch_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b548f",
   "metadata": {},
   "source": [
    "Below we train the neural network by calling the `fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3bda34e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\t MSE train loss: 16.449990753880435\n",
      "Epoch: 2\n",
      "\t MSE train loss: 4.549560983977363\n",
      "Epoch: 3\n",
      "\t MSE train loss: 4.193221062092258\n",
      "Epoch: 4\n",
      "\t MSE train loss: 3.9378276339683658\n",
      "Epoch: 5\n",
      "\t MSE train loss: 3.618077366708817\n",
      "Epoch: 6\n",
      "\t MSE train loss: 3.5460976168368643\n",
      "Epoch: 7\n",
      "\t MSE train loss: 3.2367278768822065\n",
      "Epoch: 8\n",
      "\t MSE train loss: 3.056276784418658\n",
      "Epoch: 9\n",
      "\t MSE train loss: 2.895021778191542\n",
      "Epoch: 10\n",
      "\t MSE train loss: 2.7887999597168847\n",
      "Epoch: 11\n",
      "\t MSE train loss: 2.698668306108599\n",
      "Epoch: 12\n",
      "\t MSE train loss: 2.621336524133603\n",
      "Epoch: 13\n",
      "\t MSE train loss: 2.566868801058394\n",
      "Epoch: 14\n",
      "\t MSE train loss: 2.479515622310063\n",
      "Epoch: 15\n",
      "\t MSE train loss: 2.420526773154072\n",
      "Epoch: 16\n",
      "\t MSE train loss: 2.3768261447457686\n",
      "Epoch: 17\n",
      "\t MSE train loss: 2.3279661684634685\n",
      "Epoch: 18\n",
      "\t MSE train loss: 2.3002839885576476\n",
      "Epoch: 19\n",
      "\t MSE train loss: 2.3126950996166307\n",
      "Epoch: 20\n",
      "\t MSE train loss: 2.3146260823290197\n",
      "Epoch: 21\n",
      "\t MSE train loss: 2.1944456242962516\n",
      "Epoch: 22\n",
      "\t MSE train loss: 2.1741156656787566\n",
      "Epoch: 23\n",
      "\t MSE train loss: 2.199897602186714\n",
      "Epoch: 24\n",
      "\t MSE train loss: 2.225072159317886\n",
      "Epoch: 25\n",
      "\t MSE train loss: 2.09733268243609\n",
      "Epoch: 26\n",
      "\t MSE train loss: 2.0801551267892116\n",
      "Epoch: 27\n",
      "\t MSE train loss: 2.100516365793956\n",
      "Epoch: 28\n",
      "\t MSE train loss: 2.068234535904771\n",
      "Epoch: 29\n",
      "\t MSE train loss: 2.054273528279077\n",
      "Epoch: 30\n",
      "\t MSE train loss: 2.0891938766183955\n",
      "Epoch: 31\n",
      "\t MSE train loss: 1.959679006046938\n",
      "Epoch: 32\n",
      "\t MSE train loss: 1.9414106289965354\n",
      "Epoch: 33\n",
      "\t MSE train loss: 1.920152142032422\n",
      "Epoch: 34\n",
      "\t MSE train loss: 1.9007392061335964\n",
      "Epoch: 35\n",
      "\t MSE train loss: 2.0221929408142323\n",
      "Epoch: 36\n",
      "\t MSE train loss: 2.0936638518883415\n",
      "Epoch: 37\n",
      "\t MSE train loss: 1.8696193852827392\n",
      "Epoch: 38\n",
      "\t MSE train loss: 1.8097577750318765\n",
      "Epoch: 39\n",
      "\t MSE train loss: 1.801204312631571\n",
      "Epoch: 40\n",
      "\t MSE train loss: 1.7873132845923596\n",
      "Epoch: 41\n",
      "\t MSE train loss: 1.8390027749192654\n",
      "Epoch: 42\n",
      "\t MSE train loss: 1.8859774813350747\n",
      "Epoch: 43\n",
      "\t MSE train loss: 1.8457004152679062\n",
      "Epoch: 44\n",
      "\t MSE train loss: 1.7804898418085258\n",
      "Epoch: 45\n",
      "\t MSE train loss: 1.7207211858439249\n",
      "Epoch: 46\n",
      "\t MSE train loss: 1.665428648389512\n",
      "Epoch: 47\n",
      "\t MSE train loss: 1.7519233804504761\n",
      "Epoch: 48\n",
      "\t MSE train loss: 1.7380916661383958\n",
      "Epoch: 49\n",
      "\t MSE train loss: 1.6117573768925493\n",
      "Epoch: 50\n",
      "\t MSE train loss: 1.5915401163962712\n",
      "Epoch: 51\n",
      "\t MSE train loss: 1.5756164372530403\n",
      "Epoch: 52\n",
      "\t MSE train loss: 1.5736448790653261\n",
      "Epoch: 53\n",
      "\t MSE train loss: 1.5506386198018265\n",
      "Epoch: 54\n",
      "\t MSE train loss: 1.5321403228632398\n",
      "Epoch: 55\n",
      "\t MSE train loss: 1.541038920512585\n",
      "Epoch: 56\n",
      "\t MSE train loss: 1.5168607297327958\n",
      "Epoch: 57\n",
      "\t MSE train loss: 1.4814847429072262\n",
      "Epoch: 58\n",
      "\t MSE train loss: 1.4664058990589797\n",
      "Epoch: 59\n",
      "\t MSE train loss: 1.5493706046846212\n",
      "Epoch: 60\n",
      "\t MSE train loss: 1.4932252155451697\n",
      "Epoch: 61\n",
      "\t MSE train loss: 1.4136830770621214\n",
      "Epoch: 62\n",
      "\t MSE train loss: 1.4049627379961585\n",
      "Epoch: 63\n",
      "\t MSE train loss: 1.3825344939416886\n",
      "Epoch: 64\n",
      "\t MSE train loss: 1.3574374617261986\n",
      "Epoch: 65\n",
      "\t MSE train loss: 1.3582389230373204\n",
      "Epoch: 66\n",
      "\t MSE train loss: 1.3491650343541055\n",
      "Epoch: 67\n",
      "\t MSE train loss: 1.3835186058410756\n",
      "Epoch: 68\n",
      "\t MSE train loss: 1.3383378410755733\n",
      "Epoch: 69\n",
      "\t MSE train loss: 1.3215513873051652\n",
      "Epoch: 70\n",
      "\t MSE train loss: 1.2956329952166932\n",
      "Epoch: 71\n",
      "\t MSE train loss: 1.3592652900418207\n",
      "Epoch: 72\n",
      "\t MSE train loss: 1.2631654206035932\n",
      "Epoch: 73\n",
      "\t MSE train loss: 1.2872525339072687\n",
      "Epoch: 74\n",
      "\t MSE train loss: 1.2323096394133182\n",
      "Epoch: 75\n",
      "\t MSE train loss: 1.2033458466189664\n",
      "Epoch: 76\n",
      "\t MSE train loss: 1.1910657338250106\n",
      "Epoch: 77\n",
      "\t MSE train loss: 1.1826523548969963\n",
      "Epoch: 78\n",
      "\t MSE train loss: 1.1687719956555171\n",
      "Epoch: 79\n",
      "\t MSE train loss: 1.1556654050195383\n",
      "Epoch: 80\n",
      "\t MSE train loss: 1.1593330595951772\n",
      "Epoch: 81\n",
      "\t MSE train loss: 1.1986725181825268\n",
      "Epoch: 82\n",
      "\t MSE train loss: 1.1350033597164335\n",
      "Epoch: 83\n",
      "\t MSE train loss: 1.1900977665258967\n",
      "Epoch: 84\n",
      "\t MSE train loss: 1.1252345132441461\n",
      "Epoch: 85\n",
      "\t MSE train loss: 1.1020402294055665\n",
      "Epoch: 86\n",
      "\t MSE train loss: 1.084318539024855\n",
      "Epoch: 87\n",
      "\t MSE train loss: 1.163142971185074\n",
      "Epoch: 88\n",
      "\t MSE train loss: 1.284231993032575\n",
      "Epoch: 89\n",
      "\t MSE train loss: 1.1413762505440568\n",
      "Epoch: 90\n",
      "\t MSE train loss: 1.0502321197613218\n",
      "Epoch: 91\n",
      "\t MSE train loss: 1.0200855344879214\n",
      "Epoch: 92\n",
      "\t MSE train loss: 1.0346021686568139\n",
      "Epoch: 93\n",
      "\t MSE train loss: 1.0139180614809171\n",
      "Epoch: 94\n",
      "\t MSE train loss: 0.9922344466321539\n",
      "Epoch: 95\n",
      "\t MSE train loss: 1.0329927146931266\n",
      "Epoch: 96\n",
      "\t MSE train loss: 1.0622028409513447\n",
      "Epoch: 97\n",
      "\t MSE train loss: 1.0013847131097118\n",
      "Epoch: 98\n",
      "\t MSE train loss: 0.9644253560413495\n",
      "Epoch: 99\n",
      "\t MSE train loss: 0.9346325023076745\n",
      "Epoch: 100\n",
      "\t MSE train loss: 0.9385498280646617\n",
      "Epoch: 101\n",
      "\t MSE train loss: 0.9173866182772332\n",
      "Epoch: 102\n",
      "\t MSE train loss: 0.9192924307323983\n",
      "Epoch: 103\n",
      "\t MSE train loss: 0.8960537292118117\n",
      "Epoch: 104\n",
      "\t MSE train loss: 0.9045096809357386\n",
      "Epoch: 105\n",
      "\t MSE train loss: 0.8956855189275895\n",
      "Epoch: 106\n",
      "\t MSE train loss: 0.8943331372520847\n",
      "Epoch: 107\n",
      "\t MSE train loss: 0.8755915180403856\n",
      "Epoch: 108\n",
      "\t MSE train loss: 0.876214963273878\n",
      "Epoch: 109\n",
      "\t MSE train loss: 0.925780138796294\n",
      "Epoch: 110\n",
      "\t MSE train loss: 0.849020488724228\n",
      "Epoch: 111\n",
      "\t MSE train loss: 0.8415470219472166\n",
      "Epoch: 112\n",
      "\t MSE train loss: 0.8204416372735522\n",
      "Epoch: 113\n",
      "\t MSE train loss: 0.8216059188028754\n",
      "Epoch: 114\n",
      "\t MSE train loss: 0.8004676584056495\n",
      "Epoch: 115\n",
      "\t MSE train loss: 0.7874199368940942\n",
      "Epoch: 116\n",
      "\t MSE train loss: 0.804777816499041\n",
      "Epoch: 117\n",
      "\t MSE train loss: 0.8117933139787435\n",
      "Epoch: 118\n",
      "\t MSE train loss: 0.7664255109229609\n",
      "Epoch: 119\n",
      "\t MSE train loss: 0.7822829945327932\n",
      "Epoch: 120\n",
      "\t MSE train loss: 0.7795336245488437\n",
      "Epoch: 121\n",
      "\t MSE train loss: 0.7411484992928072\n",
      "Epoch: 122\n",
      "\t MSE train loss: 0.7397719562260994\n",
      "Epoch: 123\n",
      "\t MSE train loss: 0.7474382731505814\n",
      "Epoch: 124\n",
      "\t MSE train loss: 0.7226264795371563\n",
      "Epoch: 125\n",
      "\t MSE train loss: 0.7205644215450002\n",
      "Epoch: 126\n",
      "\t MSE train loss: 0.7067244499165407\n",
      "Epoch: 127\n",
      "\t MSE train loss: 0.7325866757383486\n",
      "Epoch: 128\n",
      "\t MSE train loss: 0.7260673404273844\n",
      "Epoch: 129\n",
      "\t MSE train loss: 0.6962650561107299\n",
      "Epoch: 130\n",
      "\t MSE train loss: 0.6831315456647978\n",
      "Epoch: 131\n",
      "\t MSE train loss: 0.6882088380232884\n",
      "Epoch: 132\n",
      "\t MSE train loss: 0.698061753966082\n",
      "Epoch: 133\n",
      "\t MSE train loss: 0.6728390048902861\n",
      "Epoch: 134\n",
      "\t MSE train loss: 0.6506367764906537\n",
      "Epoch: 135\n",
      "\t MSE train loss: 0.6921990955086156\n",
      "Epoch: 136\n",
      "\t MSE train loss: 0.6611621966034527\n",
      "Epoch: 137\n",
      "\t MSE train loss: 0.6630783408252335\n",
      "Epoch: 138\n",
      "\t MSE train loss: 0.6740990946808029\n",
      "Epoch: 139\n",
      "\t MSE train loss: 0.630865487023641\n",
      "Epoch: 140\n",
      "\t MSE train loss: 0.62389715108574\n",
      "Epoch: 141\n",
      "\t MSE train loss: 0.6253044789622559\n",
      "Epoch: 142\n",
      "\t MSE train loss: 0.6027125559870438\n",
      "Epoch: 143\n",
      "\t MSE train loss: 0.6060025335534757\n",
      "Epoch: 144\n",
      "\t MSE train loss: 0.5865337531262533\n",
      "Epoch: 145\n",
      "\t MSE train loss: 0.582558699130832\n",
      "Epoch: 146\n",
      "\t MSE train loss: 0.5805346193345471\n",
      "Epoch: 147\n",
      "\t MSE train loss: 0.6093027172357631\n",
      "Epoch: 148\n",
      "\t MSE train loss: 0.5644494320295812\n",
      "Epoch: 149\n",
      "\t MSE train loss: 0.5857253324848422\n",
      "Epoch: 150\n",
      "\t MSE train loss: 0.6031951550412298\n",
      "Epoch: 151\n",
      "\t MSE train loss: 0.5601136598760804\n",
      "Epoch: 152\n",
      "\t MSE train loss: 0.5437397666060818\n",
      "Epoch: 153\n",
      "\t MSE train loss: 0.5379070782295905\n",
      "Epoch: 154\n",
      "\t MSE train loss: 0.5571272452190771\n",
      "Epoch: 155\n",
      "\t MSE train loss: 0.5800212584299179\n",
      "Epoch: 156\n",
      "\t MSE train loss: 0.5464369129967794\n",
      "Epoch: 157\n",
      "\t MSE train loss: 0.5326228475769418\n",
      "Epoch: 158\n",
      "\t MSE train loss: 0.5148281932366094\n",
      "Epoch: 159\n",
      "\t MSE train loss: 0.5194420276656984\n",
      "Epoch: 160\n",
      "\t MSE train loss: 0.5035923330451412\n",
      "Epoch: 161\n",
      "\t MSE train loss: 0.4973810183004007\n",
      "Epoch: 162\n",
      "\t MSE train loss: 0.49616137294096846\n",
      "Epoch: 163\n",
      "\t MSE train loss: 0.4967176334819158\n",
      "Epoch: 164\n",
      "\t MSE train loss: 0.4893529577966863\n",
      "Epoch: 165\n",
      "\t MSE train loss: 0.48939852806279865\n",
      "Epoch: 166\n",
      "\t MSE train loss: 0.479574760167596\n",
      "Epoch: 167\n",
      "\t MSE train loss: 0.49469070100947066\n",
      "Epoch: 168\n",
      "\t MSE train loss: 0.4705396026344138\n",
      "Epoch: 169\n",
      "\t MSE train loss: 0.46635378593457644\n",
      "Epoch: 170\n",
      "\t MSE train loss: 0.4601936888416776\n",
      "Epoch: 171\n",
      "\t MSE train loss: 0.4630280446814281\n",
      "Epoch: 172\n",
      "\t MSE train loss: 0.45029365880394706\n",
      "Epoch: 173\n",
      "\t MSE train loss: 0.46537603513051773\n",
      "Epoch: 174\n",
      "\t MSE train loss: 0.45119485882896404\n",
      "Epoch: 175\n",
      "\t MSE train loss: 0.4409166591370473\n",
      "Epoch: 176\n",
      "\t MSE train loss: 0.4340375153754899\n",
      "Epoch: 177\n",
      "\t MSE train loss: 0.4309153599181388\n",
      "Epoch: 178\n",
      "\t MSE train loss: 0.43001565688892085\n",
      "Epoch: 179\n",
      "\t MSE train loss: 0.4365434929234604\n",
      "Epoch: 180\n",
      "\t MSE train loss: 0.42053354893204786\n",
      "Epoch: 181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t MSE train loss: 0.41514538094766495\n",
      "Epoch: 182\n",
      "\t MSE train loss: 0.4149055724811421\n",
      "Epoch: 183\n",
      "\t MSE train loss: 0.4247373821166949\n",
      "Epoch: 184\n",
      "\t MSE train loss: 0.4446857226010513\n",
      "Epoch: 185\n",
      "\t MSE train loss: 0.421167204355591\n",
      "Epoch: 186\n",
      "\t MSE train loss: 0.3968466333991475\n",
      "Epoch: 187\n",
      "\t MSE train loss: 0.3952190183635511\n",
      "Epoch: 188\n",
      "\t MSE train loss: 0.3985683653071304\n",
      "Epoch: 189\n",
      "\t MSE train loss: 0.3926717134349758\n",
      "Epoch: 190\n",
      "\t MSE train loss: 0.38466337736617573\n",
      "Epoch: 191\n",
      "\t MSE train loss: 0.38435976858552656\n",
      "Epoch: 192\n",
      "\t MSE train loss: 0.3791293927102647\n",
      "Epoch: 193\n",
      "\t MSE train loss: 0.37825722980046433\n",
      "Epoch: 194\n",
      "\t MSE train loss: 0.39885545943189954\n",
      "Epoch: 195\n",
      "\t MSE train loss: 0.37962718500123843\n",
      "Epoch: 196\n",
      "\t MSE train loss: 0.37055850418465425\n",
      "Epoch: 197\n",
      "\t MSE train loss: 0.36305986568096216\n",
      "Epoch: 198\n",
      "\t MSE train loss: 0.367317398759969\n",
      "Epoch: 199\n",
      "\t MSE train loss: 0.36910673616573414\n",
      "Epoch: 200\n",
      "\t MSE train loss: 0.35623976399571433\n",
      "Epoch: 201\n",
      "\t MSE train loss: 0.35779166312317917\n",
      "Epoch: 202\n",
      "\t MSE train loss: 0.37385440464644887\n",
      "Epoch: 203\n",
      "\t MSE train loss: 0.3467094664802998\n",
      "Epoch: 204\n",
      "\t MSE train loss: 0.3468909871394551\n",
      "Epoch: 205\n",
      "\t MSE train loss: 0.341907143777467\n",
      "Epoch: 206\n",
      "\t MSE train loss: 0.33860906738905\n",
      "Epoch: 207\n",
      "\t MSE train loss: 0.335773135764268\n",
      "Epoch: 208\n",
      "\t MSE train loss: 0.33377344319302765\n",
      "Epoch: 209\n",
      "\t MSE train loss: 0.3359145791093034\n",
      "Epoch: 210\n",
      "\t MSE train loss: 0.32806493139450466\n",
      "Epoch: 211\n",
      "\t MSE train loss: 0.3277233675624425\n",
      "Epoch: 212\n",
      "\t MSE train loss: 0.32501061150997373\n",
      "Epoch: 213\n",
      "\t MSE train loss: 0.3296366860189543\n",
      "Epoch: 214\n",
      "\t MSE train loss: 0.3211121241290076\n",
      "Epoch: 215\n",
      "\t MSE train loss: 0.3187103441204692\n",
      "Epoch: 216\n",
      "\t MSE train loss: 0.3259192258529597\n",
      "Epoch: 217\n",
      "\t MSE train loss: 0.31438609297802783\n",
      "Epoch: 218\n",
      "\t MSE train loss: 0.3421315923152753\n",
      "Epoch: 219\n",
      "\t MSE train loss: 0.320347974116118\n",
      "Epoch: 220\n",
      "\t MSE train loss: 0.3514371338832752\n",
      "Epoch: 221\n",
      "\t MSE train loss: 0.31465228180178306\n",
      "Epoch: 222\n",
      "\t MSE train loss: 0.30433810779389625\n",
      "Epoch: 223\n",
      "\t MSE train loss: 0.3049013375545542\n",
      "Epoch: 224\n",
      "\t MSE train loss: 0.30415205763154546\n",
      "Epoch: 225\n",
      "\t MSE train loss: 0.30185511996739756\n",
      "Epoch: 226\n",
      "\t MSE train loss: 0.2969853967797645\n",
      "Epoch: 227\n",
      "\t MSE train loss: 0.311612666918986\n",
      "Epoch: 228\n",
      "\t MSE train loss: 0.3349279266837057\n",
      "Epoch: 229\n",
      "\t MSE train loss: 0.3068533716776709\n",
      "Epoch: 230\n",
      "\t MSE train loss: 0.2980447530298856\n",
      "Epoch: 231\n",
      "\t MSE train loss: 0.2857260730216142\n",
      "Epoch: 232\n",
      "\t MSE train loss: 0.2899684832225715\n",
      "Epoch: 233\n",
      "\t MSE train loss: 0.28769585321500174\n",
      "Epoch: 234\n",
      "\t MSE train loss: 0.29001519691355054\n",
      "Epoch: 235\n",
      "\t MSE train loss: 0.29010009536978404\n",
      "Epoch: 236\n",
      "\t MSE train loss: 0.28139024155516623\n",
      "Epoch: 237\n",
      "\t MSE train loss: 0.2745397755979799\n",
      "Epoch: 238\n",
      "\t MSE train loss: 0.2775986541535592\n",
      "Epoch: 239\n",
      "\t MSE train loss: 0.27444442219284704\n",
      "Epoch: 240\n",
      "\t MSE train loss: 0.28028274297054645\n",
      "Epoch: 241\n",
      "\t MSE train loss: 0.2701202219195206\n",
      "Epoch: 242\n",
      "\t MSE train loss: 0.27632456796217536\n",
      "Epoch: 243\n",
      "\t MSE train loss: 0.27592258556617355\n",
      "Epoch: 244\n",
      "\t MSE train loss: 0.27465699348184464\n",
      "Epoch: 245\n",
      "\t MSE train loss: 0.2687270591859276\n",
      "Epoch: 246\n",
      "\t MSE train loss: 0.2747288425950746\n",
      "Epoch: 247\n",
      "\t MSE train loss: 0.26869359677173177\n",
      "Epoch: 248\n",
      "\t MSE train loss: 0.2628670663356669\n",
      "Epoch: 249\n",
      "\t MSE train loss: 0.2648169336971633\n",
      "Epoch: 250\n",
      "\t MSE train loss: 0.2676521127032164\n",
      "Epoch: 251\n",
      "\t MSE train loss: 0.2590471790452027\n",
      "Epoch: 252\n",
      "\t MSE train loss: 0.2988280744559915\n",
      "Epoch: 253\n",
      "\t MSE train loss: 0.28939437767671367\n",
      "Epoch: 254\n",
      "\t MSE train loss: 0.2553408276092377\n",
      "Epoch: 255\n",
      "\t MSE train loss: 0.25489825313744663\n",
      "Epoch: 256\n",
      "\t MSE train loss: 0.24888787136129223\n",
      "Epoch: 257\n",
      "\t MSE train loss: 0.25523139901224573\n",
      "Epoch: 258\n",
      "\t MSE train loss: 0.2831105034736145\n",
      "Epoch: 259\n",
      "\t MSE train loss: 0.269246485805698\n",
      "Epoch: 260\n",
      "\t MSE train loss: 0.2459408872723982\n",
      "Epoch: 261\n",
      "\t MSE train loss: 0.2413954122615541\n",
      "Epoch: 262\n",
      "\t MSE train loss: 0.24306531341575188\n",
      "Epoch: 263\n",
      "\t MSE train loss: 0.2566762183356364\n",
      "Epoch: 264\n",
      "\t MSE train loss: 0.26012590121544266\n",
      "Epoch: 265\n",
      "\t MSE train loss: 0.24750595808584364\n",
      "Epoch: 266\n",
      "\t MSE train loss: 0.244036935270004\n",
      "Epoch: 267\n",
      "\t MSE train loss: 0.2366920747347418\n",
      "Epoch: 268\n",
      "\t MSE train loss: 0.23547192631755748\n",
      "Epoch: 269\n",
      "\t MSE train loss: 0.23702385970525014\n",
      "Epoch: 270\n",
      "\t MSE train loss: 0.2422146356115894\n",
      "Epoch: 271\n",
      "\t MSE train loss: 0.2324260215035909\n",
      "Epoch: 272\n",
      "\t MSE train loss: 0.22992893800352923\n",
      "Epoch: 273\n",
      "\t MSE train loss: 0.2503322550862477\n",
      "Epoch: 274\n",
      "\t MSE train loss: 0.266946750919524\n",
      "Epoch: 275\n",
      "\t MSE train loss: 0.25015474093784656\n",
      "Epoch: 276\n",
      "\t MSE train loss: 0.25349445110018803\n",
      "Epoch: 277\n",
      "\t MSE train loss: 0.24905094211658071\n",
      "Epoch: 278\n",
      "\t MSE train loss: 0.23019065895587937\n",
      "Epoch: 279\n",
      "\t MSE train loss: 0.2345238183868974\n",
      "Epoch: 280\n",
      "\t MSE train loss: 0.2375425448005849\n",
      "Epoch: 281\n",
      "\t MSE train loss: 0.23891024887294726\n",
      "Epoch: 282\n",
      "\t MSE train loss: 0.22367508233338054\n",
      "Epoch: 283\n",
      "\t MSE train loss: 0.22175623386841775\n",
      "Epoch: 284\n",
      "\t MSE train loss: 0.22022460078910064\n",
      "Epoch: 285\n",
      "\t MSE train loss: 0.21852826384996418\n",
      "Epoch: 286\n",
      "\t MSE train loss: 0.22855151042813077\n",
      "Epoch: 287\n",
      "\t MSE train loss: 0.24537703947449038\n",
      "Epoch: 288\n",
      "\t MSE train loss: 0.22658758002769816\n",
      "Epoch: 289\n",
      "\t MSE train loss: 0.21532699923171778\n",
      "Epoch: 290\n",
      "\t MSE train loss: 0.21644043095640988\n",
      "Epoch: 291\n",
      "\t MSE train loss: 0.21867895196187936\n",
      "Epoch: 292\n",
      "\t MSE train loss: 0.21654242470331503\n",
      "Epoch: 293\n",
      "\t MSE train loss: 0.21227675014321165\n",
      "Epoch: 294\n",
      "\t MSE train loss: 0.2256984912863201\n",
      "Epoch: 295\n",
      "\t MSE train loss: 0.2503151484755282\n",
      "Epoch: 296\n",
      "\t MSE train loss: 0.21094544985282748\n",
      "Epoch: 297\n",
      "\t MSE train loss: 0.2129712149266182\n",
      "Epoch: 298\n",
      "\t MSE train loss: 0.20948839526327515\n",
      "Epoch: 299\n",
      "\t MSE train loss: 0.2096793589507668\n",
      "Epoch: 300\n",
      "\t MSE train loss: 0.20734469130254266\n",
      "Epoch: 301\n",
      "\t MSE train loss: 0.2111195004294955\n",
      "Epoch: 302\n",
      "\t MSE train loss: 0.23305844613399004\n",
      "Epoch: 303\n",
      "\t MSE train loss: 0.22475684522904507\n",
      "Epoch: 304\n",
      "\t MSE train loss: 0.20658024359657032\n",
      "Epoch: 305\n",
      "\t MSE train loss: 0.20633882605661988\n",
      "Epoch: 306\n",
      "\t MSE train loss: 0.20415570072637335\n",
      "Epoch: 307\n",
      "\t MSE train loss: 0.20755190935542175\n",
      "Epoch: 308\n",
      "\t MSE train loss: 0.21435898930288771\n",
      "Epoch: 309\n",
      "\t MSE train loss: 0.21459253733980826\n",
      "Epoch: 310\n",
      "\t MSE train loss: 0.22207215440691874\n",
      "Epoch: 311\n",
      "\t MSE train loss: 0.26253901077026554\n",
      "Epoch: 312\n",
      "\t MSE train loss: 0.2098717220935783\n",
      "Epoch: 313\n",
      "\t MSE train loss: 0.2046611544187502\n",
      "Epoch: 314\n",
      "\t MSE train loss: 0.20313326595498424\n",
      "Epoch: 315\n",
      "\t MSE train loss: 0.2019116680927904\n",
      "Epoch: 316\n",
      "\t MSE train loss: 0.19937166226541994\n",
      "Epoch: 317\n",
      "\t MSE train loss: 0.21150042001855088\n",
      "Epoch: 318\n",
      "\t MSE train loss: 0.2044855179649036\n",
      "Epoch: 319\n",
      "\t MSE train loss: 0.2007400255350715\n",
      "Epoch: 320\n",
      "\t MSE train loss: 0.20541379490646475\n",
      "Epoch: 321\n",
      "\t MSE train loss: 0.20278875786315254\n",
      "Epoch: 322\n",
      "\t MSE train loss: 0.2072363015962136\n",
      "Epoch: 323\n",
      "\t MSE train loss: 0.19772117851738044\n",
      "Epoch: 324\n",
      "\t MSE train loss: 0.20310362963529863\n",
      "Epoch: 325\n",
      "\t MSE train loss: 0.2133144733406363\n",
      "Epoch: 326\n",
      "\t MSE train loss: 0.2006395070095623\n",
      "Epoch: 327\n",
      "\t MSE train loss: 0.19626108349384347\n",
      "Epoch: 328\n",
      "\t MSE train loss: 0.19608819857312715\n",
      "Epoch: 329\n",
      "\t MSE train loss: 0.21170487570873905\n",
      "Epoch: 330\n",
      "\t MSE train loss: 0.24514380349317336\n",
      "Epoch: 331\n",
      "\t MSE train loss: 0.23198793782039512\n",
      "Epoch: 332\n",
      "\t MSE train loss: 0.20429076962617315\n",
      "Epoch: 333\n",
      "\t MSE train loss: 0.19846889718588795\n",
      "Epoch: 334\n",
      "\t MSE train loss: 0.20900169196001284\n",
      "Epoch: 335\n",
      "\t MSE train loss: 0.19132785927874973\n",
      "Epoch: 336\n",
      "\t MSE train loss: 0.18971231219540624\n",
      "Epoch: 337\n",
      "\t MSE train loss: 0.19051211408761554\n",
      "Epoch: 338\n",
      "\t MSE train loss: 0.1881153162968813\n",
      "Epoch: 339\n",
      "\t MSE train loss: 0.1936684397096363\n",
      "Epoch: 340\n",
      "\t MSE train loss: 0.1906313199308455\n",
      "Epoch: 341\n",
      "\t MSE train loss: 0.1930708965932886\n",
      "Epoch: 342\n",
      "\t MSE train loss: 0.19294008201763962\n",
      "Epoch: 343\n",
      "\t MSE train loss: 0.1915004193653072\n",
      "Epoch: 344\n",
      "\t MSE train loss: 0.1881189653250901\n",
      "Epoch: 345\n",
      "\t MSE train loss: 0.19839202317294014\n",
      "Epoch: 346\n",
      "\t MSE train loss: 0.19995779916023895\n",
      "Epoch: 347\n",
      "\t MSE train loss: 0.18827817949646486\n",
      "Epoch: 348\n",
      "\t MSE train loss: 0.18653561959987108\n",
      "Epoch: 349\n",
      "\t MSE train loss: 0.18973347671204358\n",
      "Epoch: 350\n",
      "\t MSE train loss: 0.18408244649025357\n",
      "Epoch: 351\n",
      "\t MSE train loss: 0.1830384525240391\n",
      "Epoch: 352\n",
      "\t MSE train loss: 0.18339488728741188\n",
      "Epoch: 353\n",
      "\t MSE train loss: 0.18424888639862202\n",
      "Epoch: 354\n",
      "\t MSE train loss: 0.18219945734653759\n",
      "Epoch: 355\n",
      "\t MSE train loss: 0.1836209382197663\n",
      "Epoch: 356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t MSE train loss: 0.18141102795068065\n",
      "Epoch: 357\n",
      "\t MSE train loss: 0.19304426672187414\n",
      "Epoch: 358\n",
      "\t MSE train loss: 0.1969488312757738\n",
      "Epoch: 359\n",
      "\t MSE train loss: 0.18072667259581693\n",
      "Epoch: 360\n",
      "\t MSE train loss: 0.1860689409795868\n",
      "Epoch: 361\n",
      "\t MSE train loss: 0.18395711848634305\n",
      "Epoch: 362\n",
      "\t MSE train loss: 0.18234280498895764\n",
      "Epoch: 363\n",
      "\t MSE train loss: 0.18304001188282767\n",
      "Epoch: 364\n",
      "\t MSE train loss: 0.19128550416960283\n",
      "Epoch: 365\n",
      "\t MSE train loss: 0.18039033808249674\n",
      "Epoch: 366\n",
      "\t MSE train loss: 0.18108779119330465\n",
      "Epoch: 367\n",
      "\t MSE train loss: 0.17970055470111757\n",
      "Epoch: 368\n",
      "\t MSE train loss: 0.19898948386875556\n",
      "Epoch: 369\n",
      "\t MSE train loss: 0.18730343506878114\n",
      "Epoch: 370\n",
      "\t MSE train loss: 0.17765313967801094\n",
      "Epoch: 371\n",
      "\t MSE train loss: 0.17813171391708507\n",
      "Epoch: 372\n",
      "\t MSE train loss: 0.17905339077879132\n",
      "Epoch: 373\n",
      "\t MSE train loss: 0.1984444395043126\n",
      "Epoch: 374\n",
      "\t MSE train loss: 0.19494402794829416\n",
      "Epoch: 375\n",
      "\t MSE train loss: 0.17780704827664195\n",
      "Epoch: 376\n",
      "\t MSE train loss: 0.18082298589520707\n",
      "Epoch: 377\n",
      "\t MSE train loss: 0.18588047794618856\n",
      "Epoch: 378\n",
      "\t MSE train loss: 0.17531325448282575\n",
      "Epoch: 379\n",
      "\t MSE train loss: 0.17541233871455905\n",
      "Epoch: 380\n",
      "\t MSE train loss: 0.17663105124945308\n",
      "Epoch: 381\n",
      "\t MSE train loss: 0.18079656945588318\n",
      "Epoch: 382\n",
      "\t MSE train loss: 0.17582369299624814\n",
      "Epoch: 383\n",
      "\t MSE train loss: 0.19517520115703912\n",
      "Epoch: 384\n",
      "\t MSE train loss: 0.18558058802136718\n",
      "Epoch: 385\n",
      "\t MSE train loss: 0.17320493630076014\n",
      "Epoch: 386\n",
      "\t MSE train loss: 0.17689703604738777\n",
      "Epoch: 387\n",
      "\t MSE train loss: 0.18034247557756625\n",
      "Epoch: 388\n",
      "\t MSE train loss: 0.2013241896528501\n",
      "Epoch: 389\n",
      "\t MSE train loss: 0.17423655590813628\n",
      "Epoch: 390\n",
      "\t MSE train loss: 0.18358180316050884\n",
      "Epoch: 391\n",
      "\t MSE train loss: 0.1889093634221928\n",
      "Epoch: 392\n",
      "\t MSE train loss: 0.17305176128146343\n",
      "Epoch: 393\n",
      "\t MSE train loss: 0.17604230764407672\n",
      "Epoch: 394\n",
      "\t MSE train loss: 0.17474952585106338\n",
      "Epoch: 395\n",
      "\t MSE train loss: 0.17383513967810135\n",
      "Epoch: 396\n",
      "\t MSE train loss: 0.18272638516806008\n",
      "Epoch: 397\n",
      "\t MSE train loss: 0.17089527536964105\n",
      "Epoch: 398\n",
      "\t MSE train loss: 0.17064740959795255\n",
      "Epoch: 399\n",
      "\t MSE train loss: 0.17010582374195657\n",
      "Epoch: 400\n",
      "\t MSE train loss: 0.17081657585315235\n",
      "Epoch: 401\n",
      "\t MSE train loss: 0.17017269929374612\n",
      "Epoch: 402\n",
      "\t MSE train loss: 0.1700429677764795\n",
      "Epoch: 403\n",
      "\t MSE train loss: 0.17211410254698137\n",
      "Epoch: 404\n",
      "\t MSE train loss: 0.17210844857568974\n",
      "Epoch: 405\n",
      "\t MSE train loss: 0.17096684118918223\n",
      "Epoch: 406\n",
      "\t MSE train loss: 0.1951377512416571\n",
      "Epoch: 407\n",
      "\t MSE train loss: 0.17993981523803554\n",
      "Epoch: 408\n",
      "\t MSE train loss: 0.16983256806605046\n",
      "Epoch: 409\n",
      "\t MSE train loss: 0.16877957190012366\n",
      "Epoch: 410\n",
      "\t MSE train loss: 0.1740604362037631\n",
      "Epoch: 411\n",
      "\t MSE train loss: 0.17474757619882653\n",
      "Epoch: 412\n",
      "\t MSE train loss: 0.16921548155000604\n",
      "Epoch: 413\n",
      "\t MSE train loss: 0.1791066803062452\n",
      "Epoch: 414\n",
      "\t MSE train loss: 0.17029907295577043\n",
      "Epoch: 415\n",
      "\t MSE train loss: 0.16790834790382145\n",
      "Epoch: 416\n",
      "\t MSE train loss: 0.16918791561735663\n",
      "Epoch: 417\n",
      "\t MSE train loss: 0.16637659551858167\n",
      "Epoch: 418\n",
      "\t MSE train loss: 0.16718770536182462\n",
      "Epoch: 419\n",
      "\t MSE train loss: 0.1687559704710014\n",
      "Epoch: 420\n",
      "\t MSE train loss: 0.1689799804351905\n",
      "Epoch: 421\n",
      "\t MSE train loss: 0.1714212047264161\n",
      "Epoch: 422\n",
      "\t MSE train loss: 0.1747810077265732\n",
      "Epoch: 423\n",
      "\t MSE train loss: 0.19212816185706352\n",
      "Epoch: 424\n",
      "\t MSE train loss: 0.18417127894878982\n",
      "Epoch: 425\n",
      "\t MSE train loss: 0.17242359561880755\n",
      "Epoch: 426\n",
      "\t MSE train loss: 0.17224947964718854\n",
      "Epoch: 427\n",
      "\t MSE train loss: 0.16566566702739924\n",
      "Epoch: 428\n",
      "\t MSE train loss: 0.1676242496527334\n",
      "Epoch: 429\n",
      "\t MSE train loss: 0.16523909672522444\n",
      "Epoch: 430\n",
      "\t MSE train loss: 0.17034909397534936\n",
      "Epoch: 431\n",
      "\t MSE train loss: 0.1990435452865027\n",
      "Epoch: 432\n",
      "\t MSE train loss: 0.19559685987366546\n",
      "Epoch: 433\n",
      "\t MSE train loss: 0.16888403748123185\n",
      "Epoch: 434\n",
      "\t MSE train loss: 0.16595268814154068\n",
      "Epoch: 435\n",
      "\t MSE train loss: 0.16413023941706256\n",
      "Epoch: 436\n",
      "\t MSE train loss: 0.1685385408294313\n",
      "Epoch: 437\n",
      "\t MSE train loss: 0.16404060671554213\n",
      "Epoch: 438\n",
      "\t MSE train loss: 0.1658708137934322\n",
      "Epoch: 439\n",
      "\t MSE train loss: 0.1643427138899949\n",
      "Epoch: 440\n",
      "\t MSE train loss: 0.16718961197939586\n",
      "Epoch: 441\n",
      "\t MSE train loss: 0.17139240061312855\n",
      "Epoch: 442\n",
      "\t MSE train loss: 0.17273573682433369\n",
      "Epoch: 443\n",
      "\t MSE train loss: 0.16323168278739772\n",
      "Epoch: 444\n",
      "\t MSE train loss: 0.1681856029163623\n",
      "Epoch: 445\n",
      "\t MSE train loss: 0.16278352816261574\n",
      "Epoch: 446\n",
      "\t MSE train loss: 0.17140058729661106\n",
      "Epoch: 447\n",
      "\t MSE train loss: 0.184042071266068\n",
      "Epoch: 448\n",
      "\t MSE train loss: 0.17727535675562142\n",
      "Epoch: 449\n",
      "\t MSE train loss: 0.16721012743713237\n",
      "Epoch: 450\n",
      "\t MSE train loss: 0.16694215749635052\n",
      "Epoch: 451\n",
      "\t MSE train loss: 0.16329043608162588\n",
      "Epoch: 452\n",
      "\t MSE train loss: 0.168161077276215\n",
      "Epoch: 453\n",
      "\t MSE train loss: 0.17596809647867354\n",
      "Epoch: 454\n",
      "\t MSE train loss: 0.1611271576503128\n",
      "Epoch: 455\n",
      "\t MSE train loss: 0.1629333370571743\n",
      "Epoch: 456\n",
      "\t MSE train loss: 0.16610985221166844\n",
      "Epoch: 457\n",
      "\t MSE train loss: 0.17173638544795203\n",
      "Epoch: 458\n",
      "\t MSE train loss: 0.17657790852008662\n",
      "Epoch: 459\n",
      "\t MSE train loss: 0.19515745750226007\n",
      "Epoch: 460\n",
      "\t MSE train loss: 0.18422035386312202\n",
      "Epoch: 461\n",
      "\t MSE train loss: 0.164545640895305\n",
      "Epoch: 462\n",
      "\t MSE train loss: 0.16626916713989437\n",
      "Epoch: 463\n",
      "\t MSE train loss: 0.15993977482402738\n",
      "Epoch: 464\n",
      "\t MSE train loss: 0.16147204675679025\n",
      "Epoch: 465\n",
      "\t MSE train loss: 0.16314937949980787\n",
      "Epoch: 466\n",
      "\t MSE train loss: 0.1639559263697243\n",
      "Epoch: 467\n",
      "\t MSE train loss: 0.16580776281188528\n",
      "Epoch: 468\n",
      "\t MSE train loss: 0.1608845374132825\n",
      "Epoch: 469\n",
      "\t MSE train loss: 0.16116967994246134\n",
      "Epoch: 470\n",
      "\t MSE train loss: 0.1660029172480231\n",
      "Epoch: 471\n",
      "\t MSE train loss: 0.168486566502445\n",
      "Epoch: 472\n",
      "\t MSE train loss: 0.16709965855564726\n",
      "Epoch: 473\n",
      "\t MSE train loss: 0.1637321982195508\n",
      "Epoch: 474\n",
      "\t MSE train loss: 0.18047475360497509\n",
      "Epoch: 475\n",
      "\t MSE train loss: 0.17680014573234495\n",
      "Epoch: 476\n",
      "\t MSE train loss: 0.16579695151261953\n",
      "Epoch: 477\n",
      "\t MSE train loss: 0.16143889598450797\n",
      "Epoch: 478\n",
      "\t MSE train loss: 0.17004102881861585\n",
      "Epoch: 479\n",
      "\t MSE train loss: 0.16412653033848565\n",
      "Epoch: 480\n",
      "\t MSE train loss: 0.1623299171614535\n",
      "Epoch: 481\n",
      "\t MSE train loss: 0.1576554499830772\n",
      "Epoch: 482\n",
      "\t MSE train loss: 0.15837785138749352\n",
      "Epoch: 483\n",
      "\t MSE train loss: 0.1594450853196183\n",
      "Epoch: 484\n",
      "\t MSE train loss: 0.1568659351720908\n",
      "Epoch: 485\n",
      "\t MSE train loss: 0.15918653468503505\n",
      "Epoch: 486\n",
      "\t MSE train loss: 0.1645355745488217\n",
      "Epoch: 487\n",
      "\t MSE train loss: 0.16300802965741445\n",
      "Epoch: 488\n",
      "\t MSE train loss: 0.15601588922335444\n",
      "Epoch: 489\n",
      "\t MSE train loss: 0.15646290728802575\n",
      "Epoch: 490\n",
      "\t MSE train loss: 0.15713412220728354\n",
      "Epoch: 491\n",
      "\t MSE train loss: 0.15851170447558383\n",
      "Epoch: 492\n",
      "\t MSE train loss: 0.15742890746302252\n",
      "Epoch: 493\n",
      "\t MSE train loss: 0.1564029391273281\n",
      "Epoch: 494\n",
      "\t MSE train loss: 0.15767446184404232\n",
      "Epoch: 495\n",
      "\t MSE train loss: 0.15854981267727372\n",
      "Epoch: 496\n",
      "\t MSE train loss: 0.16210378751152724\n",
      "Epoch: 497\n",
      "\t MSE train loss: 0.16186166713703742\n",
      "Epoch: 498\n",
      "\t MSE train loss: 0.16371992525517925\n",
      "Epoch: 499\n",
      "\t MSE train loss: 0.15966322072979452\n",
      "Epoch: 500\n",
      "\t MSE train loss: 0.1769477047826351\n"
     ]
    }
   ],
   "source": [
    "weights, bias, epoch_losses = fit(\n",
    "    X, y, \n",
    "    batch_size=64, \n",
    "    hidden_neurons=20, \n",
    "    epochs=500, \n",
    "    alpha=.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3c3cc3c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_losses, label='Train loss')\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a575dfd4",
   "metadata": {},
   "source": [
    "Finally, we can make predictions simply by passing data to the `forward()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "abbd9a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.4438196 ],\n",
       "       [3.39941493],\n",
       "       [3.39051256],\n",
       "       [3.13830421],\n",
       "       [3.30190806],\n",
       "       [3.35099988],\n",
       "       [3.19593143],\n",
       "       [3.25598872],\n",
       "       [2.95642252],\n",
       "       [2.95735068],\n",
       "       [3.12238783],\n",
       "       [3.05592309],\n",
       "       [3.21350747],\n",
       "       [3.13888044],\n",
       "       [3.15067387],\n",
       "       [2.99077946],\n",
       "       [2.97243141],\n",
       "       [2.99663331],\n",
       "       [3.04074303],\n",
       "       [2.62318184],\n",
       "       [2.84362717],\n",
       "       [2.977201  ],\n",
       "       [2.84407245],\n",
       "       [2.90969515],\n",
       "       [2.82407862],\n",
       "       [2.88258666],\n",
       "       [2.94816922],\n",
       "       [2.78794219],\n",
       "       [2.74519444],\n",
       "       [2.64792441],\n",
       "       [2.26018301],\n",
       "       [2.6860188 ],\n",
       "       [2.71918225],\n",
       "       [2.58096881],\n",
       "       [2.51640161],\n",
       "       [2.69444582],\n",
       "       [2.64324097],\n",
       "       [2.59001536],\n",
       "       [2.64628725],\n",
       "       [2.6278341 ],\n",
       "       [2.48382782],\n",
       "       [2.24820533],\n",
       "       [2.59104389],\n",
       "       [2.56344294],\n",
       "       [2.04013298],\n",
       "       [2.55708037],\n",
       "       [2.50625531],\n",
       "       [2.55614979],\n",
       "       [2.54964892],\n",
       "       [2.40410065],\n",
       "       [2.54098838],\n",
       "       [2.60254749],\n",
       "       [2.65435859],\n",
       "       [2.68854206],\n",
       "       [2.73357234],\n",
       "       [2.60622178],\n",
       "       [2.73013603],\n",
       "       [2.82316923],\n",
       "       [2.79699958],\n",
       "       [2.86185652],\n",
       "       [3.19825291],\n",
       "       [3.10270353],\n",
       "       [3.21079399],\n",
       "       [3.24864265],\n",
       "       [3.29536584],\n",
       "       [3.33921097],\n",
       "       [3.42203964],\n",
       "       [3.54976168],\n",
       "       [3.85650443],\n",
       "       [4.22061266],\n",
       "       [4.5694384 ],\n",
       "       [4.53347568],\n",
       "       [4.65951056],\n",
       "       [4.86547933],\n",
       "       [4.89060761],\n",
       "       [5.03737987],\n",
       "       [5.56050471],\n",
       "       [5.64377072],\n",
       "       [5.57537314],\n",
       "       [5.76506733],\n",
       "       [5.70688021],\n",
       "       [6.06971832],\n",
       "       [6.34404287],\n",
       "       [6.52022149],\n",
       "       [6.64455257],\n",
       "       [6.94149368],\n",
       "       [6.86608159],\n",
       "       [7.50198683],\n",
       "       [7.66670173],\n",
       "       [8.02504932],\n",
       "       [8.07972526],\n",
       "       [8.37602105],\n",
       "       [8.58924549],\n",
       "       [8.78019042],\n",
       "       [8.89603563],\n",
       "       [8.99526542],\n",
       "       [9.12806923],\n",
       "       [9.17844134],\n",
       "       [9.19392533],\n",
       "       [9.33057151]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat, _, _ = forward(X, weights=weights, bias=bias)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "56c646c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb7dc6a3760>]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y, '.')\n",
    "plt.plot(y_hat, '.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
