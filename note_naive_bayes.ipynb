{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "ljZZdX21X8LE"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH3kUs4eX8LG"
   },
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    " \\newcommand{\\wv}{\\mathbf{w}}\n",
    " \\newcommand{\\yv}{\\mathbf{y}}\n",
    " \\newcommand{\\zv}{\\mathbf{z}}\n",
    " \\newcommand{\\uv}{\\mathbf{u}}\n",
    " \\newcommand{\\vv}{\\mathbf{v}}\n",
    " \\newcommand{\\tv}{\\mathbf{t}}\n",
    "  \\newcommand{\\bv}{\\mathbf{b}}\n",
    " \\newcommand{\\Chi}{\\mathcal{X}}\n",
    " \\newcommand{\\R}{\\rm I\\!R}\n",
    " \\newcommand{\\sign}{\\text{sign}}\n",
    " \\newcommand{\\Tm}{\\mathbf{T}}\n",
    " \\newcommand{\\Xm}{\\mathbf{X}}\n",
    " \\newcommand{\\Zm}{\\mathbf{Z}}\n",
    " \\newcommand{\\Im}{\\mathbf{I}}\n",
    " \\newcommand{\\Um}{\\mathbf{U}}\n",
    " \\newcommand{\\Vm}{\\mathbf{V}} \n",
    " \\newcommand{\\muv}{\\boldsymbol\\mu}\n",
    " \\newcommand{\\Sigmav}{\\boldsymbol\\Sigma}\n",
    " \\newcommand{\\Lambdav}{\\boldsymbol\\Lambda}\n",
    "$\n",
    "\n",
    "# Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAAvWkfcX8LH"
   },
   "source": [
    "## Table of notation\n",
    "\n",
    "| Symbol                     | Meaning                     | Symbol    | Meaning                                                          |\n",
    "|----------------------------|-----------------------------|-----------|------------------------------------------------------------------|\n",
    "| $\\xv$ or $\\vec{x}$         | feature/input vector        | $x_i$     | $i$th element of $\\xv$                                           |\n",
    "| $\\Xm$                      | input matrix                | $x_{i,j}$ | $i$th row and $j$th column of $\\Xm$                              |\n",
    "| $\\yv$ or $\\tv$             | labels/targets              | $n$       | number of features or columns \n",
    "| $\\wv$ or $\\mathbf{\\theta}$ | weight/parameter vector     | $m$       | number of data samples <br>(also used to refer to the slope) |samples or rows                                   |\n",
    "| $f$ or $h$                 | hypothesis function <br> (i.e., a model)        | $\\hat{\\yv}$ <br> $f(\\xv {;} \\wv)$<br>$h(\\xv {;} \\wv)$ | predictions <br> y-hat |\n",
    "| $E$              | error or sum of error (loss)  | $SSE$      | sum of squared error function                                            |\n",
    "| $MSE$                      | mean squared error| $\\nabla$  | gradient (nabla)                                       |\n",
    "| $\\partial$                 | partial derivative          | $\\alpha$  | learning rate (alpha)                                  |       \n",
    "| $J$ | general placeholder for <br>the objective function | $x^T$| transpose of a vector or matrix |\n",
    "$b$ | bias or y-intercept term | $T$ | Threshold |\n",
    "$*$| element-wise<br> multiplication | $\\cdot$ | dot product|\n",
    "| $z$<br>$\\zv$| value before applying activation function |  $X, Y$ | Random variables |\n",
    "| $K$| number/set of classes | $k$ | current class|\n",
    "| $MAP$|  maximum a posteriori | $ML$ |  maximum likelihood|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY2vJQljX8LI"
   },
   "source": [
    "## Review\n",
    "Before we dive into Naive Bayes we first need to review the fundamentals of probability theory. As we'll see shortly, Naive Bayes takes these fundamentals and creates a simple and elegant algorithm.\n",
    "\n",
    "### Probability theory\n",
    "First, recall the following foundational terms.\n",
    "\n",
    "#### **Random variables**: X and Y\n",
    "Here $X$ and $Y$ will be <u>two different [random variables](https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-discrete/v/random-variables?modal=1)</u>. In classical probability theory. Further, depending on the domain/field, the following could be other ways to write the random variables $X$ and $Y$: \n",
    "\n",
    "$$\n",
    "X = B = E \\\\\n",
    "Y = A = H\n",
    "$$\n",
    "\n",
    "*Note, $X$ and $Y$ here DO NOT refer to matrices!*\n",
    "\n",
    "#### **Marginal probability**: P(X) or P(Y)\n",
    "$P(X)$ is read as the *probability of $X$ occurring* while $P(Y)$ is read as the probability of $Y$ occurring. When we are interested in the probability of a <u>single random event occurring irrespective of the outcome of another variable</u> we refer to this as the marginal probability. Thus, we want just the probability of $X$ or just the probability of $Y$ occurring.\n",
    "\n",
    "#### **Joint probability**: P(X, Y)\n",
    "$P(X, Y)$ is read as *the probability of $X$ and $Y$*. $P(X, Y)$ can also be written $P(X \\cap Y)$ or $P(X \\land Y)$. Joint probability simply refers to when we are interested in probability of <u>two or more events occurring simultaneously</u>. Thus, we want the probability $X$ and $Y$ occur at the same time.\n",
    "\n",
    "#### **Conditional probability**: P(Y | X)\n",
    "$P(Y \\mid X)$ is read as the *probability of $Y$ given $X$*. When we are interested in the <u>probability of $Y$ occurring **given** $X$ has already occurred</u>, we refer to this as the conditional probability. Meaning, we are interested in the probability of $Y$ conditioned on $X$.\n",
    "\n",
    "#### **Product rule** or chain rule \n",
    "The product rules is defined as follows:\n",
    "$$\n",
    "P(X, Y) = P(Y \\mid X) P (X) = P(X | Y) P (Y)\n",
    "$$\n",
    "\n",
    "The product rule relates the joint and conditional probabilities and provides us with a way of computing the joint or conditional probability if one is not given. Notice that the product rule is symmetrical, meaning we can swap the places $X$ and $Y$ and achieve the same answer $P(Y \\mid X) P (X) = P(X | Y) P (Y)$.\n",
    "\n",
    "#### **Independence rule**\n",
    "When we assume our random variables are independent from one another, meaning $X$ does not influence the outcome of $Y$ and vice-versa, the following can be said about the our conditional and joint probabilities:\n",
    "\n",
    "- **Conditional**: $P(Y \\mid X) = P(Y)$\n",
    "    - If $Y$ is independent of $X$ then when given $X$, the probability of $Y$ occurring is unchanged or simply just the marginal of $Y$. The same idea is applied to the reverse $P(X \\mid Y) = P(X)$.\n",
    "    \n",
    "- **Joint**: $P(X, Y) = P(X) P(Y)$\n",
    "    - When $X$ and $Y$ are independent the joint probability is simply the two probabilities multiplied. Notice, assuming independence means we simplify the product rule simply to $P(X)P(Y)$.\n",
    "    \n",
    "<!-- *Note, it is assumed we multiply the probabilities when no operator is given.* -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYfTDelHX8LI"
   },
   "source": [
    "## Probabilistic classification\n",
    "\n",
    "With these fundamental probability definitions in mind, let's reformulate our classification problem using conditional probability $P(Y \\mid X)$.\n",
    "\n",
    "Recall $\\Xm$ contains our input features with $m$ data samples and $n$ features. Further, for each data sample $\\xv$, there is a corresponding label $y$. Thus, we'll say $X = \\xv$ and $Y = y$.\n",
    "\n",
    "We can formulate **our classification problem** as the probability of a label $y$ given a single data sample $\\xv$. Recall, this **is just a conditional probability** that can be written as follows:\n",
    "$$\n",
    "P(y \\mid \\xv).\n",
    "$$\n",
    "\n",
    "Now, recall we typically have multiple classes and therefore multiple labels such that we have $K$ classes $\\{1, 2, ..., k\\}$. This means we want to predict/compute the probability for EACH class. Thus, we compute the probability that a data sample $\\xv$ belongs to a given class $y_k$ where $k$ is simply a placeholder for the current class. We can write the following instead:\n",
    "\n",
    "$$\n",
    "P(y_k \\mid \\xv), \\textrm{ or}\\quad P(y = k\\mid \\xv)\n",
    "$$\n",
    "where in this equation\n",
    "- $P(y_k \\mid \\xv)$ is the probability of class $y_k$ given the current data sample $\\xv$. If we had 3 classes $K=3$ we would need to compute $P(y_k \\mid \\xv)$ for EACH class! See the below example.\n",
    "    - Probability of class 1:  $P(y_1 \\mid \\xv)$ \n",
    "    - Probability of class 2:  $P(y_2 \\mid \\xv)$ \n",
    "    - Probability of class 3:  $P(y_3 \\mid \\xv)$ \n",
    "\n",
    "To select the predicted label for each data sample, we simply select the class label with **largest probability**. This can be written mathematically as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg \\max_{k \\in K} P(Y=y_k \\mid X=\\xv)\n",
    "$$\n",
    "\n",
    "\n",
    "where in this equation\n",
    "- $K$ is the set of classes or number of class. For instance, if we had 3 classes then $K$ would contain the set $K \\in \\{1,2,3\\}$\n",
    "- $\\arg \\max$, in general terms, refers to the point that maximizes a given function. In relation to our problem, $\\arg \\max$ refers to selecting the **label** for the class that has the highest probability.\n",
    "    - $\\arg \\max_{k \\in K}$ indicates we want to compute the $\\arg \\max$ over all the possible classes. <u>This means $\\hat{y} $ would be the label corresponding to the class with the largest probability</u>. For instance, if our labels were {0,1,2\\}, $\\hat{y}$  could only take on ONE of the labels from the set such that $\\hat{y} \\in$  \\{0,1,2\\}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCdVGrK3X8LJ"
   },
   "source": [
    "# Naive Bayes classifier\n",
    "\n",
    "Given these probability fundamentals and the formulation of probabilistic classification we are ready to tackle Naive Bayes. Our goal will be to find what we call the posterior probability $P(Y = y_k \\mid X = \\xv)$ for each of the $K$ classes. However, directly computing the posterior probability from the training data isn't possible. All hope isn't lost, we can actually approximate the posterior probability using Bayes rule! Let's take a look at how to derive Bayes rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2edC07KuX8LJ"
   },
   "source": [
    "## Bayes rule\n",
    "\n",
    "Recall that Bayes rules is given as follows:\n",
    "$$\n",
    "P(Y \\mid X)  = \\frac{P(X \\mid Y) P(Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "How did we get here though? We'll, we can simply use the definitions of joint and conditional probability to derive Bayes rule.\n",
    "\n",
    "First, recall that the joint probability definition is symmetrical such that the following is true:\n",
    "$$\n",
    "P(Y, X) = P(X, Y). \n",
    "$$\n",
    "\n",
    "Additionally, we can write the joint probability using the product rule, which is also symmetrical, as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "P(Y, X) = P(Y \\mid X) P(X), \\\\\n",
    "P(X, Y) = P(X \\mid Y) P(Y). \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, if we substitute the product rule into the symmetrical joint distribution definition given above we get the following:\n",
    "\n",
    "$$\n",
    "P(Y \\mid X) P(X) = P(X \\mid Y) P(Y)\n",
    "$$\n",
    "\n",
    "Thus, if we move $P(X)$ to the right hand side of the equation (by dividing) we get Bayes rule as follows:\n",
    "\n",
    "$$\n",
    "P(Y \\mid X)  = \\frac{P(X \\mid Y) P(Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "where the following terms are used.\n",
    "- $P(Y \\mid X)$ is referred to as the *posterior*\n",
    "    - Recall, in practice, the posterior is the probability of a label given our data. This is what we want to find!\n",
    "- $P(X \\mid Y)$ is referred to as the *likelihood*\n",
    "    - Likelihood is slightly less intuitive. It refers to the *likelihood* a data sample belongs to a label. It is common practice to refer to the likelihood as the probability although this is technically incorrect as the likelihood is not a probability as its value is not constrained between 0 and 1. More on this soon.\n",
    "- $P(Y)$ is referred to as the *prior*\n",
    "    - The prior simply refers to the distribution of our labels or what is the probability of each label occurring. This is easily computed from the training data.\n",
    "- $P(X \\mid Y) P (Y)$ is often referred to as the *joint likelihood*\n",
    "- $P(X)$ is referred to as the *evidence*\n",
    "    - Evidence refers to the distribution over our data or the probability of data occurring. As we'll see shortly, the evidence acts as a normalization term and turns the joint likelihood into an actual probability. More on this term soon!\n",
    "\n",
    "The below picture depicts these same terms. Here $A=Y$ and $X=B$.\n",
    "<img src=\"https://miro.medium.com/max/1400/1*CnoTGGO7XeUpUMeXDrIfvA.png\" width=300 height=300>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vCGLeayX8LJ"
   },
   "source": [
    "## Naive Bayes assumption\n",
    "The big assumption that Naive Bayes makes is called *conditional independence*. Before explaining what this term means, let's see how we would compute $P(\\xv | y_k)$ first. \n",
    "\n",
    "Recall that $\\xv$ contains $n$ features meaning $P(\\xv | y_k) = P(x_1, x_2, ..., x_n | y_k)$. Thus, we would have to compute the probability of a feature $x_i$ given a particular label $y_k$ and combinations of the other features. For instance, imagine we have 3 features, to compute  $P(\\xv | y_k)$ we would have to do the following:\n",
    "$$\n",
    "\\begin{align}\n",
    "P(x_1, x_2, x_3 | y_k) = & P(x_1|x_2,x_3,y_k) P(x_1| x_2, y_k) P(x_1| x_3, y_k) \\\\ \n",
    "& P(x_2 | x_1, x_3, y_k) P(x_2|x_1,y_k) P(x_2|x_3,y_k) \\\\\n",
    "& P(x_3|x_1,x_2,y_k) P(x_3 | x_1, y_k) P(x_3 | x_1, y_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "However, if we assume our data is conditional independent we only have to compute the following:\n",
    "\n",
    "$$\n",
    "P(x_1, x_2, x_3 | y_k) = P(x_1|y_k) P(x_2|y_k) P(x_3 | y_k)\n",
    "$$\n",
    "\n",
    "Recall that **conditional independence** means that our data features are independent of each other only when conditioned on $y$. Now **this assumption is not usually actually true, however it greatly simplifies the math**. While this assumption isn't always true and often limits Naive Bayes, results produced by Naive Bayes can still be surprisingly good.\n",
    "\n",
    "**Naive Bayes is referred to as *naive* due to this conditional independence assumption**. In more general terms, for data with $n$ features we can compute $P(\\xv | y_k)$ as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\xv | y_k) &= P(x_1|y_k) P(x_2|y_k) ... P(x_n|y_k) \\\\\n",
    "&= \\prod_{i=0}^n P(x_i|y_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where is this equation\n",
    "- $\\prod$ (referred to as the upper case Greek symbol Pi)is the [product symbol](https://mathmaine.com/2018/03/04/pi-notation/) taken over all $n$ features in our data.\n",
    "- $i$ is used to index each of the $n$ features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teBlLwgzX8LK"
   },
   "source": [
    "##  Probabilistic Classification with MAP and ML\n",
    "Recall, our goal is to maximize the following equation where $\\hat{y}$ contains the label of the class with the highest probability:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg \\max_{k \\in K} P(Y=y_k \\mid X=\\xv).\n",
    "$$\n",
    "\n",
    "Further, recall that Bayes rule defines how we can compute $P(Y=y_k \\mid X=\\xv)$ such that we can substitute Bayes rule into our maximization equation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg \\max_{k \\in K} \\frac{P(\\xv \\mid y_k) P(y_k)}{P(\\xv)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ympQJG1gX8LK"
   },
   "source": [
    "### Temporally dropping the evidence P(X)\n",
    "\n",
    "In practice, we typically drop the denominator $P(X)$ when computing the predictions. In turn, this leaves us with the following equation:\n",
    "\n",
    "$$\n",
    "P(Y \\mid X)  = P(X \\mid Y) P(Y)\n",
    "$$\n",
    "\n",
    "In other words, we have: \n",
    "$$\n",
    "\\hat{y} = \\arg \\max_{k \\in K} P(\\xv \\mid y_k) P(y_k)\n",
    "$$\n",
    "\n",
    "==== Following are explanations, you may skip if not interested in. =======\n",
    "\n",
    "We can drop the denominator $P(X)$ because it acts as a constant. This is because it does not depend on a given class, rather $P(X)$ is based on the sum of ALL classes which means the term never changes (i.e., a constant). This term is also computationally expensive to compute so excluding it can lead to major performance increases as well.\n",
    "\n",
    "If we wanted to compute $P(X)$ we would use the following equation:\n",
    "$$\n",
    "P(X) = \\sum_{k=1}^K P(X | Y= y_k)P(Y = y_k).\n",
    "$$\n",
    "\n",
    "Notice how this equation is the same no matter which class we use since the sum is always over ALL classes!\n",
    "\n",
    "Finally, for some more intuition for $P(X)$, below is the Bayes rule with the expanded equation of $P(X)$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(Y \\mid X) &= \\frac{P(X | Y)P(Y)}{P(X)} \\\\\n",
    "&= \\frac{P(X | Y = y_k)P(Y = y_k)}{\\sum_{k=1}^K P(X | Y= y_k)P(Y = y_k)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice, how the numerator and denominator are almost the same, except the denominator has a summation over ALL classes! This is because $P(X)$ acts as a normalizing term. Recall the numerator is for a specific class $k$ and the denominator is for all classes! \n",
    "\n",
    "Remember how the denominator $P(X)$ computes the distribution over our data? This means our numerator computes the distribution over our data for a given class and the denominator computes the distribution over our data for ALL classes! If we left $P(X)$ in Bayes rule, it would convert the product in the numerator into an actual probability (more on this soon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaMMA2BLX8LK"
   },
   "source": [
    "\n",
    "### Maximum a posteriori (MAP)\n",
    "\n",
    "After dropping $P(\\xv)$, as this term is a constant, this leaves us with an equation we refer to as the *maximum a posteriori* (MAP). MAP is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}_{MAP} &=  \\arg \\max_{k \\in K} P(y_k) \\prod_{i=0}^n P(x_i|y_k) \\\\\n",
    "&= \\arg \\max_{k \\in K} P(\\xv \\mid y_k) P(y_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice, we are making a prediction based on the largest joint likelihood now!\n",
    "\n",
    "### Maximum likelihood (ML)\n",
    "\n",
    "Lastly, we can drop $P(y_k)$ if we assume all the classes are equally likely to be picked. In other words, we assume there are equal number of data samples for each class such that $P(y)$ for all classes is a uniform distribution. This means, all class priors are the same! In turn, $P(y_k)$ then becomes constant and we can drop it from the equation! \n",
    "\n",
    "Finally, after dropping both $P(\\xv)$ and $P(y_k)$, this leaves us with an equation we refer to as the *maximum likelihood* (ML). ML is given as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}_{ML} &= \\arg \\max_{k \\in K} \\prod_{i=0}^n P(x_i|y_k) \\\\\n",
    "&= \\arg \\max_{k \\in K} P(\\xv \\mid y_k)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice, we are making a prediction based on the largest likelihood now!\n",
    "\n",
    "### Differences between MAP and ML\n",
    "\n",
    "The main differences between MAP and ML is that *MAP assumes some prior evidence while ML does not*. Which is better? Well, it is hard to say which is truly better. However, we can say that when given enough data MAP and ML tend to perform the same. Thus, it can be more efficient to use ML. Further, *if we have very little data, MAP can perform better as it allows us to inject our prior knowledge or beliefs into the equation*. That being said, if our prior beliefs are wrong, MAP can perform worse than ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a69kTbCcX8LK"
   },
   "source": [
    "### More efficient MAP and ML equations\n",
    "You might be wondering at this point. Hey, there is a product term $\\prod$ in both the MAP and ML equations. Isn't this an issue as computing the product over small numbers can lead to numerical instability such as [underflow](https://www.computerhope.com/jargon/u/underflo.htm)?\n",
    "\n",
    "The answer is yes, this is an issue. The solution to this is actually extremely simple and it requires using our good friend the $\\log$.\n",
    "\n",
    "Recall that taking the log of a product converts said product to a sum. For instance, if we have $f = a * b$ and we take the $\\log$ of $f$ we get the following: \n",
    "\n",
    "$$\n",
    "\\log(f) = \\log(a*b) = \\log(a) + \\log(b).\n",
    "$$ \n",
    "\n",
    "Further, if we take the $\\log$ of a product $\\prod$ we get a sum $\\sum$.\n",
    "\n",
    "#### Log maximum a posteriori (MAP)\n",
    "Applying the log to MAP gives us the following equation:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}_{MAP} &=  \\arg \\max_{k \\in K} \\log P(y_k) + \\sum_{i=0}^n \\log P(x_i|y_k) \\\\\n",
    "&=  \\arg \\max_{k \\in K} \\log P(\\xv \\mid y_k) + \\log P(y_k).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Log maximum likelihood  (ML)\n",
    "Applying the log to ML gives us the following equation:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}_{ML} &= \\arg \\max_{k \\in K} \\sum_{i=0}^n \\log P(x_i|y_k) \\\\\n",
    "&= \\arg \\max_{k \\in K} \\log P(\\xv \\mid y_k).\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ou7jUx1X8LL"
   },
   "source": [
    "### Goal\n",
    "**Our goal for Naive Bayes is to make predictions using either the $\\log$ of the MAP or ML equations, <u>however we'll focus on MAP.</u>** For Naive Bayes this means we need to simply compute the $P(\\xv \\mid y_k)$ and $P(y_k)$ from our training data and then apply the $\\log$. Now, to compute these terms there are multiple ways for doing. For this module, we are going to look at how to do so using Gaussian Naive Bayes. \n",
    "\n",
    "*Note, in the future we'll see algorithms that use a variation of ML as a cost function (for those interested I'm referring to cross-entropy and the negative log likelihood)!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFSlSqmTX8LL"
   },
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "Gaussian Naive Bayes aims to expand Naive Bayes to work with continuous features. Typically, [classical Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#categorical-naive-bayes) (also called Categorical Naive Bayes) computes the likelihood $P(\\xv \\mid y_k)$ for each feature in a class by counting the frequency of unique values (i.e., categories) and dividing by the total number of values seen for said feature. Notice, that is impossible for continuous features since almost all the values for the continuous feature will be unique. Therefore, classical Naive Bayes can only be used on discrete/categorical features. Thus, we need a different approach that works for continuous features.\n",
    "\n",
    "Gaussian Naive Bayes computes the likelihood $P(\\xv \\mid y_k)$ by modeling each feature as a Gaussian distribution (i.e., Gaussian Naive Bayes assumes our data follows a Gaussian distribution which may not be true). By doing so, Gaussian Naive Bayes **ONLY** works with continuous features. \n",
    "\n",
    "All we need to do to compute the likelihoods $P(\\xv \\mid y_k)$ is that: for **every class** is we need to **compute the mean and standard deviation** for **every continuous feature**. We do so using the training data only. Once this is done, we can simply pass any data along with the means and standard deviations to the Gaussian equation to compute the likelihoods for all data samples and classes!\n",
    "\n",
    "Recall, to make a prediction we will use the $\\log$ of the MAP equation given previously! That being said, keep in mind that when the priors $P(y_k)$ are equal for all classes, MAP essentially reduces to a scaled version ML. We can compute the priors $P(y_k)$ for each class by computing the ratio of data samples per class divided by the total number of data samples. \n",
    "\n",
    "Let's take a deeper look at how to compute the $\\log$ of $P(\\xv \\mid y_k)$ and $P(y_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgxsXJgVX8LL"
   },
   "source": [
    "### Toy Data\n",
    "\n",
    "Before we get started we need to define some toy data. We do so using Sklearn's `make_circles()` function which makes non-linear data that consists of 2 class, an outer circle class (class 0, label 0) and inner circle class (class 1, label 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "rmwIEi71X8LL",
    "outputId": "48f9684f-aae6-4723-e6e1-42e05bb8fda5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (100, 2)\n",
      "y shape: (100,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "def nonlinear_data(\n",
    "    n_samples: int = 100, \n",
    "    balance: List = None,\n",
    "    seed: int = 42\n",
    ") -> List[np.ndarray]:\n",
    "    X, y = make_circles(random_state=seed, factor=.5, noise=.05)\n",
    "    \n",
    "    classes = np.unique(y)\n",
    "    \n",
    "    # Create imbalanced data by selecting only a\n",
    "    # subset of the actual data samples for each class\n",
    "    if balance is not None:\n",
    "        rng = np.random.RandomState(seed)\n",
    "        new_X = []\n",
    "        new_y = []\n",
    "        for idx, c in enumerate(classes):\n",
    "            class_locations = np.where(y == c)[0]\n",
    "            selected_samples = int(len(X)*balance[idx])\n",
    "            subsample_locs = rng.choice(class_locations, selected_samples)\n",
    "            new_X.append(X[subsample_locs])\n",
    "            new_y.append(y[subsample_locs])\n",
    "        X = np.vstack(new_X)\n",
    "        y = np.hstack(new_y)\n",
    "    return X, y\n",
    "    \n",
    "X, y = nonlinear_data()\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Find data samples for each class\n",
    "class_0 = np.where(y == 0)[0]\n",
    "class_1 = np.where(y == 1)[0]\n",
    "\n",
    "# Plot data\n",
    "plt.plot(X[class_0, 0], X[class_0, 1], 'or', label='class 0')\n",
    "plt.plot(X[class_1, 0], X[class_1, 1], 'xb', label='class 1')\n",
    "plt.xlim([np.min(X[:, 0])-.5, np.max(X[:, 0])+.5])\n",
    "plt.ylim([np.min(X[:, 1])-.5, np.max(X[:, 1])+.5])\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2fm4yW43X8LM"
   },
   "source": [
    "Below we can see our data is a (100, 2) meaning we have 100 data samples and 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-deVTzeX8LM",
    "outputId": "ea7be6b3-3006-49d5-81ea-00b454cc40ff",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23609814, -0.4675    ],\n",
       "       [ 0.48869037,  0.24924534],\n",
       "       [-0.39392915,  0.32374486],\n",
       "       [ 0.76420024, -0.59338464],\n",
       "       [ 0.80241569, -0.74074202],\n",
       "       [ 0.23488158, -0.89596143],\n",
       "       [-0.90688223,  0.28391018],\n",
       "       [-0.38816624, -0.29794857],\n",
       "       [ 0.33240673,  0.98786263],\n",
       "       [ 0.96101491, -0.04219482],\n",
       "       [-0.64495068,  0.72223536],\n",
       "       [-0.80149254, -0.59345631],\n",
       "       [-0.35261517,  0.0730904 ],\n",
       "       [-0.57492919, -0.82313077],\n",
       "       [ 0.19814375, -0.36116696],\n",
       "       [ 0.95745271,  0.52294033],\n",
       "       [-0.48140439, -0.0178351 ],\n",
       "       [-0.51480768, -0.14015324],\n",
       "       [-0.01133069,  0.98658435],\n",
       "       [-0.68083656, -0.69503173],\n",
       "       [ 0.36580635,  0.27590372],\n",
       "       [ 0.06759942, -0.5118016 ],\n",
       "       [-0.94961864, -0.19089944],\n",
       "       [ 0.49231155, -0.86966009],\n",
       "       [-0.42998185,  0.48945672],\n",
       "       [-0.36383028,  0.99680073],\n",
       "       [ 0.27571396, -0.97674589],\n",
       "       [ 0.45321948, -0.31380268],\n",
       "       [ 0.39264238,  0.89750341],\n",
       "       [-0.50718909,  0.14293781],\n",
       "       [ 0.24171767,  1.03253659],\n",
       "       [ 0.90768818, -0.37233281],\n",
       "       [-0.09265769, -0.5468522 ],\n",
       "       [-0.91287535, -0.26939599],\n",
       "       [ 0.41696373, -0.48225218],\n",
       "       [ 0.77088565,  0.59621822],\n",
       "       [-0.27898349,  0.41900119],\n",
       "       [-0.16137305,  0.54559434],\n",
       "       [-0.31128713, -0.94020968],\n",
       "       [-0.4001581 ,  0.93200149],\n",
       "       [ 0.99351967, -0.18518759],\n",
       "       [-0.38629565, -0.88325258],\n",
       "       [ 0.58258637,  0.73639816],\n",
       "       [ 0.44888408, -0.26361847],\n",
       "       [-1.00459806, -0.26218658],\n",
       "       [-0.73614174,  0.71539729],\n",
       "       [-0.30203728, -0.3389911 ],\n",
       "       [-0.94153962,  0.04366585],\n",
       "       [ 0.04167777,  0.4268392 ],\n",
       "       [-0.11417606,  1.03508876],\n",
       "       [-1.07764118,  0.13128216],\n",
       "       [ 0.97980077,  0.3757456 ],\n",
       "       [-0.56589666,  0.80199721],\n",
       "       [ 0.10359816, -0.97444136],\n",
       "       [ 0.52155233,  0.89759255],\n",
       "       [-0.42218835, -0.09463258],\n",
       "       [ 0.75731333,  0.67632208],\n",
       "       [-0.1008402 ,  0.39441512],\n",
       "       [-0.17984836, -0.983073  ],\n",
       "       [ 0.02718695, -0.49240801],\n",
       "       [ 0.43164272,  0.30554617],\n",
       "       [ 0.44806262, -0.00742021],\n",
       "       [ 0.46986535,  0.3939316 ],\n",
       "       [ 0.57149548, -0.73449388],\n",
       "       [ 0.51579455,  0.00296406],\n",
       "       [-0.22238779,  0.42067769],\n",
       "       [ 0.77948787, -0.52412155],\n",
       "       [-0.25049032,  0.44385807],\n",
       "       [ 0.0750347 ,  0.51632348],\n",
       "       [ 0.60408515, -0.18562162],\n",
       "       [-0.43873251, -0.2313469 ],\n",
       "       [ 0.45612808, -0.92227211],\n",
       "       [ 0.29821987,  0.32495248],\n",
       "       [ 0.9854031 , -0.2461584 ],\n",
       "       [ 0.47341873, -0.14810781],\n",
       "       [ 0.28824333,  0.34458057],\n",
       "       [-0.61590341, -0.01340092],\n",
       "       [-0.5826075 , -0.78728379],\n",
       "       [ 0.39623994, -0.3962836 ],\n",
       "       [ 0.14760477,  0.41487543],\n",
       "       [-0.08267025,  0.43986138],\n",
       "       [-0.19040094, -0.36346731],\n",
       "       [-0.09613769, -1.08261764],\n",
       "       [-0.84346681, -0.55354872],\n",
       "       [ 0.93295142,  0.15008302],\n",
       "       [ 0.4760261 ,  0.08998618],\n",
       "       [-0.87288912,  0.46128323],\n",
       "       [ 1.06306054,  0.21489965],\n",
       "       [-1.01425379,  0.22593823],\n",
       "       [-0.05203334, -0.46923118],\n",
       "       [ 0.11331276, -0.43111223],\n",
       "       [-0.52312162,  0.10150777],\n",
       "       [-0.09607284, -0.4652089 ],\n",
       "       [-0.33828209, -0.36335549],\n",
       "       [-0.7258285 ,  0.55082343],\n",
       "       [ 0.14914877,  0.45121739],\n",
       "       [-0.35850425,  0.21917976],\n",
       "       [-0.22401592,  0.95337341],\n",
       "       [ 0.26112753, -0.46183921],\n",
       "       [ 0.47763485,  0.07052671]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0eEa3ovX8LM",
    "outputId": "0b5633c3-f460-4ebb-effb-e1f065eac9bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-c3u69pX8LM"
   },
   "source": [
    "Next, we can see our labels `y` consist of either the label 0 for class 0 or 1 for class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5AmRSgvxX8LM",
    "outputId": "f7564362-7fda-4a15-ea4a-f3081ca044f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ymiRpu8X8LM"
   },
   "source": [
    "## Computing priors P(Y)\n",
    "\n",
    "The first and easiest thing we need to do when computing MAP is to compute the priors $P(y_k)$. To do so, we simply count the number of data samples for each class and divide by the total number of data samples.\n",
    "\n",
    "We can easily get the unique labels from `y` and the number of data samples for each label (i.e., class) by using NumPy's `np.unique()` function and passing the keyword argument `return_counts=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "s9_bsCjTX8LN"
   },
   "outputs": [],
   "source": [
    "class_labels, class_count = np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFKp3tDIX8LN",
    "outputId": "13ea6626-ff94-455e-e864-3021d7832635"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oQMMA2UX8LN",
    "outputId": "f318c6c6-75ba-4155-974b-356d245b11ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50, 50])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GuZeCb_X8LN"
   },
   "source": [
    "Turning this idea into a function can be see below in the `compute_priors()` function. Notice that all we do to compute the priors is divide the `class_count` array by the total number of data samples `total_data_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "zmxiSC-eX8LN"
   },
   "outputs": [],
   "source": [
    "def compute_priors(y, verbose=True):\n",
    "    class_labels, class_count = np.unique(y, return_counts=True)\n",
    "    total_data_samples = len(y)\n",
    "    priors = class_count / total_data_samples\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Number of samples per class\")\n",
    "        for c, label in enumerate(class_labels):\n",
    "            print(f\"\\t Class: {c} Label: {label} Count: {class_count[c]}\")\n",
    "    \n",
    "    return priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9IXFcq4X8LN"
   },
   "source": [
    "As we can see, the priors are equal meaning our solution to MAP will just be a scaled version of ML. We say \"scaled\" because the prior just acts as a constant that scales the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fhKlfCOhX8LN",
    "outputId": "131f5acf-b73b-4797-bbd5-abd061d83e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples per class\n",
      "\t Class: 0 Label: 0 Count: 50\n",
      "\t Class: 1 Label: 1 Count: 50\n",
      "priors: [0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "X, y = nonlinear_data()\n",
    "\n",
    "priors = compute_priors(y, verbose=True)\n",
    "print(f\"priors: {priors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIe-FAoDX8LN"
   },
   "source": [
    " Furthermore, if the classes were not balanced, the priors might look like the below example. Notice, when we only include 50% of the class 0 data, we can see our priors shift and are no longer equal as our data is imbalanced!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fz30Pr1-X8LO",
    "outputId": "85e4cf34-53c3-4a8a-997e-0850cdcd134f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples per class\n",
      "\t Class: 0 Label: 0 Count: 50\n",
      "\t Class: 1 Label: 1 Count: 100\n",
      "imbalanced priors: [0.33333333 0.66666667]\n"
     ]
    }
   ],
   "source": [
    "imbalanced_X, imbalanced_y = nonlinear_data(balance=[.5, 1])\n",
    "\n",
    "imbalanced_priors = compute_priors(imbalanced_y, verbose=True)\n",
    "print(f\"imbalanced priors: {imbalanced_priors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbApxLz3X8LO"
   },
   "source": [
    "## Computing likelihood P(X | Y)\n",
    "\n",
    "Recall, all we need to do to compute $P(\\xv \\mid y_k)$ is for **each class**, compute the mean and standard deviation for **every continuous feature**. We do so using the training data only!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2efDqGrX8LO"
   },
   "source": [
    "### Gaussian distribution\n",
    "\n",
    "Recall that a [normal or Gaussian distribution](https://machinelearningmastery.com/statistical-data-distributions/) has the parameters $\\mu$ (referred to as mu) which represents the mean and $\\sigma$ (referred to as sigma) represents the standard deviation. An image of a Gaussian is given below.\n",
    " \n",
    " \n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fcommons%2Fthumb%2Fc%2Fc8%2FGaussian_distribution.svg%2F1280px-Gaussian_distribution.svg.png&f=1&nofb=1\" width=500 height=500>\n",
    "\n",
    "\n",
    "The formula for a normal distribution is given as follows:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/00cb9b2c9b866378626bcfa45c86a6de2f2b2e40)\n",
    "\n",
    "<!-- $$\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}}\\exp^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}\n",
    "$$ -->\n",
    "\n",
    "Thus, in order to compute the likelihood for a given class all we need to do is compute the mean $\\mu$ and standard deviation (std) $\\sigma$ for each feature. Computing the likelihood for each feature and class is then done by simply plugging in our data $\\Xm$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_FrXfw_X8LO"
   },
   "source": [
    "Below is the `gaussian_distribution()` function which contains the equation for the Gaussian distribution. A plot is also provided to make sure the `gaussian_distribution()` function works as intended.\n",
    "\n",
    "The `gaussian_distribution()` will be returning the feature likelihoods which are used to compute the likelihoods for each class as given in the following equation:\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\xv | y_k) &= P(x_1|y_k) P(x_2|y_k) ... P(x_n|y_k) \\\\\n",
    "&= \\prod_{i=0}^n P(x_i|y_k)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "z3r-WUCfX8LO"
   },
   "outputs": [],
   "source": [
    "def gaussian_distribution(X, mu, sig):\n",
    "    leading_term =  1 / (sig * np.sqrt(2 * np.pi))\n",
    "    exponent = -0.5 * ( (X - mu) / sig)**2\n",
    "    return leading_term * np.exp(exponent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "-NeSFU1OX8LO",
    "outputId": "b9306ccb-761c-41f5-c6c9-2cc86ac4dfd0",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values = np.linspace(-4, 4, 120)\n",
    "y_values = gaussian_distribution(x_values, mu=0, sig=1)\n",
    "plt.plot(x_values, y_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrxnoZnZX8LO"
   },
   "source": [
    "### Computing parameters\n",
    "Now, onto computing the mean $\\mu$ and std $\\sigma$. To do so, the function `compute_paramters()` is defined. All this function does is loop over the labels (i.e., classes), find the corresponding data samples for each label, and compute the mean and std for each feature. Here is what is happening.\n",
    "\n",
    "First, we get all the unique labels (i.e., classes) that are contained in the labels/targets `y` by using the `np.unique()` function.\n",
    "```Python\n",
    "class_labels = np.unique(y)\n",
    "```\n",
    "\n",
    "\n",
    "We then loop over each unique label found in `class_labels`. \n",
    "```Python\n",
    "for label in class_labels:\n",
    "```\n",
    "\n",
    "Within this loop we then find all the data samples for the current label/class using `np.where()`. \n",
    "\n",
    "```Python\n",
    "class_locs = np.where(y == label)[0]\n",
    "class_X = X[class_locs]\n",
    "```\n",
    "\n",
    "Next, using ONLY the current classes data samples, we then compute the mean and std for all features which are stored in `class_X`. Instead of passing one data sample at a time $\\xv$, we use ALL the data samples for a given class `class_X`. Lastly, we specify `axis=0` so we get the mean and std for all the features (i.e., columns).\n",
    "\n",
    "```Python\n",
    "class_mean = np.mean(class_X, axis=0)\n",
    "means.append(class_mean)\n",
    "\n",
    "class_std = np.std(class_X, axis=0)\n",
    "stds.append(class_std)\n",
    "```\n",
    "\n",
    "Lastly, we stack all the means and stds into a NumPy array. Notice we also add what we call a *smoothing* term to the std. Essentially, all this smoothing term does is prevent the std from ever being 0. To do so, we add a small number like .0001 to ALL the computed std values. If we have a std of zero, computing the likelihoods using the Gaussian distribution equation can become numerically unstable and lead to divide by zero errors. You can think about the smoothing term as a hyperparamter that can be adjusted as well.\n",
    "\n",
    "```Python\n",
    "means = np.vstack(means)\n",
    "stds = np.vstack(stds) + smoothing\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "BOprHyfVX8LO"
   },
   "outputs": [],
   "source": [
    "def compute_parameters(X, y, smoothing=10e-5):\n",
    "    # Find all unique labels for each class\n",
    "    class_labels = np.unique(y)\n",
    "    \n",
    "    means = []\n",
    "    stds = []\n",
    "    # Compute means and stds for each class and feature\n",
    "    for label in class_labels:\n",
    "        class_locs = np.where(y == label)[0]\n",
    "        class_X = X[class_locs]\n",
    "        \n",
    "        class_mean = np.mean(class_X, axis=0)\n",
    "        means.append(class_mean)\n",
    "        \n",
    "        class_std = np.std(class_X, axis=0)\n",
    "        stds.append(class_std)\n",
    "    \n",
    "    means = np.vstack(means)\n",
    "    stds = np.vstack(stds) + smoothing\n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "325iOVPFX8LP"
   },
   "outputs": [],
   "source": [
    "means, stds = compute_parameters(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZR_Vkg-pX8LP"
   },
   "source": [
    "We can see the output of `means` and `stds` is as given below. Notice that the rows correspond to the classes and the columns correspond to the features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "5AnwgKKCX8LP",
    "outputId": "184e7d37-1f85-412f-9e45-055755a6030e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c557cab7-88be-4ac5-ba03-5d7a9942da13\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 0</th>\n",
       "      <td>-0.001266</td>\n",
       "      <td>-0.004266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>0.006055</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c557cab7-88be-4ac5-ba03-5d7a9942da13')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c557cab7-88be-4ac5-ba03-5d7a9942da13 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c557cab7-88be-4ac5-ba03-5d7a9942da13');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         Feature 1  Feature 2\n",
       "Class 0  -0.001266  -0.004266\n",
       "Class 1   0.006055   0.000297"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Means for each feature and class\n",
    "means_df = pd.DataFrame(means, index=['Class 0', 'Class 1'], columns=[\"Feature 1\", \"Feature 2\"])\n",
    "means_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "qcVC0A3fX8LP",
    "outputId": "e843c4c7-25d9-40e7-ce35-25e613dad120"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-cb08ceb9-903c-4891-bf4b-fa21a56d13bc\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Class 0</th>\n",
       "      <td>0.704095</td>\n",
       "      <td>0.711904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class 1</th>\n",
       "      <td>0.359206</td>\n",
       "      <td>0.352977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cb08ceb9-903c-4891-bf4b-fa21a56d13bc')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-cb08ceb9-903c-4891-bf4b-fa21a56d13bc button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-cb08ceb9-903c-4891-bf4b-fa21a56d13bc');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         Feature 1  Feature 2\n",
       "Class 0   0.704095   0.711904\n",
       "Class 1   0.359206   0.352977"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stds for each feature and class\n",
    "stds_df = pd.DataFrame(stds, index=['Class 0', 'Class 1'], columns=[\"Feature 1\", \"Feature 2\"])\n",
    "stds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Op8fz3e-X8LP"
   },
   "source": [
    "### Computing log likelihoods logP(X | Y)\n",
    "\n",
    "Next, we need to compute the likelihood $ P(\\xv \\mid y_k)$ and apply the $\\log$ to get $\\log P(\\xv \\mid y_k)$. To do so is more straight forward than it sounds. The below function `compute_log_likelihoods()` does exactly this. Here is how it works.\n",
    "\n",
    "First, notice the function takes in data `X` along with the `means` and `stds` we computed using `compute_parameters()` function. Using these parameters, the function loops over the mean and std for each class. Recall, that the rows of `means` and `stds` correspond to classes! Each iteration of the loop is then looking at a given class's mean and std.\n",
    "```Python\n",
    "for class_mean, class_std in zip(mean, stds):\n",
    "```\n",
    "\n",
    "Next, the likelihood for EVERY feature is computed for the current class. This is done by passing **ALL** data samples and the current class's mean and std to the `gaussian_distribution()` function. \n",
    "\n",
    "```Python\n",
    "feature_likelihoods = gaussian_distribution(X, \n",
    "                                            mu=class_mean,\n",
    "                                            sig=class_std)\n",
    "```\n",
    "\n",
    "The next line applies the log to each of the feature likelihoods. Recall, this allows us to only have to **SUM** the likelihoods of each feature instead of taking the product between them.\n",
    "```Python\n",
    "feature_log_likelihood = np.log(feature_likelihoods)\n",
    "```\n",
    "\n",
    "As you might expect, the feature log likelihoods are then summed for all data samples (hence `axis=1` is passed as we want to compress the columns or feature log likelihoods). This computes the log likelihood for the current class for ALL data samples!\n",
    "```Python\n",
    "class_log_likelihoods = np.sum(feature_log_likelihood, axis=1)\n",
    "log_likelihoods.append(class_log_likelihoods)\n",
    "```\n",
    "\n",
    "After iterating through all the classes and computing the log likelihoods for all data samples, the  `log_likelihoods` list is then stacked and transformed. The transform `.T` is applied because the shape of `log_likelihoods`, after being vertically stacked, is `(2, 100)` and we want a `(100 , 2)` so that the data samples are the 1st dimension and the classes are the 2nd dimension.\n",
    "```Python\n",
    "log_likelihoods = np.vstack(log_likelihoods)\n",
    "log_likelihoods = log_likelihoods.T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "hPYAvg-mX8LP"
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihoods(X, mean, stds, verbose=False):\n",
    "    log_likelihoods = []\n",
    "    for class_mean, class_std in zip(mean, stds):\n",
    "        # Compute likelihood for every feature uisng all data samples\n",
    "        feature_likelihoods = gaussian_distribution(X, \n",
    "                                                    mu=class_mean,\n",
    "                                                    sig=class_std)\n",
    "    \n",
    "        feature_log_likelihood = np.log(feature_likelihoods)\n",
    "        \n",
    "        class_log_likelihoods = np.sum(feature_log_likelihood, axis=1)\n",
    "        \n",
    "        log_likelihoods.append(class_log_likelihoods)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"feature_likelihoods shape: {feature_likelihoods.shape}\")\n",
    "            print(f\"feature_log_likelihood shape: {feature_log_likelihood.shape}\")\n",
    "            print(f\"class_log_likelihoods shape: {class_log_likelihoods.shape}\")\n",
    "            print(\"-\"*50)\n",
    "        \n",
    "    log_likelihoods = np.vstack(log_likelihoods)\n",
    "    log_likelihoods = log_likelihoods.T\n",
    "    return log_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJbZLWG_X8LP",
    "outputId": "4f9bc08f-e839-4dca-8cc7-06bdcefcab18",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_likelihoods shape: (100, 2)\n",
      "feature_log_likelihood shape: (100, 2)\n",
      "class_log_likelihoods shape: (100,)\n",
      "--------------------------------------------------\n",
      "feature_likelihoods shape: (100, 2)\n",
      "feature_log_likelihood shape: (100, 2)\n",
      "class_log_likelihoods shape: (100,)\n",
      "--------------------------------------------------\n",
      "log likelihood shape: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "log_likelihoods = compute_log_likelihoods(X, means, stds, verbose=True)\n",
    "print(f\"log likelihood shape: {log_likelihoods.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-zw2S9jX8LP"
   },
   "source": [
    "Notice that the output is of shape `(100, 2)`. This is because we just computed the log likelihoods $\\log P(\\Xm \\mid y_k)$ for all data samples and classes. Thus, the rows correspond to data samples and the columns correspond to the classes. Each element then corresponds to the log likelihood that a data sample corresponds to a given class $\\log P(\\xv \\mid y_k)$.\n",
    "\n",
    "Here we can roughly say that the larger the value, the *more likely* a data sample belongs to a particular class. For instance, the first data sample is more likely to belong to class 2. However, because we are using MAP, and not ML, we still need to add our log priors to log likelihoods before we can truly say which class a data sample belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "_yhxQIXPX8LP",
    "outputId": "179fcd66-73dd-47a5-d6ad-d5d21f22a5c4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-176abc5d-05dd-449c-ae4e-a04d0ba956da\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class 0</th>\n",
       "      <th>Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.414546</td>\n",
       "      <td>-0.878092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.452743</td>\n",
       "      <td>-0.924029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.408875</td>\n",
       "      <td>-0.812474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.080584</td>\n",
       "      <td>-3.414450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.333776</td>\n",
       "      <td>-4.433950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-1.374720</td>\n",
       "      <td>-0.667986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-1.325194</td>\n",
       "      <td>-0.479945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-2.102022</td>\n",
       "      <td>-3.623078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-1.423225</td>\n",
       "      <td>-0.881859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-1.384054</td>\n",
       "      <td>-0.654232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-176abc5d-05dd-449c-ae4e-a04d0ba956da')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-176abc5d-05dd-449c-ae4e-a04d0ba956da button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-176abc5d-05dd-449c-ae4e-a04d0ba956da');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     Class 0   Class 1\n",
       "0  -1.414546 -0.878092\n",
       "1  -1.452743 -0.924029\n",
       "2  -1.408875 -0.812474\n",
       "3  -2.080584 -3.414450\n",
       "4  -2.333776 -4.433950\n",
       "..       ...       ...\n",
       "95 -1.374720 -0.667986\n",
       "96 -1.325194 -0.479945\n",
       "97 -2.102022 -3.623078\n",
       "98 -1.423225 -0.881859\n",
       "99 -1.384054 -0.654232\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_likelihoods_df = pd.DataFrame(log_likelihoods, \n",
    "                                  columns=[\"Class 0\", \"Class 1\"])\n",
    "log_likelihoods_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGzYguDAX8LQ"
   },
   "source": [
    "## Computing predictions \n",
    "To compute the predictions is easy! Recall, we need want to use the log of MAP which is defined as follows:\n",
    "\n",
    "$$\n",
    "\\arg \\max_{k \\in K} \\log P(\\xv \\mid y_k) + \\log P(y_k)\n",
    "$$\n",
    "\n",
    "\n",
    "All that is left is taking the log of the priors `priors`, adding the log priors $\\log P(y_k)$ to the log likelihoods $\\log P(\\xv \\mid y_k)$, and taking the $\\arg \\max_{k \\in K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJae5A8lX8LQ"
   },
   "source": [
    "Below we compute the log of the priors by applying the `np.log()` function to `priors` and add the output to `log_likelihoods`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5GcAlQTX8LQ",
    "outputId": "73a3f6cb-a4af-45ed-b228-af18a59fd31f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint log likelihood shape: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "joint_log_likelihoods = log_likelihoods + np.log(priors)\n",
    "print(f\"joint log likelihood shape: {joint_log_likelihoods.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "Ja2zM0m1X8LQ",
    "outputId": "6057fdf5-1b66-44ff-e4b0-fc9678e7efd9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-d2248243-2599-4b86-adc1-dff8fef459b5\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class 0</th>\n",
       "      <th>Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.107693</td>\n",
       "      <td>-1.571240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.145890</td>\n",
       "      <td>-1.617176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.102022</td>\n",
       "      <td>-1.505622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.773731</td>\n",
       "      <td>-4.107598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.026923</td>\n",
       "      <td>-5.127098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-2.067867</td>\n",
       "      <td>-1.361133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-2.018341</td>\n",
       "      <td>-1.173092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-2.795169</td>\n",
       "      <td>-4.316225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-2.116372</td>\n",
       "      <td>-1.575006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>-2.077201</td>\n",
       "      <td>-1.347379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2248243-2599-4b86-adc1-dff8fef459b5')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-d2248243-2599-4b86-adc1-dff8fef459b5 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-d2248243-2599-4b86-adc1-dff8fef459b5');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     Class 0   Class 1\n",
       "0  -2.107693 -1.571240\n",
       "1  -2.145890 -1.617176\n",
       "2  -2.102022 -1.505622\n",
       "3  -2.773731 -4.107598\n",
       "4  -3.026923 -5.127098\n",
       "..       ...       ...\n",
       "95 -2.067867 -1.361133\n",
       "96 -2.018341 -1.173092\n",
       "97 -2.795169 -4.316225\n",
       "98 -2.116372 -1.575006\n",
       "99 -2.077201 -1.347379\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_log_likelihoods_df = pd.DataFrame(joint_log_likelihoods, \n",
    "                                        columns=[\"Class 0\", \"Class 1\"])\n",
    "joint_log_likelihoods_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZMI6xJuX8LQ"
   },
   "source": [
    "Lastly, we just need to take the $\\arg \\max_{k \\in K}$ which can be easily done by using NumPy's `np.argmax()` function. When this function uses `axis=1` it finds the maximum value for each row and returns the **INDEX** for which column contained the largest joint log likelihood. Remember that the columns represents the classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mj8ejxqAX8LQ",
    "outputId": "2cd1f92c-e607-4e6d-908a-6f57dcd27180"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = np.unique(y)\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dfRU_vfzX8LQ",
    "outputId": "b76a802f-6f01-487d-c938-7798551ac8f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = np.argmax(joint_log_likelihoods, axis=1)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnvsX2nSX8LQ"
   },
   "source": [
    "Below is a visualization of data and the decision boundaries Gaussian Naive Bayes creates. Notice that the yellow area corresponds to all points that would be predicted as class 1 and the purple area corresponds to all the points that would be predicted as class 0.\n",
    "\n",
    "As you can see, Naive Bayes can easily handle this non-linear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "QNU01btwX8LR",
    "outputId": "e14b57ae-28fc-441e-89d2-88bd5bf80a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples per class\n",
      "\t Class: 0 Label: 0 Count: 50\n",
      "\t Class: 1 Label: 1 Count: 50\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD7CAYAAACG50QgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xU1dkv8N+ThJsYI0gNEhWohxMvFN5SQBJaAYtWEeXu7Zg2IKTITS6W0vpqz6vHo9JyE0QMlETj8VIQKAIVoYp6DNSgnwaDNC2KCEHAC2Kg3BLW+8fMhMlkLnv2fc/+fT8fPkxm9uy9di7PrP2sZ60tSikQEZE/pDndACIisg+DPhGRjzDoExH5CIM+EZGPMOgTEfkIgz4RkY+YEvRFZLmIHBaRqhiv9xeRoyLy9+C/h804LhERJSfDpP2UAlgE4Pk427yrlBps0vGIiEgHU4K+UuodEelkxr7CNU9rpVplXGD2bomIUtZ3Zw5/pZT6XqzXzerpa5EnIpUADgB4QCm1M9EbWmVcgPyL77C+ZUREKeL1moV7471uV9D/EEBHpdQxERkEYA2ALtE2FJEiAEUA0DI906bmERH5gy3VO0qp75RSx4KPNwBoJiLtYmxbrJTqqZTq2TytlR3NIyLyDVuCvoi0FxEJPu4dPO7XdhybiIjOMSW9IyIvAegPoJ2I7AfwOwDNAEAptQTASAD3iUgdgBMA7lRc3pOINGid1RIj7/8Jsju1QVqg7+h7Z5XCoc+OYOWCd3H86Mmk3mtW9c5dCV5fhEBJJxFRUkbe/xNc3aMLWmS0gjDoAwCUUriobTuMvB947pFNSb2XM3KJyNWyO7VhwI8gImiR0QrZndok/V4GfSJytTQRBvwoRERXuotBn4jIRxj0iYji+PKrw5j260kYOPgnGH7XLRg38RfYs/dT7K/Zh8EjbrDkmKdPn8LUmRNxw63XYdQ9Q7C/Zp9p+2bQJ6KUkrlhDTrfnI8uP+yEzjfnI3PDGt37Ukph0vQi9O7ZB5vXvYtVL63HjCm/xtdff2Vii5tasfoVXHBBFja99g4K77kXf1jwhGn7ZtAnopSRuWENsh+ZhWZf1ECUQrMvapD9yCzdgX9bRTkyMprhrlH3NDx3Ze7V6Nmjd6Pt9tfsw92jR2LYnYMw7M5B+PDv2wEAh788hP81ZhSG3H4zBo+4Ads/fB/19fWY9dAMDB5xA24deSNKy5Y1Oe6bWzZh2K0jAAA/GzgIW99/D2ZVudu59g4RkaXaLZyNtJMnGj2XdvIE2i2cjdpBQ5Pe3792/xPXXPWDhNtd1LYdSpa8gBYtWuKzvXsw/TeTserFdVj3lz/jx3nX4b5xk1FfX48TJ09gV/XHOHT4INa9Gii1/O67o032d+jwQVzSvgMAICMjA5nnZ+LIt0fQtk3bpM8hEoM+EaWMjIMHknreLHV1Z/DIEw/jH9UfIy0tHZ99/ikA4AfXdMdv//evUFdXh4EDbsRVV16Dyy69HPtqPsejTzyMfj+5Hj/Ou87StkVieoeIUkZdsHes9flEulzRBTt3fZRwu9IX/oh2F7XDn//0Ol598TWcOXMGANDrR9fihT+uQPbF7THr4Qew5rVXkXVBFv78p9fRu2cfvLzi/+HB/5rZZH/ZF7fHF8EPqrq6OtQeq0WbC5OvyY+GQZ+IUsZXk2fibMvGCzWebdkKX01uGli16NO7L06fPo1XVr7Y8Nw//rkL2z98v9F2tcdq8b12FyMtLQ1/XrcK9fX1AICaA/vR7qJ2uH3EXRg1/A7s3FWFb458A3X2LH42cBCmTnoAH+9qusr89f0GYvVrrwIANm7egD698k2bq8D0DhGljFDevt3C2cg4eAB17Tvgq8kzdeXzgcAEqEVzi/F/f/9fWFr6DFo0b4GcDpfit7/6XaPt7r69AJMfGI8161bhJ/n9cF6r8wAA72/fhj8+9ywyMjJw3nmt8eT/mYvDhw/iN797AGfPBgZmp09p+oE0ctgd+NWD03DDrdch64ILMe9J81axETeve5bVPFvxJipE/jar5A5ccvFlTjfDlb44vA9PjH6l0XOv1yz8QCnVM9Z7mN4hIvIRBn0iIh9h0Cci8hEGfSIiH2HQJyLyEQZ9IiIfYdAnIorDiaWVKz74G4bdOQhX/+j7eH3TelP3zaBPRCljaUkmtlW0aPTctooWWFqSqWt/Ti2tfEn7Dnj8kTkYfPMQ0/fNoE9EKeMHXU9j6sx2DYF/W0ULTJ3ZDj/oelrX/pxaWvnSnMtw5f+8CmlifojmMgxElDL69DqF+bO/wtSZ7XDXqGN4acX5mD/7K/TpdUrX/pxaWtlKDPpElFL69DqFu0Ydw+LiLEwoOqo74CeDSysTETlkW0ULvLTifEwoOoqXVpzfJMefDKeWVrYSe/rke/2PV6Owdiu+V1+LL9MzUZqZhy2tc51ulmvb5WahHH4opXNtr5ONvk5Wn959MXfh7/HKyhdxx8i7AQSWVj52rBbtsy9p2K72WC3aZ7dHWloaVq9d2Whp5fbZl+D2EXfh9JlT2LmrCtf9eACaN2uGnw0chM6drsCvfjvVnJPXiEGffK3/8Wrcf/RNtFR1AIDs+lrcf/RNAHA0wLq1XW73UVXzRgE+lOP/qKq5rqDv1NLKO6oqMWl6Eb777ijeemczFj4zD+tXbU66/VHPyYyllUVkOYDBAA4rpbpGeV0ALAAwCMC/ARQqpT5MtF8urUxWKz1Yiuz62ibPH0rPRGH7QvsbFOTWdjmBSyvHpmdpZbN6+qUAFgF4PsbrNwPoEvx3LYBngv8TRWVXauN7UQJrvOft4tZ2kfeZEvSVUu+ISKc4mwwB8LwKXFZsE5ELReQSpdQXZhyfUoudqY0v0zOj9qi/TI8+mceuDyOt7WLen5JlV/VODoB9YV/vDz5H1ERh7daGgB/SUtWhsHar6ccqzczDSWnc9zkpGSjNzGuybejDKLu+Fmk492HU/3i1I+2ysz1OOqsU3HyHP6copXBWx/fFdQO5IlIEoAgAWsbobVFqM5raSKb3G3pey/bxPozM7l1raZed7XHSoc+O4KK27dAio5VpNwf3OqUUTtWdwKHPjiT9XruCfg2A8JGYS4PPNaGUKgZQDAQGcq1vGrlBeKBWEABNf/SxUi6R+0k2NbSlda6mIGl3nj1Ru/yS91+54F2MvB/I7tQGaQz6AAJXP4c+O4KVC95N+r12Bf21ACaJyMsIDOAeZT6fQiIDNaCgAIT/ecdKuUSysvebbP7fam5rj1WOHz2J5x7Z5HQzUoYpOX0ReQnAVgC5IrJfRO4VkfEiMj64yQYAnwLYDWApgAlmHJdSQ7RALQDqITiLQJnigqzrHe+NJ5P/t4Pb2kPeYFb1zl0JXlcAJppxLEo9sQKyQOGWnMlJ7cvK3m8y+X87JGqPnsoeVgOlPtcN5JL/mBmoSzPzIlJF5vZ+teb/7RKrPXrGNjgL2B+44Bo5zsw0xZbWuViQdT0OpWcmnRpKJXrKXu0slSXnsKdPjjM7beK23rgT9Ixt+KUayO8Y9MkVGKjNpSdl5pdqIL9jeocoBelJmbEayB/Y0ydKQXpSZm6rTiJrMOiTISzxcy89KTOm2VIfgz7pxhI/Iu9hTp90Y4kfkfcw6JNuLPEj8h6md0g3lvj5C8dvUgODPsUV7w/d6iUPyD04fpM6GPR9SkuvLdEfOkv8/MMvN2zxAwZ9H9Laa9Pyh84SP3/g+E3qYND3Ia29Nv6hU4jZ4zccH3AOq3d8SGswj/UHzYFa/zFziQa/3NDdrRj0fUhrMOdaLBRi5pLVnN/hLKZ3fEhr1U2qD9QeGNEZtVecdboZyJ291+kmaGLW+A3Ths5i0PehZIK5lwdqT3TNifnaNxOPo6z7XBtbE1tZv3xs/3WPJs+3qqpxoDXW4/wOZ0ng9rXulNU8W+VffIfTzSAPqnnmQpR1L3G6GYYUVI5G26dbp1zwj6weAwJXmn68w5kVXq9Z+IFSqmes19nTp5RQPbMjzu98FABw4+XVeKLNaodbZFxZ9xKUPZmPNz5vHAiP7cnyTEoomlBgH3/0HVygTgIATiHdySb5CoM+edKJrjk4ktscAKAGHsHq7vMdbpE1CtqUo6BNeeMnuwdSQpuW92l4qsOre2xumXEtUAcJPs5SpzjD1yYM+uQ51TM7YvXQ1AzyWhW0KUfBjHMfBgUDvZUK4gxf5zDo+5hXJsgcGNEZN4zZ1vB1KqRuzBZKBYWseru3q1NArOBxDoO+T7l5Aa0TXXPw+aDAr+b5nY+aVmVTsmQ8unarRK/8c/XgFeV5qNrRHaPHLzHlGE4KTwMVDC1HQWf39v5ZweMcBn2fcuvltZWpm67dKjFz4jOY/fR96JW/FRXleQ1fp6Ky7iVAMTBszVRcviHws3bLB4AVK7R65crVaaYEfRG5CcACAOkAlimlnoh4vRDA7wGEfuMWKaWWmXFs0sdtl9cnuubgm4nHLR2Q7ZW/FbOfvg8zJz6DUfeUYcULBQ0fAGZx49XE6qHzgaGBx2VHAgPATg/8mj3xz81Xrm5jeBkGEUkH8DSAmwFcDeAuEbk6yqavKKX+I/iPAd9hTq+r82ntT/H1qS4AAr37BcWLMOX4P1CyZLylx+2VvxWj7ilD8VNTMeqeMlMDPnDuaqKiPNBjDV1NdO1Waepx9CpoU47nZ8xF9cyOTjcFW1rnorB9IW7JmYzC9oWGgjOXdtDOjLV3egPYrZT6VCl1GsDLAIaYsF+ykJPr6hwY0Rl1Pz2ND4/di+r7fojVQ+fbFhwryvOw4oUCFE2ZjxUvFDQEZ7OEX00snjOjUTrJTVYPnY9r1h/CgRGdnW6KKdx25epmZqR3cgDsC/t6P4Bro2w3QkSuA/BPANOUUvuibENJ0pvHdGpdnUDOPjAwW3H7ikBwPHzUklRLpPAcfq/8reiVV25JUA6/miiaMt91AT8kVPY57Iqprq700YIDw9rZtcrmawA6KaW6AdgE4LlYG4pIkYhsF5Htp8+esKl53mR0iVozL68TOTCiM65Zf6jRIK3VqZZIVTu6NwrwoV551Y7uph4n3tVEyZLxTa4uKsrzLE9rxZMKvX6uCKudGUG/BsBlYV9finMDtgAApdTXSqlTwS+XAfhRrJ0ppYqVUj2VUj2bp7UyoXmpyyt5zOqZHfH8jLlNZpaalWrRGkhHj1/S5IOlV/7WhgFWMwJy+NXEhBlzGlI9of26NecfyvXXPHOho+3Qy8yln1OdGUG/AkAXEeksIs0B3AlgbfgGInJJ2Je3AdhlwnF9z+15zGi9+5BEwTGR8AAdCqRlS8diYmEpypaObRRItQZuPQE58oOiakd3jJmwqOHqIfJqwu05/7LuJZ7t9dt55eplhoO+UqoOwCQAGxEI5n9SSu0UkUdE5LbgZlNEZKeIVAKYAqDQ6HHJ+QqceGL17kOMplrCA3Sv/K0YM2ER5j72EOrPZGDuYw9hzIRFjWrxtfSk9QTkyA+Krt0qsXzxpEbHi7yaANAorRX+vBuE9/rjLU9N3sSllT3MrUvUhmrurV7aOBTQQzX3ffu/hXWrRmLw8JV4b8sAzbX4kbX1i+fMQPFTU9G777sofvHupNsR73gV5XmYVrQUgODu0cvxYskYAArzisfZ2tvXOp/ALXX9pF2ipZV5u0QPc2se8/NBGbasZR8+ENy331t4b8sAFE2Zj/e2DEDffm9pHiAO761XlOfhxZIxaNny3/h4R3dN6abkB6QFQKizpYJf20trKsuLvf7+x6tRerAU62sWovRgKe+9G4E9fTJNaGG0WCkds4UCVd9+b2H9muGY/uCjKBi3DGVLx2LuYw/hlqGr8N7bAzTlzEM98LozzZDR7AzmFY8DAE0pnmR6+qEedsXW/HMlnXnljszYTabdgDd6/W69+rUTe/pki0Q5fLOFDwRfcWU1pj/4KJYvnoSypWOxfPEkTH/wUVxxZbXmAeJe+VtxdbcdOHnyPNw9uiRQx69hnCHZAelQYA+vWgp/3k7JXqF4odfvlYo2J3HBNTIk1Lu3e7nj8IHgULC68pqdKC3+ZZMeayhwJ+qt/2vX1Q2BuFdeecO+470v3oB0tPcZmSBm9ro+kSWzoXNOpKx7CYYNmorcqqQPaTm3V7S5AdM7pNuBEZ3x/AzvL3scGYgjvzaTkfOsKM/DlDElmDDjDygYt6yhnWMmLEJdfUZS3yej5+zW+/eWHiyNOjP3UHomCtsX2t8gBzC9Q57g5KQlu2bqAokniMXTK38rJsz4A+Y+9hD+c9q8hoAfWSIaTbLzCRIp616Cnk9+6Lp6fs7MTYw9fdLFinXvkx1Y9Kv/nDYP61aNRI/e27BndxfNA9VWXM0MW+O+dXv8vq5+op4+c/oe44ZfaKtudOKVhcqcVFGeh/e2DECP3tvw4ft9MHjYyobvU6LUkRX3Ehje731s+sRdFT1bWuf6Ksgni+kdDzG6wJrbWb3ssdeF5/D37O6CwcNWYv2a4ShbOhZA4hSZFQvcuWl9ftKGPX0PcfoWh1be3cquZY+9LJSHX754UsP3JfeanVg85wFcec3OhHcGi1atU7Wju+vu9EXWYk/fQ5wuR7Nypq2dg6leNXr8EtTVZzT6PhWMW4anlo9uNCAbrTcfaz5BRnqdKQPoq4fOZ2/fIziQ6wJa8/ROlqPZtZ5OiBvvNesFsQbD430/Q2khM3L9bi3l9BMO5LpcMjd0Ls3MizrF3OpyNKsGbuMJBaJo1SYUXbwUWbQPyvCJZ34bQHdDQYRTmN5xWDLTxp1YYO1E1xyc3/moZfuPxe3rzruR3hSZmQPoofp9ty7TAKR+QUQi7Ok7LNk8vZ/K0VjCmZxEvflorBhAL2hTjjcm5qLt0zmuTPM4XRDhNPb0HebmG6E4jSWc1vPjALrTBRFOY0/fYU7l6bWwe6nkcCzhtIeeqwM30ZOb/zI9M2pBhF86WuzpO8ytN0I50TUHauAR3QHf6E3G/dgDTSVl3UvwzcTjlub29ebm/b4+D3v6NonXI0nFPL3R6huv90ApEPjLnrTuxit6c/Oh1/xavcOgb4NkyjJTRaLZoURGGcnNp2JHSyumd2zgp7v5hKd1wqtvulz1MQO+yxhNwTmNRRD6MOjbwE/VAk1vMj46eJPxbqy+cRkn72FgBr/n5vVi0LeBn3okobTOtKJlmDy6FIBgYUkh5hWP03Sv2hCv90K9ILSAW/gEuPAbq7idW4sg3I5B3wZe65EcGNEZC4oX6V5nJ3CT8crgTcaXa77JeDiv90K9oGu3SixfPAl9+72F4qemom+/tzTdhctNtrTORWH7QtySMxmF7QsZ8DXgQK4NjFQL2L1GSKhU0wi9NxkPx4Fg6/XK34oxExZh7mMPoUfvbVi/ZjimP/gov8cpjkHfJnqqBbxY9WPmpCouw2CtivI8LF88CbcMW4V1q0Zi8PCVWL54UsPa/JSaTEnviMhNIlItIrtFZFaU11uIyCvB1/8mIp3MOG6q82LVj5mTqrgMg7VCOf33tgxA0ZT5eG/LAE/l9Ekfw0FfRNIBPA3gZgBXA7hLRK6O2OxeAEeUUv8DwDwATxo9rh94sepn9PglTXqJvfK3Jr0GfqybfjDwGxM+QB7K6Y+ZsAitWh/HgJv+gmcXTGuU0+fgeeoxo6ffG8BupdSnSqnTAF4GMCRimyEAngs+XgngpyIiJhw7pbmp6kdvNY3e93EZBmuED5CH336xa7dK3HzrWgAKr6+9DQAHz1OVGUE/B8C+sK/3B5+Luo1Sqg7AUQAXmXDslOamqh+91TR632fWFQM1Fj5AfuJ460b32+2VvxXzisfhzY038x4GKcx1A7kiUgSgCABapmAdezLctEaI3moaVuG4T7wBci8Pnvv5bljJMCPo1wC4LOzrS4PPRdtmv4hkAMgC8HW0nSmligEUA4F75JrQPk9z0xohegOClwNJKoocIA+V1CZ6zc28WOnmFDPSOxUAuohIZxFpDuBOAGsjtlkL4BfBxyMBvKncfEd2ikpvNQ2rcNwj3gC5lwfPvVjp5hTDPX2lVJ2ITAKwEUA6gOVKqZ0i8giA7UqptQD+CKBMRHYD+AaBDwbyEL319/HeV7WjO7p2q2z0/tAAI3P31kg0QB7rNbf39r1Y6eYUU3L6SqkNADZEPPdw2OOTAEaZcSxyRrxgES8gxHuf0TX3KXnJ3qfAK/cw8PvdsJLhuoFccie9NzVJ9D4O8pIZ3HzbUbfhgmvUSKuqGsjmNig7km/L8cIHeUfdU8aAT7pwxU3tGPSpiQ6v7sH2X/dAQeVoXe9PZkIWB3mdkYpLV3PFTW0Y9Ml0WidkeblaxOu4dLV/MadPptM6IUvv4DAZx0lz/sWg73JenWWoZUKW3sFhMoeeSXMlS8YnVWa76u3eyH11j6nt1sqrfztWY3rHxUKzDLPra5GGc7MM+x+vdrppCTFX7356fkZa00JlR/Lx8znTkTt7ryVtT8TLfztWY9B3MSdnGbaqqkHOfd9i2JqpSb+XuXr30/szCk8LxVuU7Y3Pc9Gm+rSVpxAXZ+jGxqDvYm6YZXj5hrqkq3gSzfpMxcoRrzGydLUXymzd8LfjVgz6Luam9fSTkWhZZFaOOM/I0tVeSN3VSouoz7v9b8cOHMh1iWiDTqk6y5CVI85JdiA2UqI1mMqO5GPT8j7IeXUPgG8tPJPY+h+vxnnqTJPnT0M8/7djBvb0XSDWoBOARrMMj0oLnEIGfvXtGyg9WGrLoFQot693olYsXkgRpCKjV1mJ0kJO5/KBQD6/Gc42ef6EtGD1Dhj0XSHeoFNoluHvL7wRLVCPLHXSkWoEvYO6sXghRZCKtA7ExuKFO5rFyttnqpM2t8SdGPRdQMugUypVI0SrHJkypgRlS8c22Y6Du+ZL9assr46F2YVB3wW0/JKmUjVCtBTBhBl/wOI5D3Bw12IlS8ajbOnYRldZZUvHmvLhOmzNVOTc9y1aVUXeOM9ebrq3tBsx6LuAll9SN/Re9JRvRhMtRVAwbhmeWj5ad9qBtMlIr8Pcxx7CmAmLMGHGHIyZsAhzH3sIGel1id8cx7A1Ux2biBWJK27Gx+odF9ByA3Q3VPIEBnWBYTOnYvXQ+abvX8uyAEarT/yurj4D0x98FMsXT0Ltd1lY8UIBpj/4KOrqUysUuOne0m6TWj9pD0v0S6rlg8Hrot2UO/KWil27VWJa0VL8bPBreOiJ38S925ZXPiDsbGdof7XfZTV8uBaMW6Z7f6ESTafW16HkMb3jIW5ZLzx39l5TK3mA2MsCZKTXRVkeQLBx3a0J00BemQRmdzvNrJxa9XZvdHAg4Pc/Xo3Sg6VYX7PQtvLlVCFKKafbEFNW82yVf/EdTjeDYjjRNQffTDyOsu4lhvcVr7cbCorhE7kqtuY39FQnzJgTc7+hAOr2SWB2tTNyclXk18kI9fLtDvqheS2RqU7m7QNer1n4gVKqZ6zX2dMn3YwsyhYpXv13ZIkhAM09Va+UJ5rdzljrG5UW/1L3mjvhhq2Zip23ZDvSy0+l8mUnMOiTYZmfpFl6T93wdMSLJWMwrWip5tUhvTIJzOx2xkoZFRY9a2hyldNLJgOpVb7sBAZ9MqzDq3uw6u3eluw7Mtd/461rAUjD6/F6qm5Y4lnLiqJWtNPozNtonErnRHJD+bKXMeiTKXJn78X9RZNM7/FHTuR6+PHfYF7x2EZBPlZP1cjywWbRMkhrVTu9ktpKFidfGcOBXDJd9cyOltTxW8nKskkjg7RG2mXm4HBB5Wjk3OfMqpnR8FaIsXEgl2wX6vWbvTJnIkZuzhLZI3/kN49jWtGyRj1yvWsBGelx6y3nNCtlFMrhuyngA+4pX/YiQ0FfRNqKyCYR+Vfw/zYxtqsXkb8H/601ckzyBjMre7QyUu8emQN/47XbAJy7Co62L60fMkYGafXm5kMpo6od3VFRntcoZaT1w8vJCh2yjtGe/iwAf1VKdQHw1+DX0ZxQSv1H8N9tBo9JHmJnr9/o4GV4j/zu0csxr3hc3H1p+ZAxo8et50ohVOoa3sbwr+N9ELqhQoesYzToDwHwXPDxcwCGGtwfpSA7e/1GUimRPXIAcfcV+SEzZUwJxkxY1Gi719fehgE3/cXQIK2dVwrs3ac+o2vvZCulvgg+PgggO8Z2LUVkO4A6AE8opdYYPC55UO7svbh/wyTTZvFGE239Hi2BP9ptAKcVLQOg4u4r/ENm8PCVWL54Eq68ZmfDbNc3N97cZF2gXvlbNX8YJbo9oRZaFrJzag0dDsjaL2FPX0Q2i0hVlH9DwrdTgTKgWKVAHYOjyXcDmC8iV8Q5XpGIbBeR7afPnkjmXMgDrOz1G0mllBb/skkvva4uHZ2u+CTuvsI/ZN7bMgBjJiwytTbejHLORFcKTvXuY90mlOvoWMtQyaaIVAPor5T6QkQuAbBFKRX3Y1pESgGsU0qtTLR/lmwmx2u9JjPX7gHMKW8MBdhHZz2OjetuxbzicQ37i9xXrHVs+vZ7C+tWj0y4LpAd4q2184+rxNHJVqUHS5EdZRbtofRMFLYvtL9BKSJRyabRoP97AF8rpZ4QkVkA2iqlZkZs0wbAv5VSp0SkHYCtAIYopT5OtH8Gfe28vAjVgRGd8fyMuU43I+m69mgfMmVLx2LxnAdQMG6pKxZ4i2xjQeVoHNvRGd++fwWu/SBhv8tS62sWRk01nAVwS85ku5uTMqwO+hcB+BOAywHsBXC7UuobEekJYLxSaqyI5AN4FoGfZRqA+UqpP2rZP4O+drF6TfUQCJTre/6hXv+Nlwcu7QvalDvSjsVzZmhavTMaM1ewNFtB5Wi0fbq147cyDOl/vBoPfLsJ6VEywuzpG5Mo6BsayFVKfQ3gp1Ge3w5gbPBxOYAfGDkOJRZrsanQH1UoXwrAlYE/dFeu7V17AAA25fbBDWO22Rr89Q4Ch8TLvzsZ9M/dytAdE6xCV6XRAj6XU7Ael2FIEbF6+pG81ouqntmx4bGVSzu4uZeerILK0Ti2JwtA4L7Gbundh8S7Kv3DhTe4slPiJZb29Mk9ot1DNxqvLT8bPkHIynJPt/bStQgtcvfG57mQzW2Q8+oeuKVXH02s30GBYsC3AYN+ioi8h66CRL189kLxCC4AAAnoSURBVPLys1bemD1adU8y9fROOZe6AXLwLdwc7EO+TM+M2tP38u+mlzDop5Dwm6vHquZJhXxpaJLXkdzmjZ6vveKs51b3TFZB5WjI5sASV22qTyO3yntLJUS7Kk2V300vYNBPUZE9f7dX7ySrVVUNWlU1ff7nn0yHGnikyfNWzQC2StmRfLzxeeOf1bE9Wa4akNUr1X833Y4DueQL1TM7Yni/9x0rBdWq7Eg+Vr3dm4udkW4cyKWkeG1Wr1a5s/di+4Ye2I4eUV//fFCGbamhYWum4vINsQfcvZiyIe9g0KcGkeMAbq/tT1a80sXcqkB10OeDrP2TyPwkzfZFzYjCMehTg8LarU1KPluqOhTWbk2JoJ9Iq6oa5EYZJ6DUvQL0IwZ9ahCrftprtf1kLjuuAPmhYh/eI5caxKqTZv20v8W7AjQDl1i2F4M+NSjNzMNJaXzxx/ppsvoK0OoPFWqM6R1qkGz9NC/J/cHqGbRMK9qLQZ8aCZ/VG0+qV/rQOVbPoOWyDPZieod04SW5f2xpnYsFWdfjUHomziKwUquZN+dhWtFe7OmTLrwk9xetV4B69w1wWQa7MOiTLrXSElnqZNTniZJl5YcKNcb0DukUa80m967lRETs6ZNOmepUUs+Te7EKy18Y9EkXqyouGIDsxSos/2F6h3SxouKCMzPP6X+8GqUHS7G+ZiFKD5Za9j1gFZb/sKdPulhRceH3Bd9CrO59h19NSYxtWIWVuhj0STezKy5YBhpg5YdftNtoRsOJUamLQZ9cQ8s4gR9y/lZ++EX7QInEiVGpjTl9co1E4wR+yflbudpprA8OBVgy25bchz19co1E4wRezPnruTKJttbNaQhanj2D9TULDV3hxLqaOpyeicL2hUnvj7yHQZ9cJd44gZG0hxNpIb0DspEffrXSAuepMw0zoI0M7Fq9eBq5n6H0joiMEpGdInJWRGLefV1EbhKRahHZLSKzjByT/Etv2sOptJCRcsgtrXNR2L4Qt+RMxsm05miGs7r2E22/Vi6eRu5ntKdfBWA4gGdjbSAi6QCeBnADgP0AKkRkrVLqY4PHJp/R20t1Ki1k1oCs2QO7XOfG3wz19JVSu5RSibpLvQHsVkp9qpQ6DeBlAEOMHJf8SW8v1alSULMGZHkbSzKTHTn9HAD7wr7eD+DaWBuLSBGAIgBoyV9qiqCnl+rUTTrMyp8zD09mStjTF5HNIlIV5Z8lvXWlVLFSqqdSqmfztFZWHIJ8xqmbdJiVP2censyUsKevlBpo8Bg1AC4L+/rS4HNEtnDyJh1m5c+Zhyez2JHeqQDQRUQ6IxDs7wRwtw3HJWqgJWj6YbYvkdGSzWEish9AHoD1IrIx+HwHEdkAAEqpOgCTAGwEsAvAn5RSO401m8hcfpntS2Sop6+UWg1gdZTnDwAYFPb1BgAbjByLyEpenO1LpAfX3iECV/gk/2DQJwJr4ck/GPSJ4FxZJ5HduOAaEZwt6ySyE4M+URBr4ckPmN4hIvIRBn0iIh9h0Cci8hEGfSIiH2HQJyLyEQZ9IiIfYdAnIvIRBn0iIh9h0Cci8hEGfSIiH2HQJyLyEQZ9IiIfYdAnIvIRBn0iIh9h0Cci8hEGfSIiH2HQJyLyEQZ9IiIfYdAnIvIRBn0iIh9h0Cci8hFDQV9ERonIThE5KyI942z3mYh8JCJ/F5HtRo5JRET6ZRh8fxWA4QCe1bDtAKXUVwaPR0REBhgK+kqpXQAgIua0hoiILGVXTl8BeENEPhCRongbikiRiGwXke2nz56wqXlERP6QsKcvIpsBtI/y0oNKqT9rPM6PlVI1InIxgE0i8g+l1DvRNlRKFQMoBoCs5tlK4/6JiEiDhEFfKTXQ6EGUUjXB/w+LyGoAvQFEDfpERGQdy9M7ItJaRDJDjwHciMAAMBER2cxoyeYwEdkPIA/AehHZGHy+g4hsCG6WDeD/i0glgPcBrFdKvW7kuEREpI/R6p3VAFZHef4AgEHBx58C6G7kOEREZA5Ryr1jpSLyJYC9cTZpByDVav95Tt6RiufFc/KOWOfVUSn1vVhvcnXQT0REtiulYs4E9iKek3ek4nnxnLxD73lx7R0iIh9h0Cci8hGvB/1ipxtgAZ6Td6TiefGcvEPXeXk6p09ERMnxek+fiIiS4Kmgn4rr9ydxTjeJSLWI7BaRWXa2MVki0lZENonIv4L/t4mxXX3wZ/R3EVlrdzu1SvS9F5EWIvJK8PW/iUgn+1uZHA3nVCgiX4b9fMY60c5kiMhyETksIlFn/EvAU8Fz3iEiPexuY7I0nFN/ETka9nN6OOFOlVKe+QfgKgC5ALYA6Blnu88AtHO6vWadE4B0AJ8A+D6A5gAqAVztdNvjnNNsALOCj2cBeDLGdsecbquGc0n4vQcwAcCS4OM7AbzidLtNOKdCAIucbmuS53UdgB4AqmK8PgjAXwAIgD4A/uZ0m004p/4A1iWzT0/19JVSu5RS1U63w0waz6k3gN1KqU+VUqcBvAxgiPWt020IgOeCj58DMNTBthil5Xsffr4rAfxU3H2TCa/9PmmiAiv3fhNnkyEAnlcB2wBcKCKX2NM6fTScU9I8FfSToHn9fo/IAbAv7Ov9wefcKlsp9UXw8UEE1l+KpmXw3gnbRMStHwxavvcN2yil6gAcBXCRLa3TR+vv04hgGmSliFxmT9Ms5bW/I63yRKRSRP4iItck2tjo7RJNZ/f6/XYw6ZxcJd45hX+hlFIiEqtErGPw5/R9AG+KyEdKqU/Mbivp8hqAl5RSp0TklwhcyVzvcJuoqQ8R+Ds6JiKDAKwB0CXeG1wX9FUKrt9vwjnVAAjvaV0afM4x8c5JRA6JyCVKqS+Cl8+HY+wj9HP6VES2APghArlmN9HyvQ9ts19EMgBkAfjanubpkvCclFLh7V+GwDiN17nu78gopdR3YY83iMhiEWmn4tyPPOXSOym6fn8FgC4i0llEmiMwWOjaahcE2vaL4ONfAGhyNSMibUSkRfBxOwB9AXxsWwu10/K9Dz/fkQDeVMFRNpdKeE4Rue7bAOyysX1WWQvg58Eqnj4AjoalIT1JRNqHxo9EpDcCMT1+h8Pp0ekkR7KHIZCHOwXgEICNwec7ANgQfPx9BKoRKgHsRCCF4njbjZxT8OtBAP6JQE/Y7ed0EYC/AvgXgM0A2gaf7wlgWfBxPoCPgj+njwDc63S745xPk+89gEcA3BZ83BLACgC7EbhnxPedbrMJ5/R48O+nEsBbAK50us0azuklAF8AOBP8m7oXwHgA44OvC4Cng+f8EeJUALrln4ZzmhT2c9oGID/RPjkjl4jIR1IuvUNERLEx6BMR+QiDPhGRjzDoExH5CIM+EZGPMOgTEfkIgz4RkY8w6BMR+ch/AxUdD7jrGPRTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "buffer = .5\n",
    "\n",
    "# Generate fake data to cover entire space\n",
    "x_min, x_max = X[:, 0].min() - buffer, X[:, 0].max() + buffer\n",
    "y_min, y_max = X[:, 1].min() - buffer, X[:, 1].max() + buffer\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .02),\n",
    "                     np.arange(y_min, y_max, .02))\n",
    "fake_data = np.c_[xx.ravel(), yy.ravel()].reshape(-1,2)\n",
    "\n",
    "# Make prediction\n",
    "lp = np.log(compute_priors(y))\n",
    "jll = compute_log_likelihoods(fake_data, means, stds)\n",
    "bcl = np.argmax(jll+lp, axis=1)\n",
    "fake_y_hat = class_labels[bcl]\n",
    "\n",
    "# Plot\n",
    "plt.contourf(xx, yy, fake_y_hat.reshape(xx.shape))\n",
    "plt.plot(X[class_0, 0], X[class_0, 1], 'or', label='Class 0')\n",
    "plt.plot(X[class_1, 0], X[class_1, 1], 'xb', label='Class 1')\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSgO7TSSX8LR"
   },
   "source": [
    "## Computing probabilities  \n",
    "\n",
    "Last thing we can do is compute the probability a given data samples belongs to any of the classes in our data. To do so, we have to use the evidence $P(X)$ that we removed earlier. Remember we removed this term as it is not needed UNLESS we want to compute the probabilities!\n",
    "\n",
    "### Adding back the evidence P(X)\n",
    "While making a prediction does not require the evidence $P(X = \\xv)$, computing the probabilities does. Recall that $P(\\xv)$ is the normalizing term that converts $P(\\xv \\mid y_k) P(y_k)$ into a probability! In order to compute the probability that each data sample belongs a particular class we have to compute $P(X = \\xv)$ and divide $P(\\xv \\mid y_k) P(y_k)$ by it. \n",
    "\n",
    "Recall, below is full Naive Bayes equation which computes the probability a given data sample belongs to a specific class.\n",
    "$$\n",
    "\\begin{align}\n",
    "P(y_k \\mid \\xv) &= \\frac{P(\\xv \\mid y_k) P(y_k)}{P(\\xv)} \\\\\n",
    "&= \\frac{(\\xv \\mid y_k) P(y_k)}{\\sum_{k=1}^K P(\\xv |y_k)P(y_k)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyJBbdOoX8LR"
   },
   "source": [
    "To compute the probabilities, notice that we have no logs. Thus, we have to undo the logs we applied to the log likelihoods by using NumPy's `np.exp()`. Recall, `np.log()` uses base $e$ so we can use the exponential function `np.exp()` to undo the $\\log$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X88UREY1X8LR",
    "outputId": "9358e08b-7c78-4872-d384-ee9452788cd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joint likelihood shape: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "joint_likelihoods = np.exp(joint_log_likelihoods)\n",
    "print(f\"joint likelihood shape: {joint_likelihoods.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vE97X3D6X8LR"
   },
   "source": [
    "To compute the evidence $P(\\xv)$ for a single data sample we sum the likelihoods across all classes. Thus, we use NumPy's `np.sum()` and pass `axis=1` to sum across all the columns (i.e., classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S9rsrFkOX8LR",
    "outputId": "1845687c-936a-4172-f8f9-5c4e63ba6af7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evidence shape BEFORE reshape: (100,)\n",
      "evidence shape AFTER reshape: (100, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.32930546],\n",
       "       [0.31542215],\n",
       "       [0.34408836],\n",
       "       [0.07887588],\n",
       "       [0.05439827],\n",
       "       [0.08888843],\n",
       "       [0.08194461],\n",
       "       [0.36588247],\n",
       "       [0.06202358],\n",
       "       [0.08049392],\n",
       "       [0.07710259],\n",
       "       [0.07127278],\n",
       "       [0.51256825],\n",
       "       [0.06995485],\n",
       "       [0.45653037],\n",
       "       [0.05404325],\n",
       "       [0.37539373],\n",
       "       [0.32213222],\n",
       "       [0.07290274],\n",
       "       [0.07671981],\n",
       "       [0.40848401],\n",
       "       [0.33845261],\n",
       "       [0.07766364],\n",
       "       [0.07135623],\n",
       "       [0.21870316],\n",
       "       [0.05860169],\n",
       "       [0.06807035],\n",
       "       [0.31191949],\n",
       "       [0.07476968],\n",
       "       [0.32845485],\n",
       "       [0.05883   ],\n",
       "       [0.07576931],\n",
       "       [0.29952098],\n",
       "       [0.08183876],\n",
       "       [0.23437417],\n",
       "       [0.07660891],\n",
       "       [0.34976586],\n",
       "       [0.28552734],\n",
       "       [0.07292286],\n",
       "       [0.06710908],\n",
       "       [0.06915158],\n",
       "       [0.07885978],\n",
       "       [0.08519742],\n",
       "       [0.34307147],\n",
       "       [0.06295492],\n",
       "       [0.06478053],\n",
       "       [0.40348031],\n",
       "       [0.08413542],\n",
       "       [0.43285155],\n",
       "       [0.06206371],\n",
       "       [0.05464554],\n",
       "       [0.06119526],\n",
       "       [0.07400981],\n",
       "       [0.07539383],\n",
       "       [0.06287196],\n",
       "       [0.42912475],\n",
       "       [0.06751825],\n",
       "       [0.45629117],\n",
       "       [0.07106973],\n",
       "       [0.36191057],\n",
       "       [0.33357794],\n",
       "       [0.42381572],\n",
       "       [0.25496151],\n",
       "       [0.08821371],\n",
       "       [0.35052998],\n",
       "       [0.37873579],\n",
       "       [0.08625238],\n",
       "       [0.34313668],\n",
       "       [0.33244121],\n",
       "       [0.24284065],\n",
       "       [0.35948948],\n",
       "       [0.06538412],\n",
       "       [0.42567625],\n",
       "       [0.06809624],\n",
       "       [0.37037159],\n",
       "       [0.41587216],\n",
       "       [0.24852411],\n",
       "       [0.07525824],\n",
       "       [0.30141298],\n",
       "       [0.42190222],\n",
       "       [0.41016538],\n",
       "       [0.45260835],\n",
       "       [0.05540146],\n",
       "       [0.06882967],\n",
       "       [0.08485101],\n",
       "       [0.38327432],\n",
       "       [0.07298264],\n",
       "       [0.05517625],\n",
       "       [0.06258258],\n",
       "       [0.38367065],\n",
       "       [0.41532078],\n",
       "       [0.32281092],\n",
       "       [0.38020053],\n",
       "       [0.35783565],\n",
       "       [0.09232124],\n",
       "       [0.38282539],\n",
       "       [0.44228447],\n",
       "       [0.07445474],\n",
       "       [0.32747415],\n",
       "       [0.38520101]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidence = np.sum(joint_likelihoods, axis=1)\n",
    "print(f\"evidence shape BEFORE reshape: {evidence.shape}\")\n",
    "evidence = evidence.reshape(-1, 1)\n",
    "print(f\"evidence shape AFTER reshape: {evidence.shape}\")\n",
    "evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSs6sMgfX8LR"
   },
   "source": [
    "To compute the probabilities for each data sample we simply divide the `joint_likelihoods` by the `evidence` as is done in the full Naive Bayes equation given above.  Recall, `joint_likelihoods` contains the likelihoods for all the data $\\Xm$ which means we can compute the evidence for every data sample at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_B0RyEbpX8LR",
    "outputId": "aff9c783-74e4-49b6-b977-95a4c20d5eec",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs shape: (100, 2)\n"
     ]
    }
   ],
   "source": [
    "probs = joint_likelihoods / evidence\n",
    "print(f\"probs shape: {probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-98Dt1YX8LR"
   },
   "source": [
    "Below we can see the probability each data sample belongs to a given class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ONSPxilIX8LS",
    "outputId": "43a0f31a-7bf2-435d-e231-78af02bc2d24"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1f0cd0cc-bb55-48dc-94dc-39ef10d4a380\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class 0</th>\n",
       "      <th>Class 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.369013</td>\n",
       "      <td>0.630987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.370817</td>\n",
       "      <td>0.629183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.355168</td>\n",
       "      <td>0.644832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.791479</td>\n",
       "      <td>0.208521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.890920</td>\n",
       "      <td>0.109080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.330321</td>\n",
       "      <td>0.669679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.300430</td>\n",
       "      <td>0.699570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.820694</td>\n",
       "      <td>0.179306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.367870</td>\n",
       "      <td>0.632130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.325234</td>\n",
       "      <td>0.674766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f0cd0cc-bb55-48dc-94dc-39ef10d4a380')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1f0cd0cc-bb55-48dc-94dc-39ef10d4a380 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1f0cd0cc-bb55-48dc-94dc-39ef10d4a380');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "     Class 0   Class 1\n",
       "0   0.369013  0.630987\n",
       "1   0.370817  0.629183\n",
       "2   0.355168  0.644832\n",
       "3   0.791479  0.208521\n",
       "4   0.890920  0.109080\n",
       "..       ...       ...\n",
       "95  0.330321  0.669679\n",
       "96  0.300430  0.699570\n",
       "97  0.820694  0.179306\n",
       "98  0.367870  0.632130\n",
       "99  0.325234  0.674766\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_df = pd.DataFrame(probs, columns=[\"Class 0\", \"Class 1\"])\n",
    "probs_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
